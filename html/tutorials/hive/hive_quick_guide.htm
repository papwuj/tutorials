<!DOCTYPE html>
<html lang="en-US">
<head>
<title>Hive - Quick Guide - Tutorialspoint</title>
<meta charset="utf-8">
<meta name="description" content="Hive - Quick Guide - The term âBig Dataâ is used for collections of large datasets that include huge volume, high velocity, and a variety of data that is increasing day by"/>
<meta name="keywords" content="C, C++, Python, Java, HTML, CSS, JavaScript, SQL, PHP, jQuery, XML, DOM, Bootstrap, Tutorials, Articles, Programming, training, learning, quiz, preferences, examples, code"/>
<link rel="canonical" href="https://www.tutorialspoint.com/hive/hive_quick_guide.htm" />
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<meta name="viewport" content="width=device-width,initial-scale=1.0,user-scalable=yes">
<script src="/theme/js/script-min-v2.js?v=3"></script>
<link rel="stylesheet" href="/theme/css/style-min-v2.css?v=6">
<script src="//services.bilsyndication.com/adv1/?d=901" defer="" async=""></script>
<script> var vitag = vitag || {};</script>
<script> vitag.outStreamConfig = { enablePC: false, enableMobile: false };</script>  
<style>
.right-menu .mui-btn {
    background-color:#ffb109;
}
a.demo {
    background:#ffb109;
}
li.heading {
    background:#ffb109;
}
.course-box{background:#ffb109}
.home-intro-sub p{color:#ffb109}
</style>
</head>
<body>
<header id="header">
<!-- Top sub-menu Starts Here -->
<div class="mui-appbar mui-container-fulid top-menu">
<div class="mui-container">
<div class="top-menu-item home">
<a href="https://www.tutorialspoint.com/index.htm" target="_blank" title="TutorialsPoint - Home"><i class="fal fa-home"></i> <span>Home</span></a>
</div>
<div class="top-menu-item qa">
<a href="https://www.tutorialspoint.com/about/about_careers.htm" target="_blank" title="Job @ Tutorials Point"><i class="fa fa-suitcase"></i> <span>Jobs</span></a>
</div>
<div class="top-menu-item tools">
<a href="https://www.tutorialspoint.com/online_dev_tools.htm" target="_blank" title="Tools - Online Development and Testing Tools"><i class="fal fa-cogs"></i> <span>Tools</span></a>
</div>
<div class="top-menu-item coding-ground">
<a href="https://www.tutorialspoint.com/codingground.htm" target="_blank" title="Coding Ground - Free Online IDE and Terminal"><i class="fal fa-code"></i> <span>Coding Ground </span></a> 
</div>
<div class="top-menu-item current-affairs">
<a href="https://www.tutorialspoint.com/current_affairs.htm" target="_blank" title="Daily Current Affairs"><i class="fal fa-layer-plus"></i> <span>Current Affairs</span></a>
</div>
<div class="top-menu-item upsc-notes">
<a href="https://www.tutorialspoint.com/upsc_ias_exams.htm" target="_blank" title="UPSC IAS Exams Notes - TutorialsPoint"><i class="fal fa-user-tie"></i> <span>UPSC Notes</span></a>
</div>      
<div class="top-menu-item online-tutoris">
<a href="https://www.tutorialspoint.com/tutor_connect/index.php" target="_blank" title="Top Online Tutors - Tutor Connect"><i class="fal fa-user"></i> <span>Online Tutors</span></a>
</div>
<div class="top-menu-item whiteboard">
<a href="https://www.tutorialspoint.com/whiteboard.htm" target="_blank" title="Free Online Whiteboard"><i class="fal fa-chalkboard"></i> <span>Whiteboard</span></a>
</div>
<div class="top-menu-item net-meeting">
<a href="https://www.tutorialspoint.com/netmeeting.php" target="_blank" title="A free tool for online video conferencing"><i class="fal fa-chalkboard-teacher"></i> <span>Net Meeting</span></a> 
</div>
<div class="top-menu-item articles">
<a href="https://www.tutorix.com" target="_blank" title="Tutorx - The Best Learning App" rel="nofollow"><i class="fal fa-video"></i> <span>Tutorix</span></a> 
</div>        
<div class="social-menu-item">
<a href="https://www.facebook.com/tutorialspointindia" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Facebook"><i class="fab fa-facebook-f"></i></a> 
<a href="https://www.twitter.com/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Twitter"><i class="fab fa-twitter"></i></a>
<a href="https://www.linkedin.com/company/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Linkedin"><i class="fab fa-linkedin-in"></i></a>
<a href="https://www.youtube.com/channel/UCVLbzhxVTiTLiVKeGV7WEBg" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint YouTube"><i class="fab fa-youtube"></i></a>
</div>        
</div>
</div>
<!-- Top sub-menu Ends Here -->
<!-- Top main-menu Starts Here -->
<div class="mui-appbar mui-container-fulid mui--appbar-line-height mui--z1" id="logo-menu">
<div class="mui-container">
<div class="left-menu">
<a href="https://www.tutorialspoint.com/index.htm" title="Tutorialspoint">
<img class="tp-logo" alt="tutorialspoint" src="/hive/images/logo.png">
</a>
<div class="mui-dropdown">
<a class="mui-btn mui-btn--primary categories" data-mui-toggle="dropdown"><i class="fa fa-th-large"></i> 
<span>Categories <span class="mui-caret"></span></span></a>            
<ul class="mui-dropdown__menu cat-menu">
<li>
<ul>
<li><a href="/academic_tutorials.htm"><i class="fa fa-caret-right"></i> Academic Tutorials</a></li>
<li><a href="/big_data_tutorials.htm"><i class="fa fa-caret-right"></i> Big Data &amp; Analytics </a></li>
<li><a href="/computer_programming_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Programming </a></li>
<li><a href="/computer_science_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Science </a></li>
<li><a href="/database_tutorials.htm"><i class="fa fa-caret-right"></i> Databases </a></li>
<li><a href="/devops_tutorials.htm"><i class="fa fa-caret-right"></i> DevOps </a></li>
<li><a href="/digital_marketing_tutorials.htm"><i class="fa fa-caret-right"></i> Digital Marketing </a></li>
<li><a href="/engineering_tutorials.htm"><i class="fa fa-caret-right"></i> Engineering Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> Exams Syllabus </a></li>
<li><a href="/famous_monuments.htm"><i class="fa fa-caret-right"></i> Famous Monuments </a></li>
<li><a href="/gate_exams_tutorials.htm"><i class="fa fa-caret-right"></i> GATE Exams Tutorials</a></li>
<li><a href="/latest_technologies.htm"><i class="fa fa-caret-right"></i> Latest Technologies </a></li>
<li><a href="/machine_learning_tutorials.htm"><i class="fa fa-caret-right"></i> Machine Learning </a></li>
<li><a href="/mainframe_tutorials.htm"><i class="fa fa-caret-right"></i> Mainframe Development </a></li>
<li><a href="/management_tutorials.htm"><i class="fa fa-caret-right"></i> Management Tutorials </a></li>
<li><a href="/maths_tutorials.htm"><i class="fa fa-caret-right"></i> Mathematics Tutorials</a></li>
<li><a href="/microsoft_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Microsoft Technologies </a></li>
<li><a href="/misc_tutorials.htm"><i class="fa fa-caret-right"></i> Misc tutorials </a></li>
<li><a href="/mobile_development_tutorials.htm"><i class="fa fa-caret-right"></i> Mobile Development </a></li>
<li><a href="/java_technology_tutorials.htm"><i class="fa fa-caret-right"></i> Java Technologies </a></li>
<li><a href="/python_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Python Technologies </a></li>
<li><a href="/sap_tutorials.htm"><i class="fa fa-caret-right"></i> SAP Tutorials </a></li>
<li><a href="/scripting_lnaguage_tutorials.htm"><i class="fa fa-caret-right"></i>Programming Scripts </a></li>
<li><a href="/selected_reading.htm"><i class="fa fa-caret-right"></i> Selected Reading </a></li>
<li><a href="/software_quality_tutorials.htm"><i class="fa fa-caret-right"></i> Software Quality </a></li>
<li><a href="/soft_skill_tutorials.htm"><i class="fa fa-caret-right"></i> Soft Skills </a></li>
<li><a href="/telecom_tutorials.htm"><i class="fa fa-caret-right"></i> Telecom Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> UPSC IAS Exams </a></li>
<li><a href="/web_development_tutorials.htm"><i class="fa fa-caret-right"></i> Web Development </a></li>
<li><a href="/sports_tutorials.htm"><i class="fa fa-caret-right"></i> Sports Tutorials </a></li>
<li><a href="/xml_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> XML Technologies </a></li>
<li><a href="/multi_language_tutorials.htm"><i class="fa fa-caret-right"></i> Multi-Language Tutorials</a></li>
<li><a href="/questions_and_answers.htm"><i class="fa fa-caret-right"></i> Interview Questions</a></li>
</ul>
</li>
</ul>
<div class="clear"></div>
</div> 
</div>
<div class="right-menu">
<div class="toc-toggle">
<a href="javascript:void(0);"><i class="fa fa-bars"></i></a>
</div>
<div class="mobile-search-btn">
<a href="https://www.tutorialspoint.com/search.htm"><i class="fal fa-search"></i></a>
</div>
<div class="search-box">
<form method="get" class="" name="searchform" action="https://www.google.com/search" target="_blank" novalidate="">
<input type="hidden" name="sitesearch" value="www.tutorialspoint.com" class="user-valid valid">
<input class="header-search-box" type="text" id="search-string" name="q" placeholder="Search your favorite tutorials..." onfocus="if (this.value == 'Search your favorite tutorials...') {this.value = '';}" onblur="if (this.value == '') {this.value = 'Search your favorite tutorials...';}" autocomplete="off">
<button><i class="fal fa-search"></i></button>
</form>
</div>
<div class="menu-btn library-btn">
<a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a>
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a> 
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/questions/index.php"><i class="fa fa-location-arrow"></i> <span>Q/A</span></a>
</div>
<div class="menu-btn ebooks-btn">
<a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a>
</div>
<div class="mui-dropdown">
<button class="mui-btn mui-btn--primary" data-mui-toggle="dropdown">
<span class="mui-caret"></span>
</button>
<ul class="mui-dropdown__menu">
<li><a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a></li>
<li><a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a></li>
<li><a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a></li>
</ul>
</div>
</div>
</div>
</div>
<!-- Top main-menu Ends Here -->
</header>
<div class="mui-container-fluid content">
<div class="mui-container">
<!-- Tutorial ToC Starts Here -->
<div class="mui-col-md-3 tutorial-toc">
<div class="mini-logo">
<img src="/hive/images/hive-mini-logo.jpg" alt="Hive Tutorial" />
</div>
<ul class="toc chapters">
<li class="heading">Hive Tutorial</li>
<li><a target="_top" href="/hive/index.htm">Hive - Home</a></li>
<li><a target="_top" href="/hive/hive_introduction.htm">Hive - Introduction</a></li>
<li><a target="_top" href="/hive/hive_installation.htm">Hive - Installation</a></li>
<li><a target="_top" href="/hive/hive_data_types.htm">Hive - Data Types</a></li>
<li><a target="_top" href="/hive/hive_create_database.htm">Hive - Create Database</a></li>
<li><a target="_top" href="/hive/hive_drop_database.htm">Hive - Drop Database</a></li>
<li><a target="_top" href="/hive/hive_create_table.htm">Hive - Create Table</a></li>
<li><a target="_top" href="/hive/hive_alter_table.htm">Hive - Alter Table</a></li>
<li><a target="_top" href="/hive/hive_drop_table.htm">Hive - Drop Table</a></li>
<li><a target="_top" href="/hive/hive_partitioning.htm">Hive - Partitioning</a></li>
<li><a target="_top" href="/hive/hive_built_in_operators.htm">Hive - Built-In Operators</a></li>
<li><a target="_top" href="/hive/hive_built_in_functions.htm">Hive - Built-In Functions</a></li>
<li><a target="_top" href="/hive/hive_views_and_indexes.htm">Hive - Views And Indexes</a></li>
</ul>
<ul class="toc chapters">
<li class="heading">HiveQL</li>
<li><a target="_top" href="/hive/hiveql_select_where.htm">HiveQL -  Select Where</a></li>
<li><a target="_top" href="/hive/hiveql_select_order_by.htm">HiveQL - Select Order By</a></li>
<li><a target="_top" href="/hive/hiveql_group_by.htm">HiveQL - Select Group By</a></li>
<li><a target="_top" href="/hive/hiveql_joins.htm">HiveQL - Select Joins</a></li>
</ul>
<ul class="toc chapters">
<li class="heading">Hive Useful Resources</li>
<li><a target="_top" href="/hive/hive_questions_answers.htm">Hive - Questions and Answers</a></li>
<li><a target="_top" href="/hive/hive_quick_guide.htm">Hive - Quick Guide</a></li>
<li><a target="_top" href="/hive/hive_useful_resources.htm">Hive - Useful Resources</a></li>
<li><a target="_top" href="/hive/hive_discussion.htm">Hive - Discussion</a></li>
</ul>
<ul class="toc reading">
<li class="sreading">Selected Reading</li>
<li><a target="_top" href="/upsc_ias_exams.htm">UPSC IAS Exams Notes</a></li>
<li><a target="_top" href="/developers_best_practices/index.htm">Developer's Best Practices</a></li>
<li><a target="_top" href="/questions_and_answers.htm">Questions and Answers</a></li>
<li><a target="_top" href="/effective_resume_writing.htm">Effective Resume Writing</a></li>
<li><a target="_top" href="/hr_interview_questions/index.htm">HR Interview Questions</a></li>
<li><a target="_top" href="/computer_glossary.htm">Computer Glossary</a></li>
<li><a target="_top" href="/computer_whoiswho.htm">Who is Who</a></li>
</ul>
</div>
<!-- Tutorial ToC Ends Here -->
<!-- Tutorial Content Starts Here -->
<div class="mui-col-md-6 tutorial-content">
<h1>Hive - Quick Guide</h1>
<hr />
<div class="top-ad-heading">Advertisements</div>
<div style="text-align: center;">
<script><!--
google_ad_client = "pub-7133395778201029";
var width = document.getElementsByClassName("tutorial-content")[0].clientWidth - 40;
google_ad_width = width;
google_ad_height = 150;
google_ad_format = width + "x150_as";
google_ad_type = "image";
google_ad_channel = "";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="mui-container-fluid button-borders">
<div class="pre-btn">
<a href="/hive/hive_questions_answers.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/hive/hive_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="clearer"></div>
<h1>Hive - Introduction</h1>
<p>The term ‘Big Data’ is used for collections of large datasets that include huge volume, high velocity, and a variety of data that is increasing day by day. Using traditional data management systems, it is difficult to process Big Data. Therefore, the Apache Software Foundation introduced a framework called Hadoop to solve Big Data management and processing challenges.</p>
<h2>Hadoop</h2>
<p>Hadoop is an open-source framework to store and process Big Data in a distributed environment. It contains two modules, one is MapReduce and another is Hadoop Distributed File System (HDFS).</p>
<ul class="list">
<li><p><b>MapReduce:</b> It is a parallel programming model for processing large amounts of structured, semi-structured, and unstructured data on large clusters of commodity hardware.</p></li>
<li><p><b>HDFS:</b>Hadoop Distributed File System is a part of Hadoop framework, used to store and process the datasets. It provides a fault-tolerant file system to run on commodity hardware.</p></li>
</ul>
<p>The Hadoop ecosystem contains different sub-projects (tools) such as Sqoop, Pig, and Hive that are used to help Hadoop modules.</p>
<ul class="list">
<li><p><b>Sqoop:</b> It is used to import and export data to and fro between HDFS and RDBMS.</p></li>
<li><p><b>Pig:</b> It is a procedural language platform used to develop a script for MapReduce operations.</p></li>
<li><p><b>Hive:</b> It is a platform used to develop SQL type scripts to do MapReduce operations.</p></li>
</ul>
<p><b>Note:</b> There are various ways to execute MapReduce operations:</p>
<ul class="list">
<li>The traditional approach using Java MapReduce program for structured, semi-structured, and unstructured data.</li>
<li>The scripting approach for MapReduce to process structured and semi structured data using Pig.</li>
<li>The Hive Query Language (HiveQL or HQL) for MapReduce to process structured data using Hive.</li>
</ul>
<h2>What is Hive</h2>
<p>Hive is a data warehouse infrastructure tool to process structured data in Hadoop. It resides on top of Hadoop to summarize Big Data, and makes querying and analyzing easy.</p>
<p>Initially Hive was developed by Facebook, later the Apache Software Foundation took it up and developed it further as an open source under the name Apache Hive. It is used by different companies. For example, Amazon uses it in Amazon Elastic MapReduce.</p>
<h3>Hive is not</h3>
<ul class="list">
<li>A relational database</li>
<li>A design for OnLine Transaction Processing (OLTP)</li>
<li>A language for real-time queries and row-level updates</li>
</ul>
<h2>Features of Hive</h2>
<ul class="list">
<li>It stores schema in a database and processed data into HDFS.</li>
<li>It is designed for OLAP.</li>
<li>It provides SQL type language for querying called HiveQL or HQL.</li>
<li>It is familiar, fast, scalable, and extensible.</li>
</ul>
<h2>Architecture of Hive</h2>
<p>The following component diagram depicts the architecture of Hive:</p>
<img src="/hive/images/hive_architecture.jpg" alt="Hive Architecture"/>
<p>This component diagram contains different units. The following table describes each unit:</p>
<table class="table table-bordered">
<tr>
<th style="width:20%">Unit Name</th>
<th>Operation</th>
</tr>
<tr>
<td>User Interface</td>
<td>Hive is a data warehouse infrastructure software that can create interaction between user and HDFS. The user interfaces that Hive supports are Hive Web UI, Hive command line, and Hive HD Insight (In Windows server).</td>
</tr>
<tr>
<td>Meta Store</td>
<td>Hive chooses respective database servers to store the schema or Metadata of tables, databases, columns in a table, their data types, and HDFS mapping.</td>
</tr>
<tr>
<td>HiveQL Process Engine</td>
<td>HiveQL is similar to SQL for querying on schema info on the Metastore. It is one of the replacements of traditional approach for MapReduce program. Instead of writing MapReduce program in Java, we can write a query for MapReduce job and process it.</td>
</tr>
<tr>
<td>Execution Engine</td>
<td>The conjunction part of HiveQL process Engine and MapReduce is Hive Execution Engine. Execution engine processes the query and generates results as same as MapReduce results. It uses the flavor of MapReduce.</td>
</tr>
<tr>
<td>HDFS or HBASE</td>
<td>Hadoop distributed file system or HBASE are the data storage techniques to store data into file system.</td>
</tr>
</table>
<h2>Working of Hive</h2>
<p>The following diagram depicts the workflow between Hive and Hadoop.</p>
<img src="/hive/images/how_hive_works.jpg" alt="How Hive Works"/>
<p>The following table defines how Hive interacts with Hadoop framework:</p>
<table class="table table-bordered">
<tr>
<th style="width:13%">Step No.</th>
<th>Operation</th>
</tr>
<tr>
<td>1</td>
<td><b>Execute Query</b>
<p>The Hive interface such as Command Line or Web UI sends query to Driver (any database driver such as JDBC, ODBC, etc.) to execute.</p></td>
</tr>
<tr>
<td>2</td>
<td><b>Get Plan</b>
<p>The driver takes the help of query compiler that parses the query to check the syntax and query plan or the requirement of query.</p></td>
</tr>
<tr>
<td>3</td>
<td><b>Get Metadata</b>
<p>The compiler sends metadata request to Metastore (any database).</p></td>
</tr>
<tr>
<td>4</td>
<td><b>Send Metadata</b>
<p>Metastore sends metadata as a response to the compiler.</p></td>
</tr>
<tr>
<td>5</td>
<td><b>Send Plan</b>
<p>The compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete.</p></td>
</tr>
<tr>
<td>6</td>
<td><b>Execute Plan</b>
<p>The driver sends the execute plan to the execution engine.</p></td>
</tr>
<tr>
<td>7</td>
<td><b>Execute Job</b>
<p>Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job.</p></td>
</tr>
<tr>
<td>7.1</td>
<td><b>Metadata Ops</b>
<p>Meanwhile in execution, the execution engine can execute metadata operations with Metastore.</p></td>
</tr>
<tr>
<td>8</td>
<td><b>Fetch Result</b>
<p>The execution engine receives the results from Data nodes.</p></td>
</tr>
<tr>
<td>9</td>
<td><b>Send Results</b>
<p>The execution engine sends those resultant values to the driver.</p></td>
</tr>
<tr>
<td>10</td>
<td><b>Send Results</b>
<p>The driver sends the results to Hive Interfaces.</p></td>
</tr>
</table>
<h1>Hive - Installation</h1>
<p>All Hadoop sub-projects such as Hive, Pig, and HBase support Linux operating system. Therefore, you need to install any Linux flavored OS. The following simple steps are executed for Hive installation:</p>
<h2>Step 1: Verifying JAVA Installation</h2>
<p>Java must be installed on your system before installing Hive. Let us verify java installation using the following command:</p>
<pre class="prettyprint notranslate">
$ java –version
</pre>
<p>If Java is already installed on your system, you get to see the following response:</p>
<pre class="prettyprint notranslate">
java version "1.7.0_71" 
Java(TM) SE Runtime Environment (build 1.7.0_71-b13) 
Java HotSpot(TM) Client VM (build 25.0-b02, mixed mode)
</pre>
<p>If java is not installed in your system, then follow the steps given below for installing java.</p>
<h2>Installing Java</h2>
<h3>Step I:</h3>
<p>Download java (JDK &lt;latest version&gt; - X64.tar.gz) by visiting the following link <a   rel="nofollow" href="http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html"/>http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html.</a></p>
<p>Then jdk-7u71-linux-x64.tar.gz will be downloaded onto your system.</p>
<h3>Step II:</h3>
<p>Generally you will find the downloaded java file in the Downloads folder. Verify it and extract the jdk-7u71-linux-x64.gz file using the following commands.</p>
<pre class="prettyprint notranslate">
$ cd Downloads/
$ ls
jdk-7u71-linux-x64.gz
$ tar zxf jdk-7u71-linux-x64.gz
$ ls
jdk1.7.0_71 jdk-7u71-linux-x64.gz
</pre>
<h3>Step III:</h3>
<p>To make java available to all the users, you have to move it to the location “/usr/local/”. Open root, and type the following commands.</p>
<pre class="prettyprint notranslate">
$ su
password:
# mv jdk1.7.0_71 /usr/local/
# exit
</pre>
<h3>Step IV:</h3>
<p>For setting up PATH and JAVA_HOME variables, add the following commands to ~/.bashrc file.</p>
<pre class="prettyprint notranslate">
export JAVA_HOME=/usr/local/jdk1.7.0_71
export PATH=PATH:$JAVA_HOME/bin
</pre>
<p>Now verify the installation using the command java -version from the terminal as explained above.</p>
<h2>Step 2: Verifying Hadoop Installation</h2>
<p>Hadoop must be installed on your system before installing Hive. Let us verify the Hadoop installation using the following command:</p>
<pre class="prettyprint notranslate">
$ hadoop version
</pre>
<p>If Hadoop is already installed on your system, then you will get the following response:</p>
<pre class="prettyprint notranslate">
Hadoop 2.4.1 Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768 
Compiled by hortonmu on 2013-10-07T06:28Z 
Compiled with protoc 2.5.0 
From source with checksum 79e53ce7994d1628b240f09af91e1af4
</pre>
<p>If Hadoop is not installed on your system, then proceed with the following steps:</p>
<h2>Downloading Hadoop</h2>
<p>Download and extract Hadoop 2.4.1 from Apache Software Foundation using the following commands.</p>
<pre class="prettyprint notranslate">
$ su
password:
# cd /usr/local
# wget http://apache.claz.org/hadoop/common/hadoop-2.4.1/
hadoop-2.4.1.tar.gz
# tar xzf hadoop-2.4.1.tar.gz
# mv hadoop-2.4.1/* to hadoop/
# exit
</pre>
<h2>Installing Hadoop in Pseudo Distributed Mode</h2>
<p>The following steps are used to install Hadoop 2.4.1 in pseudo distributed mode.</p>
<h3>Step I: Setting up Hadoop</h3>
<p>You can set Hadoop environment variables by appending the following commands to <b>~/.bashrc</b> file.</p>
<pre class="prettyprint notranslate">
export HADOOP_HOME=/usr/local/hadoop 
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export
PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
</pre>
<p>Now apply all the changes into the current running system.</p>
<pre class="prettyprint notranslate">
$ source ~/.bashrc
</pre>
<h3>Step II: Hadoop Configuration</h3>
<p>You can find all the Hadoop configuration files in the location “$HADOOP_HOME/etc/hadoop”. You need to make suitable changes in those configuration files according to your Hadoop infrastructure.</p>
<pre class="prettyprint notranslate">
$ cd $HADOOP_HOME/etc/hadoop
</pre>
<p>In order to develop Hadoop programs using java, you have to reset the java environment variables in <b>hadoop-env.sh</b> file by replacing <b>JAVA_HOME</b> value with the location of java in your system.</p>
<pre class="prettyprint notranslate">
export JAVA_HOME=/usr/local/jdk1.7.0_71
</pre>
<p>Given below are the list of files that you have to edit to configure Hadoop.</p>
<p><b>core-site.xml</b></p>
<p>The <b>core-site.xml</b> file contains information such as the port number used for Hadoop instance, memory allocated for the file system, memory limit for storing the data, and the size of Read/Write buffers.</p>
<p>Open the core-site.xml and add the following properties in between the &lt;configuration&gt; and &lt;/configuration&gt; tags.</p>
<pre class="prettyprint notranslate">
&lt;configuration&gt;

   &lt;property&gt; 
      &lt;name&gt;fs.default.name&lt;/name&gt; 
      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; 
   &lt;/property&gt;
   
&lt;/configuration&gt;
</pre>
<p><b>hdfs-site.xml</b></p>
<p>The <b>hdfs-site.xml</b> file contains information such as the value of replication data, the namenode path, and the datanode path of your local file systems. It means the place where you want to store the Hadoop infra.</p>
<p>Let us assume the following data.</p>
<pre class="prettyprint notranslate">
dfs.replication (data replication value) = 1

(In the following path /hadoop/ is the user name.
hadoopinfra/hdfs/namenode is the directory created by hdfs file system.)

namenode path = //home/hadoop/hadoopinfra/hdfs/namenode

(hadoopinfra/hdfs/datanode is the directory created by hdfs file system.)
datanode path = //home/hadoop/hadoopinfra/hdfs/datanode
</pre>
<p>Open this file and add the following properties in between the &lt;configuration&gt;, &lt;/configuration&gt; tags in this file.</p>
<pre class="prettyprint notranslate">
&lt;configuration&gt;

   &lt;property&gt; 
      &lt;name&gt;dfs.replication&lt;/name&gt; 
      &lt;value&gt;1&lt;/value&gt; 
   &lt;/property&gt; 
   &lt;property&gt; 
      &lt;name&gt;dfs.name.dir&lt;/name&gt; 
      &lt;value&gt;file:///home/hadoop/hadoopinfra/hdfs/namenode &lt;/value&gt; 
   &lt;/property&gt; 
   &lt;property&gt; 
      &lt;name&gt;dfs.data.dir&lt;/name&gt;
      &lt;value>file:///home/hadoop/hadoopinfra/hdfs/datanode &lt;/value &gt; 
   &lt;/property&gt;
   
&lt;/configuration&gt;
</pre>
<p><b>Note:</b> In the above file, all the property values are user-defined and you can make changes according to your Hadoop infrastructure.</p>
<p><b>yarn-site.xml</b></p>
<p>This file is used to configure yarn into Hadoop. Open the yarn-site.xml file and add the following properties in between the &lt;configuration&gt;, &lt;/configuration&gt; tags in this file.</p>
<pre class="prettyprint notranslate">
&lt;configuration&gt;

   &lt;property> 
      &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; 
      &lt;value&gt;mapreduce_shuffle&lt;/value&gt; 
   &lt;/property&gt;
   
&lt;/configuration&gt;
</pre>
<p><b>mapred-site.xml</b></p>
<p>This file is used to specify which MapReduce framework we are using. By default, Hadoop contains a template of yarn-site.xml. First of all, you need to copy the file from mapred-site,xml.template to mapred-site.xml file using the following command.</p>
<pre class="prettyprint notranslate">
$ cp mapred-site.xml.template mapred-site.xml
</pre>
<p>Open <b>mapred-site.xml</b> file and add the following properties in between the &lt;configuration&gt;, &lt;/configuration&gt; tags in this file.</p>
<pre class="prettyprint notranslate">
&lt;configuration&gt;

   &lt;property&gt; 
      &lt;name&gt;mapreduce.framework.name&lt;/name&gt; 
      &lt;value&gt;yarn&lt;/value&gt; 
   &lt;/property&gt;

&lt;/configuration&gt;
</pre>
<h2>Verifying Hadoop Installation</h2>
<p>The following steps are used to verify the Hadoop installation.</p>
<h3>Step I: Name Node Setup</h3>
<p>Set up the namenode using the command “hdfs namenode -format” as follows.</p>
<pre class="prettyprint notranslate">
$ cd ~
$ hdfs namenode -format
</pre>
<p>The expected result is as follows.</p>
<pre class="prettyprint notranslate">
10/24/14 21:30:55 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************ 
STARTUP_MSG: Starting NameNode 
STARTUP_MSG: host = localhost/192.168.1.11 
STARTUP_MSG: args = [-format] 
STARTUP_MSG: version = 2.4.1 
... 
... 
10/24/14 21:30:56 INFO common.Storage: Storage directory 
/home/hadoop/hadoopinfra/hdfs/namenode has been successfully formatted. 
10/24/14 21:30:56 INFO namenode.NNStorageRetentionManager: Going to 
retain 1 images with txid >= 0 
10/24/14 21:30:56 INFO util.ExitUtil: Exiting with status 0
10/24/14 21:30:56 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************ 
SHUTDOWN_MSG: Shutting down NameNode at localhost/192.168.1.11
 ************************************************************/
</pre>
<h3>Step II: Verifying Hadoop dfs</h3>
<p>The following command is used to start dfs. Executing this command will start your Hadoop file system.</p>
<pre class="prettyprint notranslate">
$ start-dfs.sh
</pre>
<p>The expected output is as follows:</p>
<pre class="prettyprint notranslate">
10/24/14 21:37:56 
Starting namenodes on [localhost] 
localhost: starting namenode, logging to /home/hadoop/hadoop-2.4.1/logs/hadoop-hadoop-namenode-localhost.out 
localhost: starting datanode, logging to /home/hadoop/hadoop-2.4.1/logs/hadoop-hadoop-datanode-localhost.out 
Starting secondary namenodes [0.0.0.0]
</pre>
<h3>Step III: Verifying Yarn Script</h3>
<p>The following command is used to start the yarn script. Executing this command will start your yarn daemons.</p>
<pre class="prettyprint notranslate">
$ start-yarn.sh
</pre>
<p>The expected output is as follows:</p>
<pre class="prettyprint notranslate">
starting yarn daemons 
starting resourcemanager, logging to /home/hadoop/hadoop-2.4.1/logs/yarn-hadoop-resourcemanager-localhost.out 
localhost: starting nodemanager, logging to /home/hadoop/hadoop-2.4.1/logs/yarn-hadoop-nodemanager-localhost.out
</pre>
<h3>Step IV: Accessing Hadoop on Browser</h3>
<p>The default port number to access Hadoop is 50070. Use the following url to get Hadoop services on your browser.</p>
<pre class="prettyprint notranslate">
http://localhost:50070/
</pre>
<img src="/hive/images/hadoop_browser.jpg" alt="Hadoop Browser"/>
<h3>Step V: Verify all applications for cluster</h3>
<p>The default port number to access all applications of cluster is 8088. Use the following url to visit this service.</p>
<pre class="prettyprint notranslate">
http://localhost:8088/
</pre>
<img src="/hive/images/all_applications.jpg" alt="All Applications"/>
<h3>Step 3: Downloading Hive</h3>
<p>We use hive-0.14.0 in this tutorial. You can download it by visiting the following link <a  rel="nofollow" href="http://apache.petsads.us/hive/hive-0.14.0/">http://apache.petsads.us/hive/hive-0.14.0/.</a> Let us assume it gets downloaded onto the /Downloads directory. Here, we download Hive archive named “apache-hive-0.14.0-bin.tar.gz” for this tutorial. The following command is used to verify the download:</p>
<pre class="prettyprint notranslate">
$ cd Downloads
$ ls
</pre>
<p>On successful download, you get to see the following response:</p>
<pre class="prettyprint notranslate">
apache-hive-0.14.0-bin.tar.gz
</pre>
<h2>Step 4: Installing Hive</h2>
<p>The following steps are required for installing Hive on your system. Let us assume the Hive archive is downloaded onto the /Downloads directory.</p>
<h3>Extracting and verifying Hive Archive</h3>
<p>The following command is used to verify the download and extract the hive archive:</p>
<pre class="prettyprint notranslate">
$ tar zxvf apache-hive-0.14.0-bin.tar.gz
$ ls
</pre>
<p>On successful download, you get to see the following response:</p>
<pre class="prettyprint notranslate">
apache-hive-0.14.0-bin apache-hive-0.14.0-bin.tar.gz
</pre>
<h3>Copying files to /usr/local/hive directory</h3>
<p>We need to copy the files from the super user “su -”. The following commands are used to copy the files from the extracted directory to the /usr/local/hive” directory.</p>
<pre class="prettyprint notranslate">
$ su -
passwd:

# cd /home/user/Download
# mv apache-hive-0.14.0-bin /usr/local/hive
# exit
</pre>
<h3>Setting up environment for Hive</h3>
<p>You can set up the Hive environment by appending the following lines to <b>~/.bashrc</b> file:</p>
<pre class="prettyprint notranslate">
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:/usr/local/Hadoop/lib/*:.
export CLASSPATH=$CLASSPATH:/usr/local/hive/lib/*:.
</pre>
<p>The following command is used to execute ~/.bashrc file.</p>
<pre class="prettyprint notranslate">
$ source ~/.bashrc
</pre>
<h2>Step 5: Configuring Hive</h2>
<p>To configure Hive with Hadoop, you need to edit the <b>hive-env.sh</b> file, which is placed in the <b>$HIVE_HOME/conf</b> directory. The following commands redirect to Hive <b>config</b> folder and copy the template file:</p>
<pre class="prettyprint notranslate">
$ cd $HIVE_HOME/conf
$ cp hive-env.sh.template hive-env.sh
</pre>
<p>Edit the <b>hive-env.sh</b> file by appending the following line:</p>
<pre class="prettyprint notranslate">
export HADOOP_HOME=/usr/local/hadoop
</pre>
<p>Hive installation is completed successfully. Now you require an external database server to configure Metastore. We use Apache Derby database.</p>
<h2>Step 6: Downloading and Installing Apache Derby</h2>
<p>Follow the steps given below to download and install Apache Derby:</p>
<h3>Downloading Apache Derby</h3>
<p>The following command is used to download Apache Derby. It takes some time to download.</p>
<pre class="prettyprint notranslate">
$ cd ~
$ wget http://archive.apache.org/dist/db/derby/db-derby-10.4.2.0/db-derby-10.4.2.0-bin.tar.gz
</pre>
<p>The following command is used to verify the download:</p>
<pre class="prettyprint notranslate">
$ ls
</pre>
<p>On successful download, you get to see the following response:</p>
<pre class="prettyprint notranslate">
db-derby-10.4.2.0-bin.tar.gz
</pre>
<h3>Extracting and verifying Derby archive</h3>
<p>The following commands are used for extracting and verifying the Derby archive:</p>
<pre class="prettyprint notranslate">
$ tar zxvf db-derby-10.4.2.0-bin.tar.gz
$ ls
</pre>
<p>On successful download, you get to see the following response:</p>
<pre class="prettyprint notranslate">
db-derby-10.4.2.0-bin db-derby-10.4.2.0-bin.tar.gz
</pre>
<h3>Copying files to /usr/local/derby directory</h3>
<p>We need to copy from the super user “su -”. The following commands are used to copy the files from the extracted directory to the /usr/local/derby directory:</p>
<pre class="prettyprint notranslate">
$ su -
passwd:
# cd /home/user
# mv db-derby-10.4.2.0-bin /usr/local/derby
# exit
</pre>
<h3>Setting up environment for Derby</h3>
<p>You can set up the Derby environment by appending the following lines to <b>~/.bashrc</b> file:</p>
<pre class="prettyprint notranslate">
export DERBY_HOME=/usr/local/derby
export PATH=$PATH:$DERBY_HOME/bin
Apache Hive
18
export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar
</pre>
<p>The following command is used to execute <b>~/.bashrc</b> file:</p>
<pre class="prettyprint notranslate">
$ source ~/.bashrc
</pre>
<h3>Create a directory to store Metastore</h3>
<p>Create a directory named data in $DERBY_HOME directory to store Metastore data.</p>
<pre class="prettyprint notranslate">
$ mkdir $DERBY_HOME/data
</pre>
<p>Derby installation and environmental setup is now complete.</p>
<h2>Step 7: Configuring Metastore of Hive</h2>
<p>Configuring Metastore means specifying to Hive where the database is stored. You can do this by editing the hive-site.xml file, which is in the $HIVE_HOME/conf directory. First of all, copy the template file using the following command:</p>
<pre class="prettyprint notranslate">
$ cd $HIVE_HOME/conf
$ cp hive-default.xml.template hive-site.xml
</pre>
<p>Edit <b>hive-site.xml</b> and append the following lines between the &lt;configuration&gt; and &lt;/configuration&gt; tags:</p>
<pre class="prettyprint notranslate">
&lt;property&gt;
   &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
   &lt;value&gt;jdbc:derby://localhost:1527/metastore_db;create=true &lt;/value&gt;
   &lt;description&gt;JDBC connect string for a JDBC metastore &lt;/description&gt;
&lt;/property&gt;
</pre>
<p>Create a file named jpox.properties and add the following lines into it:</p>
<pre class="prettyprint notranslate">
javax.jdo.PersistenceManagerFactoryClass =

org.jpox.PersistenceManagerFactoryImpl
org.jpox.autoCreateSchema = false
org.jpox.validateTables = false
org.jpox.validateColumns = false
org.jpox.validateConstraints = false
org.jpox.storeManagerType = rdbms
org.jpox.autoCreateSchema = true
org.jpox.autoStartMechanismMode = checked
org.jpox.transactionIsolation = read_committed
javax.jdo.option.DetachAllOnCommit = true
javax.jdo.option.NontransactionalRead = true
javax.jdo.option.ConnectionDriverName = org.apache.derby.jdbc.ClientDriver
javax.jdo.option.ConnectionURL = jdbc:derby://hadoop1:1527/metastore_db;create = true
javax.jdo.option.ConnectionUserName = APP
javax.jdo.option.ConnectionPassword = mine
</pre>
<h2>Step 8: Verifying Hive Installation</h2>
<p>Before running Hive, you need to create the <b>/tmp</b> folder and a separate Hive folder in HDFS. Here, we use the <b>/user/hive/warehouse</b> folder. You need to set write permission for these newly created folders as shown below:</p>
<pre class="prettyprint notranslate">
chmod g+w
</pre>
<p>Now set them in HDFS before verifying Hive. Use the following commands:</p>
<pre class="prettyprint notranslate">
$ $HADOOP_HOME/bin/hadoop fs -mkdir /tmp 
$ $HADOOP_HOME/bin/hadoop fs -mkdir /user/hive/warehouse
$ $HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp 
$ $HADOOP_HOME/bin/hadoop fs -chmod g+w /user/hive/warehouse
</pre>
<p>The following commands are used to verify Hive installation:</p>
<pre class="prettyprint notranslate">
$ cd $HIVE_HOME
$ bin/hive
</pre>
<p>On successful installation of Hive, you get to see the following response:</p>
<pre class="prettyprint notranslate">
Logging initialized using configuration in jar:file:/home/hadoop/hive-0.9.0/lib/hive-common-0.9.0.jar!/hive-log4j.properties 
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201312121621_1494929084.txt
………………….
hive>
</pre>
<p>The following sample command is executed to display all the tables:</p>
<pre class="prettyprint notranslate">
hive> show tables; 
OK 
Time taken: 2.798 seconds 
hive>
</pre>
<h1>Hive - Data Types</h1>
<p>This chapter takes you through the different data types in Hive, which are involved in the table creation. All the data types in Hive are classified into four types, given as follows:</p>
<ul class="list">
<li>Column Types</li>
<li>Literals</li>
<li>Null Values</li>
<li>Complex Types</li>
</ul>
<h2>Column Types</h2>
<p>Column type are used as column data types of Hive. They are as follows:</p>
<h3>Integral Types</h3>
<p>Integer type data can be specified using integral data types, INT. When the data range exceeds the range of INT, you need to use BIGINT and if the data range is smaller than the INT, you use SMALLINT. TINYINT is smaller than SMALLINT.</p>
<p>The following table depicts various INT data types:</p>
<table class="table table-bordered">
<tr>
<th>Type</th>
<th>Postfix</th>
<th>Example</th>
</tr>
<tr>
<td>TINYINT</td>
<td>Y</td>
<td>10Y</td>
</tr>
<tr>
<td>SMALLINT</td>
<td>S</td>
<td>10S</td>
</tr>
<tr>
<td>INT</td>
<td>-</td>
<td>10</td>
</tr>
<tr>
<td>BIGINT</td>
<td>L</td>
<td>10L</td>
</tr>
</table>
<h3>String Types</h3>
<p>String type data types can be specified using single quotes (' ') or double quotes (" "). It contains two data types: VARCHAR and CHAR. Hive follows C-types escape characters.</p>
<p>The following table depicts various CHAR data types:</p>
<table class="table table-bordered">
<tr>
<th style="width:30%">Data Type</th>
<th>Length</th>
</tr>
<tr>
<td>VARCHAR</td>
<td>1 to 65355</td>
</tr>
<tr>
<td>CHAR</td>
<td>255</td>
</tr>
</table>
<h3>Timestamp</h3>
<p>It supports traditional UNIX timestamp with optional nanosecond precision. It supports java.sql.Timestamp format “YYYY-MM-DD HH:MM:SS.fffffffff” and format “yyyy-mm-dd hh:mm:ss.ffffffffff”.</p>
<h3>Dates</h3>
<p>DATE values are described in year/month/day format in the form {{YYYY-MM-DD}}.</p>
<h3>Decimals</h3>
<p>The DECIMAL type in Hive is as same as Big Decimal format of Java. It is used for representing immutable arbitrary precision. The syntax and example is as follows:</p>
<pre class="prettyprint notranslate">
DECIMAL(precision, scale)
decimal(10,0)
</pre>
<h3>Union Types</h3>
<p>Union is a collection of heterogeneous data types. You can create an instance using <b>create union</b>. The syntax and example is as follows:</p>
<pre class="prettyprint notranslate">
UNIONTYPE&lt;int, double, array&lt;string&gt;, struct&lt;a:int,b:string&gt;&gt;

{0:1} 
{1:2.0} 
{2:["three","four"]} 
{3:{"a":5,"b":"five"}} 
{2:["six","seven"]} 
{3:{"a":8,"b":"eight"}} 
{0:9} 
{1:10.0}
</pre>
<h2>Literals</h2>
<p>The following literals are used in Hive:</p>
<h3>Floating Point Types</h3>
<p>Floating point types are nothing but numbers with decimal points. Generally, this type of data is composed of DOUBLE data type.</p>
<h3>Decimal Type</h3>
Decimal type data is nothing but floating point value with higher range than DOUBLE data type. The range of decimal type is approximately -10<sup>-308</sup> to 10<sup>308</sup>.
<h2>Null Value</h2>
<p>Missing values are represented by the special value NULL.</p>
<h2>Complex Types</h2>
<p>The Hive complex data types are as follows:</p>
<h3>Arrays</h3>
<p>Arrays in Hive are used the same way they are used in Java.</p>
<p>Syntax: ARRAY&lt;data_type&gt;</p>
<h3>Maps</h3>
<p>Maps in Hive are similar to Java Maps.</p>
<p>Syntax: MAP&lt;primitive_type, data_type&gt;</p>
<h3>Structs</h3>
<p>Structs in Hive is similar to using complex data with comment.</p>
<p>Syntax: STRUCT&lt;col_name : data_type [COMMENT col_comment], ...&gt;</p>
<h1>Hive - Create Database</h1>
<p>Hive is a database technology that can define databases and tables to analyze structured data. The theme for structured data analysis is to store the data in a tabular manner, and pass queries to analyze it. This chapter explains how to create Hive database. Hive contains a default database named <b>default</b>.</p>
<h2>Create Database Statement</h2>
<p>Create Database is a statement used to create a database in Hive. A database in Hive is a <b>namespace</b> or a collection of tables. The <b>syntax</b> for this statement is as follows:</p>
<pre class="prettyprint notranslate">
CREATE DATABASE|SCHEMA [IF NOT EXISTS] &lt;database name&gt;
</pre>
<p>Here, IF NOT EXISTS is an optional clause, which notifies the user that a database with the same name already exists. We can use SCHEMA in place of DATABASE in this command. The following query is executed to create a database named <b>userdb</b>:</p>
<pre class="prettyprint notranslate">
hive> CREATE DATABASE [IF NOT EXISTS] userdb;
</pre>
<p><b>or</b></p>
<pre class="prettyprint notranslate">
hive> CREATE SCHEMA userdb;
</pre>
<p>The following query is used to verify a databases list:</p>
<pre class="prettyprint notranslate">
hive> SHOW DATABASES;
default
userdb
</pre>
<h3>JDBC Program</h3>
<p>The JDBC program to create a database is given below.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet; 4. CREATE DATABASE
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveCreateDb {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/default", "", "");
      Statement stmt = con.createStatement();
      stmt.executeQuery("CREATE DATABASE userdb");
      System.out.println(“Database userdb created successfully.”);
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveCreateDb.java. The following commands are used to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveCreateDb.java
$ java HiveCreateDb
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
Database userdb created successfully.
</pre>
<h1>Hive - Drop Database</h1>
<p>This chapter describes how to drop a database in Hive. The usage of SCHEMA and DATABASE are same.</p>
<h2>Drop Database Statement</h2>
<p>Drop Database is a statement that drops all the tables and deletes the database. Its syntax is as follows:</p>
<pre class="prettyprint notranslate">
DROP DATABASE StatementDROP (DATABASE|SCHEMA) [IF EXISTS] database_name 
[RESTRICT|CASCADE];
</pre>
<p>The following queries are used to drop a database. Let us assume that the database name is <b>userdb</b>.</p>
<pre class="prettyprint notranslate">
hive> DROP DATABASE IF EXISTS userdb;
</pre>
<p>The following query drops the database using <b>CASCADE</b>. It means dropping respective tables before dropping the database.</p>
<pre class="prettyprint notranslate">
hive> DROP DATABASE IF EXISTS userdb CASCADE;
</pre>
<p>The following query drops the database using <b>SCHEMA</b>.</p>
<pre class="prettyprint notranslate">
hive> DROP SCHEMA userdb;
</pre>
<p>This clause was added in Hive 0.6.</p>
<h3>JDBC Program</h3>
<p>The JDBC program to drop a database is given below.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager; 5. DROP DATABASE

public class HiveDropDb {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/default", "", "");
      Statement stmt = con.createStatement();
      stmt.executeQuery("DROP DATABASE userdb");
      System.out.println(“Drop userdb database successful.”);
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveDropDb.java. Given below are the commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveDropDb.java
$ java HiveDropDb
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
Drop userdb database successful.
</pre>
<h1>Hive - Create Table</h1>
<p>This chapter explains how to create a table and how to insert data into it. The conventions of creating a table in HIVE is quite similar to creating a table using SQL.</p>
<h2>Create Table Statement</h2>
<p>Create Table is a statement used to create a table in Hive. The syntax and example are as follows:</p>
<h3>Syntax</h3>
<pre class="prettyprint notranslate">
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name

[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[ROW FORMAT row_format]
[STORED AS file_format]
</pre>
<h3>Example</h3>
<p>Let us assume you need to create a table named <b>employee</b> using <b>CREATE TABLE</b> statement. The following table lists the fields and their data types in employee table:</p>
<table class="table table-bordered">
<tr>
<th>Sr.No</th>
<th>Field Name</th>
<th>Data Type</th>
</tr>
<tr>
<td>1</td>
<td>Eid</td>
<td>int</td>
</tr>
<tr>
<td>2</td>
<td>Name</td>
<td>String</td>
</tr>
<tr>
<td>3</td>
<td>Salary</td>
<td>Float</td>
</tr>
<tr>
<td>4</td>
<td>Designation</td>
<td>string</td>
</tr>
</table>
<p>The following data is a Comment, Row formatted fields such as Field terminator, Lines terminator, and Stored File type.</p>
<pre class="prettyprint notranslate">
COMMENT ‘Employee details’
FIELDS TERMINATED BY ‘\t’
LINES TERMINATED BY ‘\n’
STORED IN TEXT FILE
</pre>
<p>The following query creates a table named <b>employee</b> using the above data.</p>
<pre class="prettyprint notranslate">
hive> CREATE TABLE IF NOT EXISTS employee ( eid int, name String,
> salary String, destination String)
> COMMENT ‘Employee details’
> ROW FORMAT DELIMITED
> FIELDS TERMINATED BY ‘\t’
> LINES TERMINATED BY ‘\n’
> STORED AS TEXTFILE;
</pre>
<p>If you add the option IF NOT EXISTS, Hive ignores the statement in case the table already exists.</p>
<p>On successful creation of table, you get to see the following response:</p>
<pre class="prettyprint notranslate">
OK
Time taken: 5.905 seconds
hive>
</pre>
<h3>JDBC Program</h3>
<p>The JDBC program to create a table is given example.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveCreateTable {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement
      Statement stmt = con.createStatement();
      // execute statement
      stmt.executeQuery("CREATE TABLE IF NOT EXISTS "
      +" employee ( eid int, name String, "
      +" salary String, destignation String)"
      +" COMMENT ‘Employee details’"
      +" ROW FORMAT DELIMITED"
      +" FIELDS TERMINATED BY ‘\t’"
      +" LINES TERMINATED BY ‘\n’"
      +" STORED AS TEXTFILE;");
      System.out.println(“ Table employee created.”);
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveCreateDb.java. The following commands are used to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveCreateDb.java
$ java HiveCreateDb
</pre>
<h3>Output</h3>
<pre class="result notranslate">
Table employee created.
</pre>
<h2>Load Data Statement</h2>
<p>Generally, after creating a table in SQL, we can insert data using the Insert statement. But in Hive, we can insert data using the LOAD DATA statement.</p>
<p>While inserting data into Hive, it is better to use LOAD DATA to store bulk records. There are two ways to load data: one is from local file system and second is from Hadoop file system.</p>
<h3>Syntex</h3>
<p>The syntax for load data is as follows:</p>
<pre class="prettyprint notranslate">
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename 
[PARTITION (partcol1=val1, partcol2=val2 ...)]
</pre>
<ul class="list">
<li>LOCAL is identifier to specify the local path. It is optional.</li>
<li>OVERWRITE is optional to overwrite the data in the table.</li>
<li>PARTITION is optional.</li>
</ul>
<h3>Example</h3>
<p>We will insert the following data into the table. It is a text file named <b>sample.txt</b> in <b>/home/user</b> directory.</p>
<pre class="prettyprint notranslate">
1201  Gopal       45000    Technical manager
1202  Manisha     45000    Proof reader
1203  Masthanvali 40000    Technical writer
1204  Krian       40000    Hr Admin
1205  Kranthi     30000    Op Admin
</pre>
<p>The following query loads the given text into the table.</p>
<pre class="prettyprint notranslate">
hive> LOAD DATA LOCAL INPATH '/home/user/sample.txt'
> OVERWRITE INTO TABLE employee;
</pre>
<p>On successful download, you get to see the following response:</p>
<pre class="prettyprint notranslate">
OK
Time taken: 15.905 seconds
hive>
</pre>
<h3>JDBC Program</h3>
<p>Given below is the JDBC program to load given data into the table.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveLoadData {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement
      Statement stmt = con.createStatement();
      // execute statement
      stmt.executeQuery("LOAD DATA LOCAL INPATH '/home/user/sample.txt'"
      +"OVERWRITE INTO TABLE employee;");
      System.out.println("Load Data into employee successful");
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveLoadData.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveLoadData.java
$ java HiveLoadData
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
Load Data into employee successful
</pre>
<h1>Hive - Alter Table</h1>
<p>This chapter explains how to alter the attributes of a table such as changing its table name, changing column names, adding columns, and deleting or replacing columns.</p>
<h2>Alter Table Statement</h2>
<p>It is used to alter a table in Hive.</p>
<h3>Syntax</h3>
<p>The statement takes any of the following syntaxes based on what attributes we wish to modify in a table.</p>
<pre class="prettyprint notranslate">
ALTER TABLE name RENAME TO new_name
ALTER TABLE name ADD COLUMNS (col_spec[, col_spec ...])
ALTER TABLE name DROP [COLUMN] column_name
ALTER TABLE name CHANGE column_name new_name new_type
ALTER TABLE name REPLACE COLUMNS (col_spec[, col_spec ...])
</pre>
<h2>Rename To… Statement</h2>
<p>The following query renames the table from <b>employee</b> to <b>emp</b>.</p>
<pre class="prettyprint notranslate">
hive> ALTER TABLE employee RENAME TO emp;
</pre>
<h3>JDBC Program</h3>
<p>The JDBC program to rename a table is as follows.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet; 
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveAlterRenameTo {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement
      Statement stmt = con.createStatement();
      // execute statement
      stmt.executeQuery("ALTER TABLE employee RENAME TO emp;");
      System.out.println("Table Renamed Successfully");
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveAlterRenameTo.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveAlterRenameTo.java
$ java HiveAlterRenameTo
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
Table renamed successfully.
</pre>
<h2>Change Statement</h2>
<p>The following table contains the fields of <b>employee</b> table and it shows the fields to be changed (in bold).</p>
<table class="table table-bordered">
<tr>
<th>Field Name</th>
<th>Convert from Data Type</th>
<th>Change Field Name</th>
<th>Convert to Data Type</th>
</tr>
<tr>
<td>eid</td>
<td>int</td>
<td>eid</td>
<td>int</td>
</tr>
<tr>
<td><b>name</b></td>
<td>String</td>
<td><b>ename</b></td>
<td>String</td>
</tr>
<tr>
<td>salary</td>
<td><b>Float</b></td>
<td>salary</td>
<td><b>Double</b></td>
</tr>
<tr>
<td>designation</td>
<td>String</td>
<td>designation</td>
<td>String</td>
</tr>
</table>
<p>The following queries rename the column name and column data type using the above data:</p>
<pre class="prettyprint notranslate">
hive> ALTER TABLE employee CHANGE name ename String;
hive> ALTER TABLE employee CHANGE salary salary Double;
</pre>
<h3>JDBC Program</h3>
<p>Given below is the JDBC program to change a column.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveAlterChangeColumn {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement
      Statement stmt = con.createStatement();
      // execute statement
      stmt.executeQuery("ALTER TABLE employee CHANGE name ename String;");
      stmt.executeQuery("ALTER TABLE employee CHANGE salary salary Double;");
      System.out.println("Change column successful.");
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveAlterChangeColumn.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveAlterChangeColumn.java
$ java HiveAlterChangeColumn
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
Change column successful.
</pre>
<h2>Add Columns Statement</h2>
<p>The following query adds a column named dept to the employee table.</p>
<pre class="prettyprint notranslate">
hive> ALTER TABLE employee ADD COLUMNS ( 
   > dept STRING COMMENT 'Department name');
</pre>
<h3>JDBC Program</h3>
<p>The JDBC program to add a column to a table is given below.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveAlterAddColumn {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
     // create statement
     Statement stmt = con.createStatement();
     // execute statement
     stmt.executeQuery("ALTER TABLE employee ADD COLUMNS "
     +" (dept STRING COMMENT 'Department name');");
     System.out.prinln("Add column successful.");
     con.close();
   }
}
</pre>
<p>Save the program in a file named HiveAlterAddColumn.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveAlterAddColumn.java
$ java HiveAlterAddColumn
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
Add column successful.
</pre>
<h2>Replace Statement</h2>
<p>The following query deletes all the columns from the <b>employee</b> table and replaces it with <b>emp</b> and <b>name</b> columns:</p>
<pre class="prettyprint notranslate">
hive> ALTER TABLE employee REPLACE COLUMNS ( 
   > eid INT empid Int, 
   > ename STRING name String);
</pre>
<h3>JDBC Program</h3>
<p>Given below is the JDBC program to replace <b>eid</b> column with <b>empid</b> and <b>ename </b>column with <b>name</b>.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveAlterReplaceColumn {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement
      Statement stmt = con.createStatement();
      // execute statement
      stmt.executeQuery("ALTER TABLE employee REPLACE COLUMNS "
      +" (eid INT empid Int,"
      +" ename STRING name String);");
      System.out.println(" Replace column successful");
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveAlterReplaceColumn.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveAlterReplaceColumn.java
$ java HiveAlterReplaceColumn
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
Replace column successful.
</pre>
<h1>Hive - Drop Table</h1>
<p>This chapter describes how to drop a table in Hive. When you drop a table from Hive Metastore, it removes the table/column data and their metadata. It can be a normal table (stored in Metastore) or an external table (stored in local file system); Hive treats both in the same manner, irrespective of their types.</p>
<h2>Drop Table Statement</h2>
<p>The syntax is as follows:</p>
<pre class="prettyprint notranslate">
DROP TABLE [IF EXISTS] table_name;
</pre>
<p>The following query drops a table named <b>employee</b>:</p>
<pre class="prettyprint notranslate">
hive> DROP TABLE IF EXISTS employee;
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
OK
Time taken: 5.3 seconds
hive>
</pre>
<h3>JDBC Program</h3>
<p>The following JDBC program drops the employee table.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveDropTable {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement
      Statement stmt = con.createStatement();
      // execute statement
      stmt.executeQuery("DROP TABLE IF EXISTS employee;");
     System.out.println("Drop table successful.");
     con.close();
   }
}
</pre>
<p>Save the program in a file named HiveDropTable.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveDropTable.java
$ java HiveDropTable
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
Drop table successful
</pre>
<p>The following query is used to verify the list of tables:</p>
<pre class="prettyprint notranslate">
hive> SHOW TABLES;
emp
ok
Time taken: 2.1 seconds
hive>
</pre>
<h1>Hive - Partitioning</h1>
<p>Hive organizes tables into partitions. It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data.</p>
<p>Tables or partitions are sub-divided into <b>buckets,</b> to provide extra structure to the data that may be used for more efficient querying. Bucketing works based on the value of hash function of some column of a table.</p>
<p>For example, a table named <b>Tab1</b> contains employee data such as id, name, dept, and yoj (i.e., year of joining). Suppose you need to retrieve the details of all employees who joined in 2012. A query searches the whole table for the required information. However, if you partition the employee data with the year and store it in a separate file, it reduces the query processing time. The following example shows how to partition a file and its data:</p>
<p>The following file contains employeedata table.</p>
<p>/tab1/employeedata/file1</p>
<p>id, name, dept, yoj</p>
<p>1, gopal, TP, 2012</p>
<p>2, kiran, HR, 2012</p>
<p>3, kaleel,SC, 2013</p>
<p>4, Prasanth, SC, 2013</p>
<p>&nbsp;</p>
<p>The above data is partitioned into two files using year.</p>
<p>/tab1/employeedata/2012/file2</p>
<p>1, gopal, TP, 2012</p>
<p>2, kiran, HR, 2012</p>
<p>&nbsp;</p>
<p>/tab1/employeedata/2013/file3</p>
<p>3, kaleel,SC, 2013</p>
<p>4, Prasanth, SC, 2013</p>
<h2>Adding a Partition</h2>
<p>We can add partitions to a table by altering the table. Let us assume we have a table called <b>employee</b> with fields such as Id, Name, Salary, Designation, Dept, and yoj.</p>
<h3>Syntax:</h3>
<pre class="prettyprint notranslate">
ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec
[LOCATION 'location1'] partition_spec [LOCATION 'location2'] ...;

partition_spec:
: (p_column = p_col_value, p_column = p_col_value, ...)
</pre>
<p>The following query is used to add a partition to the employee table.</p>
<pre class="prettyprint notranslate">
hive> ALTER TABLE employee
> ADD PARTITION (year=’2012’)
> location '/2012/part2012';
</pre>
<h2>Renaming a Partition</h2>
<p>The syntax of this command is as follows.</p>
<pre class="prettyprint notranslate">
ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec;
</pre>
<p>The following query is used to rename a partition:</p>
<pre class="prettyprint notranslate">
hive> ALTER TABLE employee PARTITION (year=’1203’)
   > RENAME TO PARTITION (Yoj=’1203’);
</pre>
<h2>Dropping a Partition</h2>
<p>The following syntax is used to drop a partition:</p>
<pre class="prettyprint notranslate">
ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec, PARTITION partition_spec,...;
</pre>
<p>The following query is used to drop a partition:</p>
<pre class="prettyprint notranslate">
hive> ALTER TABLE employee DROP [IF EXISTS]
   > PARTITION (year=’1203’);
</pre>
<h1>Hive - Built-in Operators</h1>
<p>This chapter explains the built-in operators of Hive. There are four types of operators in Hive:</p>
<ul class="lsit">
<li>Relational Operators</li>
<li>Arithmetic Operators</li>
<li>Logical Operators</li>
<li>Complex Operators</li>
</ul>
<h2>Relational Operators</h2>
<p>These operators are used to compare two operands. The following table describes the relational operators available in Hive:</p>
<table class="table table-bordered">
<tr>
<th style="width:20%">Operator</th>
<th style="width:18%">Operand</th>
<th>Description</th>
</tr>
<tr>
<td>A = B</td>
<td>all primitive types</td>
<td>TRUE if expression A is equivalent to expression B otherwise FALSE.</td>
</tr>
<tr>
<td>A != B</td>
<td>all primitive types</td>
<td>TRUE if expression A is not equivalent to expression B otherwise FALSE.</td>
</tr>
<tr>
<td>A &lt; B</td>
<td>all primitive types</td>
<td>TRUE if expression A is less than expression B otherwise FALSE.</td>
</tr>
<tr>
<td>A &lt;= B</td>
<td>all primitive types</td>
<td>TRUE if expression A is less than or equal to expression B otherwise FALSE.</td>
</tr>
<tr>
<td>A &gt; B</td>
<td>all primitive types</td>
<td>TRUE if expression A is greater than expression B otherwise FALSE.</td>
</tr>
<tr>
<td>A &gt;= B</td>
<td>all primitive types</td>
<td>TRUE if expression A is greater than or equal to expression B otherwise FALSE.</td>
</tr>
<tr>
<td>A IS NULL</td>
<td>all types</td>
<td>TRUE if expression A evaluates to NULL otherwise FALSE.</td>
</tr>
<tr>
<td>A IS NOT NULL</td>
<td>all types</td>
<td>FALSE if expression A evaluates to NULL otherwise TRUE.</td>
</tr>
<tr>
<td>A LIKE B</td>
<td>Strings</td>
<td>TRUE if string pattern A matches to B otherwise FALSE.</td>
</tr>
<tr>
<td>A RLIKE B</td>
<td>Strings</td>
<td>NULL if A or B is NULL, TRUE if any substring of A matches the Java regular expression B , otherwise FALSE.</td>
</tr>
<tr>
<td>A REGEXP B</td>
<td>Strings</td>
<td>Same as RLIKE.</td>
</tr>
</table>
<h3>Example</h3>
<p>Let us assume the <b>employee</b> table is composed of fields named Id, Name, Salary, Designation, and Dept as shown below. Generate a query to retrieve the employee details whose Id is 1205.</p>
<pre class="prettyprint notranslate">
+-----+--------------+--------+---------------------------+------+
| Id  | Name         | Salary | Designation               | Dept |
+-----+--------------+------------------------------------+------+
|1201 | Gopal        | 45000  | Technical manager         | TP   |
|1202 | Manisha      | 45000  | Proofreader               | PR   |
|1203 | Masthanvali  | 40000  | Technical writer          | TP   |
|1204 | Krian        | 40000  | Hr Admin                  | HR   |
|1205 | Kranthi      | 30000  | Op Admin                  | Admin|
+-----+--------------+--------+---------------------------+------+
</pre>
<p>The following query is executed to retrieve the employee details using the above table:</p>
<pre class="prettyprint notranslate">
hive> SELECT * FROM employee WHERE Id=1205;
</pre>
<p>On successful execution of query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+-----+-----------+-----------+----------------------------------+
| ID  | Name      | Salary    | Designation              | Dept  |
+-----+---------------+-------+----------------------------------+
|1205 | Kranthi   | 30000     | Op Admin                 | Admin |
+-----+-----------+-----------+----------------------------------+
</pre>
<p>The following query is executed to retrieve the employee details whose salary is more than or equal to Rs 40000.</p>
<pre class="prettyprint notranslate">
hive> SELECT * FROM employee WHERE Salary>=40000;
</pre>
<p>On successful execution of query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+-----+------------+--------+----------------------------+------+
| ID  | Name       | Salary | Designation                | Dept |
+-----+------------+--------+----------------------------+------+
|1201 | Gopal      | 45000  | Technical manager          | TP   |
|1202 | Manisha    | 45000  | Proofreader                | PR   |
|1203 | Masthanvali| 40000  | Technical writer           | TP   |
|1204 | Krian      | 40000  | Hr Admin                   | HR   |
+-----+------------+--------+----------------------------+------+
</pre>
<h2>Arithmetic Operators</h2>
<p>These operators support various common arithmetic operations on the operands. All of them return number types. The following table describes the arithmetic operators available in Hive:</p>
<table class="table table-bordered">
<tr>
<th>Operators</th>
<th>Operand</th>
<th>Description</th>
</tr>
<tr>
<td>A + B</td>
<td>all number types</td>
<td>Gives the result of adding A and B.</td>
</tr>
<tr>
<td>A - B</td>
<td>all number types</td>
<td>Gives the result of subtracting B from A.</td>
</tr>
<tr>
<td>A * B</td>
<td>all number types</td>
<td>Gives the result of multiplying A and B.</td>
</tr>
<tr>
<td>A / B</td>
<td>all number types</td>
<td>Gives the result of dividing B from A.</td>
</tr>
<tr>
<td>A % B</td>
<td>all number types</td>
<td>Gives the reminder resulting from dividing A by B.</td>
</tr>
<tr>
<td>A & B</td>
<td>all number types</td>
<td>Gives the result of bitwise AND of A and B.</td>
</tr>
<tr>
<td>A | B</td>
<td>all number types</td>
<td>Gives the result of bitwise OR of A and B.</td>
</tr>
<tr>
<td>A ^ B</td>
<td>all number types</td>
<td>Gives the result of bitwise XOR of A and B.</td>
</tr>
<tr>
<td>~A</td>
<td>all number types</td>
<td>Gives the result of bitwise NOT of A.</td>
</tr>
</table>
<h3>Example</h3>
<p>The following query adds two numbers, 20 and 30.</p>
<pre class="prettyprint notranslate">
hive> SELECT 20+30 ADD FROM temp;
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+--------+
|   ADD  |
+--------+
|   50   |
+--------+
</pre>
<h2>Logical Operators</h2>
<p>The operators are logical expressions. All of them return either TRUE or FALSE.</p>
<table class="table table-bordered">
<tr>
<th>Operators</th>
<th>Operands</th>
<th>Description</th>
</tr>
<tr>
<td>A AND B</td>
<td>boolean</td>
<td>TRUE if both A and B are TRUE, otherwise FALSE.</td>
</tr>
<tr>
<td>A &amp;&amp; B</td>
<td>boolean</td>
<td>Same as A AND B.</td>
</tr>
<tr>
<td>A OR B</td>
<td>boolean</td>
<td>TRUE if either A or B or both are TRUE, otherwise FALSE.</td>
</tr>
<tr>
<td>A || B</td>
<td>boolean</td>
<td>Same as A OR B.</td>
</tr>
<tr>
<td>NOT A</td>
<td>boolean</td>
<td>TRUE if A is FALSE, otherwise FALSE.</td>
</tr>
<tr>
<td>!A</td>
<td>boolean</td>
<td>Same as NOT A.</td>
</tr>
</table>
<h3>Example</h3>
<p>The following query is used to retrieve employee details whose Department is TP and Salary is more than Rs 40000.</p>
<pre class="prettyprint notranslate">
hive> SELECT * FROM employee WHERE Salary>40000 && Dept=TP;
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+------+--------------+-------------+-------------------+--------+
| ID   | Name         | Salary      | Designation       | Dept   |
+------+--------------+-------------+-------------------+--------+
|1201  | Gopal        | 45000       | Technical manager | TP     |
+------+--------------+-------------+-------------------+--------+
</pre>
<h2>Complex Operators</h2>
<p>These operators provide an expression to access the elements of Complex Types.</p>
<table class="table table-bordered">
<tr>
<th>Operator</th>
<th>Operand</th>
<th>Description</th>
</tr>
<tr>
<td>A[n]</td>
<td>A is an Array and n is an int</td>
<td>It returns the nth element in the array A. The first element has index 0.</td>
</tr>
<tr>
<td>M[key]</td>
<td>M is a Map&lt;K, V&gt; and key has type K</td>
<td>It returns the value corresponding to the key in the map.</td>
</tr>
<tr>
<td>S.x</td>
<td>S is a struct</td>
<td>It returns the x field of S.</td>
</tr>
</table>
<h1>Hiveql Select...Where</h1>
<p>The Hive Query Language (HiveQL) is a query language for Hive to process and analyze structured data in a Metastore. This chapter explains how to use the SELECT statement with WHERE clause.</p>
<p>SELECT statement is used to retrieve the data from a table. WHERE clause works similar to a condition. It filters the data using the condition and gives you a finite result. The built-in operators and functions generate an expression, which fulfils the condition.</p>
<h2>Syntax</h2>
<p>Given below is the syntax of the SELECT query:</p>
<pre class="prettyprint notranslate">
SELECT [ALL | DISTINCT] select_expr, select_expr, ... 
FROM table_reference 
[WHERE where_condition] 
[GROUP BY col_list] 
[HAVING having_condition] 
[CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]] 
[LIMIT number];
</pre>
<h2>Example</h2>
<p>Let us take an example for SELECT…WHERE clause. Assume we have the employee table as given below, with fields named Id, Name, Salary, Designation, and Dept. Generate a query to retrieve the employee details who earn a salary of more than Rs 30000.</p>
<pre class="prettyprint notranslate">
+------+--------------+-------------+-------------------+--------+
| ID   | Name         | Salary      | Designation       | Dept   |
+------+--------------+-------------+-------------------+--------+
|1201  | Gopal        | 45000       | Technical manager | TP     |
|1202  | Manisha      | 45000       | Proofreader       | PR     |
|1203  | Masthanvali  | 40000       | Technical writer  | TP     |
|1204  | Krian        | 40000       | Hr Admin          | HR     |
|1205  | Kranthi      | 30000       | Op Admin          | Admin  | 
+------+--------------+-------------+-------------------+--------+
</pre>
<p>The following query retrieves the employee details using the above scenario:</p>
<pre class="prettyprint notranslate">
hive> SELECT * FROM employee WHERE salary>30000;
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+------+--------------+-------------+-------------------+--------+
| ID   | Name         | Salary      | Designation       | Dept   |
+------+--------------+-------------+-------------------+--------+
|1201  | Gopal        | 45000       | Technical manager | TP     |
|1202  | Manisha      | 45000       | Proofreader       | PR     |
|1203  | Masthanvali  | 40000       | Technical writer  | TP     |
|1204  | Krian        | 40000       | Hr Admin          | HR     |
+------+--------------+-------------+-------------------+--------+
</pre>
<h3>JDBC Program</h3>
<p>The JDBC program to apply where clause for the given example is as follows.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveQLWhere {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement
      Statement stmt = con.createStatement();
      // execute statement
      Resultset res = stmt.executeQuery("SELECT * FROM employee WHERE
      salary>30000;");
      System.out.println("Result:");
      System.out.println(" ID \t Name \t Salary \t Designation \t Dept ");
      while (res.next()) {
         System.out.println(res.getInt(1)+" "+ res.getString(2)+" "+
         res.getDouble(3)+" "+ res.getString(4)+" "+ res.getString(5));
      }
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveQLWhere.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveQLWhere.java
$ java HiveQLWhere
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
ID       Name           Salary      Designation          Dept
1201     Gopal          45000       Technical manager    TP
1202     Manisha        45000       Proofreader          PR
1203     Masthanvali    40000       Technical writer     TP
1204     Krian          40000       Hr Admin             HR
</pre>
<h1>Hiveql Select...Order By</h1>
<p>This chapter explains how to use the ORDER BY clause in a SELECT statement. The ORDER BY clause is used to retrieve the details based on one column and sort the result set by ascending or descending order.</p>
<h2>Syntax</h2>
<p>Given below is the syntax of the ORDER BY clause:</p>
<pre class="prettyprint notranslate">
SELECT [ALL | DISTINCT] select_expr, select_expr, ... 
FROM table_reference 
[WHERE where_condition] 
[GROUP BY col_list] 
[HAVING having_condition] 
[ORDER BY col_list]] 
[LIMIT number];
</pre>
<h2>Example</h2>
<p>Let us take an example for SELECT...ORDER BY clause. Assume employee table as given below, with the fields named Id, Name, Salary, Designation, and Dept. Generate a query to retrieve the employee details in order by using Department name.</p>
<pre class="prettyprint notranslate">
+------+--------------+-------------+-------------------+--------+
| ID   | Name         | Salary      | Designation       | Dept   |
+------+--------------+-------------+-------------------+--------+
|1201  | Gopal        | 45000       | Technical manager | TP     |
|1202  | Manisha      | 45000       | Proofreader       | PR     |
|1203  | Masthanvali  | 40000       | Technical writer  | TP     |
|1204  | Krian        | 40000       | Hr Admin          | HR     |
|1205  | Kranthi      | 30000       | Op Admin          | Admin  |
+------+--------------+-------------+-------------------+--------+
</pre>
<p>The following query retrieves the employee details using the above scenario:</p>
<pre class="prettyprint notranslate">
hive> SELECT Id, Name, Dept FROM employee ORDER BY DEPT;
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+------+--------------+-------------+-------------------+--------+
| ID   | Name         | Salary      | Designation       | Dept   |
+------+--------------+-------------+-------------------+--------+
|1205  | Kranthi      | 30000       | Op Admin          | Admin  |
|1204  | Krian        | 40000       | Hr Admin          | HR     |
|1202  | Manisha      | 45000       | Proofreader       | PR     |
|1201  | Gopal        | 45000       | Technical manager | TP     |
|1203  | Masthanvali  | 40000       | Technical writer  | TP     |
+------+--------------+-------------+-------------------+--------+
</pre>
<h3>JDBC Program</h3>
<p>Here is the JDBC program to apply Order By clause for the given example.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveQLOrderBy {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement 
      Statement stmt = con.createStatement();
      // execute statement
      Resultset res = stmt.executeQuery("SELECT * FROM employee ORDER BY
      DEPT;");
      System.out.println(" ID \t Name \t Salary \t Designation \t Dept ");
      while (res.next()) {
         System.out.println(res.getInt(1)+" "+ res.getString(2)+" "+
         res.getDouble(3)+" "+ res.getString(4)+" "+ res.getString(5));
      }
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveQLOrderBy.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveQLOrderBy.java
$ java HiveQLOrderBy
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
ID       Name           Salary      Designation          Dept
1205     Kranthi        30000       Op Admin             Admin
1204     Krian          40000       Hr Admin             HR
1202     Manisha        45000       Proofreader          PR
1201     Gopal          45000       Technical manager    TP
1203     Masthanvali    40000       Technical writer     TP
1204     Krian          40000       Hr Admin             HR
</pre>
<h1>Hiveql Group By</h1>
<p>This chapter explains the details of GROUP BY clause in a SELECT statement. The GROUP BY clause is used to group all the records in a result set using a particular collection column. It is used to query a group of records.</p>
<h2>Syntax</h2>
<p>The syntax of GROUP BY clause is as follows:</p>
<pre class="prettyprint notranslate">
SELECT [ALL | DISTINCT] select_expr, select_expr, ... 
FROM table_reference 
[WHERE where_condition] 
[GROUP BY col_list] 
[HAVING having_condition] 
[ORDER BY col_list]] 
[LIMIT number];
</pre>
<h2>Example</h2>
<p>Let us take an example of SELECT…GROUP BY clause. Assume employee table as given below, with Id, Name, Salary, Designation, and Dept fields. Generate a query to retrieve the number of employees in each department.</p>
<pre class="prettyprint notranslate">
+------+--------------+-------------+-------------------+--------+ 
| ID   | Name         | Salary      | Designation       | Dept   |
+------+--------------+-------------+-------------------+--------+ 
|1201  | Gopal        | 45000       | Technical manager | TP     | 
|1202  | Manisha      | 45000       | Proofreader       | PR     | 
|1203  | Masthanvali  | 40000       | Technical writer  | TP     | 
|1204  | Krian        | 45000       | Proofreader       | PR     | 
|1205  | Kranthi      | 30000       | Op Admin          | Admin  |
+------+--------------+-------------+-------------------+--------+
</pre>
<p>The following query retrieves the employee details using the above scenario.</p>
<pre class="prettyprint notranslate">
hive> SELECT Dept,count(*) FROM employee GROUP BY DEPT;
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+------+--------------+ 
| Dept | Count(*)     | 
+------+--------------+ 
|Admin |    1         | 
|PR    |    2         | 
|TP    |    3         | 
+------+--------------+
</pre>
<h3>JDBC Program</h3>
<p>Given below is the JDBC program to apply the Group By clause for the given example.</p>
<pre class="prettyprint notranslate">
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveQLGroupBy {
   private static String driverName =
   "org.apache.hadoop.hive.jdbc.HiveDriver";
   public static void main(String[] args) throws SQLException {
      // Register driver and create driver instance
      Class.forName(driverName);
      // get connection
      Connection con = DriverManager.
      getConnection("jdbc:hive://localhost:10000/userdb", "", "");
      // create statement
      Statement stmt = con.createStatement();
      // execute statement
      Resultset res = stmt.executeQuery(“SELECT Dept,count(*) ”
      +“FROM employee GROUP BY DEPT; ”);
      System.out.println(" Dept \t count(*)");
      while (res.next()) {
         System.out.println(res.getString(1)+" "+ res.getInt(2)); 
      }
      con.close();
   }
}
</pre>
<p>Save the program in a file named HiveQLGroupBy.java. Use the following commands to compile and execute this program.</p>
<pre class="prettyprint notranslate">
$ javac HiveQLGroupBy.java
$ java HiveQLGroupBy
</pre>
<h3>Output:</h3>
<pre class="result notranslate">
 Dept     Count(*)
 Admin       1
 PR          2
 TP          3
</pre>
<h1>Hiveql Joins</h1>
<p>JOINS is a clause that is used for combining specific fields from two tables by using values common to each one. It is used to combine records from two or more tables in the database.</p>
<h2>Syntax</h2>
<pre class="prettyprint notranslate">
join_table:

   table_reference JOIN table_factor [join_condition]
   | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference
   join_condition
   | table_reference LEFT SEMI JOIN table_reference join_condition
   | table_reference CROSS JOIN table_reference [join_condition]
</pre>
<h2>Example</h2>
<p>We will use the following two tables in this chapter. Consider the following table named CUSTOMERS..</p>
<pre class="prettyprint notranslate">
+----+----------+-----+-----------+----------+ 
| ID | NAME     | AGE | ADDRESS   | SALARY   | 
+----+----------+-----+-----------+----------+ 
| 1  | Ramesh   | 32  | Ahmedabad | 2000.00  |  
| 2  | Khilan   | 25  | Delhi     | 1500.00  |  
| 3  | kaushik  | 23  | Kota      | 2000.00  | 
| 4  | Chaitali | 25  | Mumbai    | 6500.00  | 
| 5  | Hardik   | 27  | Bhopal    | 8500.00  | 
| 6  | Komal    | 22  | MP        | 4500.00  | 
| 7  | Muffy    | 24  | Indore    | 10000.00 | 
+----+----------+-----+-----------+----------+
</pre>
<p>Consider another table ORDERS as follows:</p>
<pre class="prettyprint notranslate">
+-----+---------------------+-------------+--------+ 
|OID  | DATE                | CUSTOMER_ID | AMOUNT | 
+-----+---------------------+-------------+--------+ 
| 102 | 2009-10-08 00:00:00 |           3 | 3000   | 
| 100 | 2009-10-08 00:00:00 |           3 | 1500   | 
| 101 | 2009-11-20 00:00:00 |           2 | 1560   | 
| 103 | 2008-05-20 00:00:00 |           4 | 2060   | 
+-----+---------------------+-------------+--------+
</pre>
<p>There are different types of joins given as follows:</p>
<ul class="list">
<li>JOIN</li>
<li>LEFT OUTER JOIN</li>
<li>RIGHT OUTER JOIN</li>
<li>FULL OUTER JOIN</li>
</ul>
<h2>JOIN</h2>
<p>JOIN clause is used to combine and retrieve the records from multiple tables. JOIN is same as OUTER JOIN in SQL. A JOIN condition is to be raised using the primary keys and foreign keys of the tables.</p>
<p>The following query executes JOIN on the CUSTOMER and ORDER tables, and retrieves the records:</p>
<pre class="prettyprint notranslate">
hive> SELECT c.ID, c.NAME, c.AGE, o.AMOUNT 
   > FROM CUSTOMERS c JOIN ORDERS o 
   > ON (c.ID = o.CUSTOMER_ID);
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+----+----------+-----+--------+ 
| ID | NAME     | AGE | AMOUNT | 
+----+----------+-----+--------+ 
| 3  | kaushik  | 23  | 3000   | 
| 3  | kaushik  | 23  | 1500   | 
| 2  | Khilan   | 25  | 1560   | 
| 4  | Chaitali | 25  | 2060   | 
+----+----------+-----+--------+
</pre>
<h2>LEFT OUTER JOIN</h2>
<p>The HiveQL LEFT OUTER JOIN returns all the rows from the left table, even if there are no matches in the right table. This means, if the ON clause matches 0 (zero) records in the right table, the JOIN still returns a row in the result, but with NULL in each column from the right table.</p>
<p>A LEFT JOIN returns all the values from the left table, plus the matched values from the right table, or NULL in case of no matching JOIN predicate.</p>
<p>The following query demonstrates LEFT OUTER JOIN between CUSTOMER and ORDER tables:</p>
<pre class="prettyprint notranslate">
hive> SELECT c.ID, c.NAME, o.AMOUNT, o.DATE 
   > FROM CUSTOMERS c 
   > LEFT OUTER JOIN ORDERS o 
   > ON (c.ID = o.CUSTOMER_ID);
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+----+----------+--------+---------------------+ 
| ID | NAME     | AMOUNT | DATE                | 
+----+----------+--------+---------------------+ 
| 1  | Ramesh   | NULL   | NULL                | 
| 2  | Khilan   | 1560   | 2009-11-20 00:00:00 | 
| 3  | kaushik  | 3000   | 2009-10-08 00:00:00 | 
| 3  | kaushik  | 1500   | 2009-10-08 00:00:00 | 
| 4  | Chaitali | 2060   | 2008-05-20 00:00:00 | 
| 5  | Hardik   | NULL   | NULL                | 
| 6  | Komal    | NULL   | NULL                | 
| 7  | Muffy    | NULL   | NULL                | 
+----+----------+--------+---------------------+
</pre>
<h2>RIGHT OUTER JOIN</h2>
<p>The HiveQL RIGHT OUTER JOIN returns all the rows from the right table, even if there are no matches in the left table. If the ON clause matches 0 (zero) records in the left table, the JOIN still returns a row in the result, but with NULL in each column from the left table.</p>
<p>A RIGHT JOIN returns all the values from the right table, plus the matched values from the left table, or NULL in case of no matching join predicate.</p>
<p>The following query demonstrates RIGHT OUTER JOIN between the CUSTOMER and ORDER tables.</p>
<pre class="prettyprint notranslate">
hive> SELECT c.ID, c.NAME, o.AMOUNT, o.DATE 
   > FROM CUSTOMERS c 
   > RIGHT OUTER JOIN ORDERS o 
   > ON (c.ID = o.CUSTOMER_ID);
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+------+----------+--------+---------------------+ 
| ID   | NAME     | AMOUNT | DATE                | 
+------+----------+--------+---------------------+ 
| 3    | kaushik  | 3000   | 2009-10-08 00:00:00 | 
| 3    | kaushik  | 1500   | 2009-10-08 00:00:00 | 
| 2    | Khilan   | 1560   | 2009-11-20 00:00:00 | 
| 4    | Chaitali | 2060   | 2008-05-20 00:00:00 | 
+------+----------+--------+---------------------+
</pre>
<h2>FULL OUTER JOIN</h2>
<p>The HiveQL FULL OUTER JOIN combines the records of both the left and the right outer tables that fulfil the JOIN condition. The joined table contains either all the records from both the tables, or fills in NULL values for missing matches on either side.</p>
<p>The following query demonstrates FULL OUTER JOIN between CUSTOMER and ORDER tables:</p>
<pre class="prettyprint notranslate">
hive> SELECT c.ID, c.NAME, o.AMOUNT, o.DATE 
   > FROM CUSTOMERS c 
   > FULL OUTER JOIN ORDERS o 
   > ON (c.ID = o.CUSTOMER_ID);
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
+------+----------+--------+---------------------+ 
| ID   | NAME     | AMOUNT | DATE                | 
+------+----------+--------+---------------------+ 
| 1    | Ramesh   | NULL   | NULL                | 
| 2    | Khilan   | 1560   | 2009-11-20 00:00:00 | 
| 3    | kaushik  | 3000   | 2009-10-08 00:00:00 | 
| 3    | kaushik  | 1500   | 2009-10-08 00:00:00 | 
| 4    | Chaitali | 2060   | 2008-05-20 00:00:00 | 
| 5    | Hardik   | NULL   | NULL                | 
| 6    | Komal    | NULL   | NULL                |
| 7    | Muffy    | NULL   | NULL                |  
| 3    | kaushik  | 3000   | 2009-10-08 00:00:00 | 
| 3    | kaushik  | 1500   | 2009-10-08 00:00:00 | 
| 2    | Khilan   | 1560   | 2009-11-20 00:00:00 | 
| 4    | Chaitali | 2060   | 2008-05-20 00:00:00 | 
+------+----------+--------+---------------------+
</pre>
<h1>Hive - Built-in Functions</h1>
<p>This chapter explains the built-in functions available in Hive. The functions look quite similar to SQL functions, except for their usage.</p>
<h2>Built-In Functions</h2>
<p>Hive supports the following built-in functions:</p>
<table class="table table-bordered">
<tr>
<th style="width:20%">Return Type</th>
<th>Signature</th>
<th>Description</th>
</tr>
<tr>
<td>BIGINT</td>
<td>round(double a)</td>
<td>It returns the rounded BIGINT value of the double.</td>
</tr>
<tr>
<td>BIGINT</td>
<td>floor(double a)</td>
<td>It returns the maximum BIGINT value that is equal or less than the double.</td>
</tr>
<tr>
<td>BIGINT</td>
<td>ceil(double a)</td>
<td>It returns the minimum BIGINT value that is equal or greater than the double.</td>
</tr>
<tr>
<td>double</td>
<td>rand(), rand(int seed)</td>
<td>It returns a random number that changes from row to row.</td>
</tr>
<tr>
<td>string</td>
<td>concat(string A, string B,...)</td>
<td>It returns the string resulting from concatenating B after A.</td>
</tr>
<tr>
<td>string</td>
<td>substr(string A, int start)</td>
<td>It returns the substring of A starting from start position till the end of string A.</td>
</tr>
<tr>
<td>string</td>
<td>substr(string A, int start, int length)</td>
<td>It returns the substring of A starting from start position with the given length.</td>
</tr>
<tr>
<td>string</td>
<td>upper(string A)</td>
<td>It returns the string resulting from converting all characters of A to upper case.</td>
</tr>
<tr>
<tr>
<td>string</td>
<td>ucase(string A)</td>
<td>Same as above.</td>
</tr>
<td>string</td>
<td>lower(string A)</td>
<td>It returns the string resulting from converting all characters of B to lower case.</td>
</tr>
<tr>
<td>string</td>
<td>lcase(string A)</td>
<td>Same as above.</td>
</tr>
<tr>
<td>string</td>
<td>trim(string A)</td>
<td>It returns the string resulting from trimming spaces from both ends of A.</td>
</tr>
<tr>
<td>string</td>
<td>ltrim(string A)</td>
<td>It returns the string resulting from trimming spaces from the beginning (left hand side) of A.</td>
</tr>
<tr>
<td>string</td>
<td>rtrim(string A)</td>
<td>rtrim(string A)
It returns the string resulting from trimming spaces from the end (right hand side) of A.</td>
</tr>
<tr>
<td>string</td>
<td>regexp_replace(string A, string B, string C)</td>
<td>It returns the string resulting from replacing all substrings in B that match the Java regular expression syntax with C.</td>
</tr>
<tr>
<td>int</td>
<td>size(Map&lt;K.V&gt;)</td>
<td>It returns the number of elements in the map type.</td>
</tr>
<tr>
<td>int</td>
<td>size(Array&lt;T&gt;)</td>
<td>It returns the number of elements in the array type.</td>
</tr>
<tr>
<td>value of &lt;type&gt;</td>
<td>cast(&lt;expr&gt; as &lt;type&gt;)</td>
<td>It converts the results of the expression expr to &lt;type&gt; e.g. cast('1' as BIGINT) converts the string '1' to it integral representation. A NULL is returned if the conversion does not succeed.</td>
</tr>
<tr>
<td>string</td>
<td>from_unixtime(int unixtime)</td>
<td>convert the number of seconds from Unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the format of "1970-01-01 00:00:00"</td>
</tr>
<tr>
<td>string</td>
<td>to_date(string timestamp)</td>
<td>It returns the date part of a timestamp string: to_date("1970-01-01 00:00:00") = "1970-01-01"</td>
</tr>
<tr>
<td>int</td>
<td>year(string date)</td>
<td>It returns the year part of a date or a timestamp string: year("1970-01-01 00:00:00") = 1970, year("1970-01-01") = 1970</td>
</tr>
<tr>
<td>int</td>
<td>month(string date)</td>
<td>It returns the month part of a date or a timestamp string: month("1970-11-01 00:00:00") = 11, month("1970-11-01") = 11</td>
</tr>
<tr>
<td>int</td>
<td>day(string date)</td>
<td>It returns the day part of a date or a timestamp string: day("1970-11-01 00:00:00") = 1,   day("1970-11-01") = 1</td>
</tr>
<tr>
<td>string</td>
<td>get_json_object(string json_string, string path)</td>
<td>It extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It returns NULL if the input json string is invalid.</td>
</tr>
</table>
<h3>Example</h3>
<p>The following queries demonstrate some built-in functions:</p>
<h3>round() function</h3>
<pre class="prettyprint notranslate">
hive> SELECT round(2.6) from temp;
</pre>
<p>On successful execution of query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
3.0
</pre>
<h3>floor() function</h3>
<pre class="prettyprint notranslate">
hive> SELECT floor(2.6) from temp;
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
2.0
</pre>
<h3>ceil() function</h3>
<pre class="prettyprint notranslate">
hive> SELECT ceil(2.6) from temp;
</pre>
<p>On successful execution of the query, you get to see the following response:</p>
<pre class="prettyprint notranslate">
3.0
</pre>
<h2>Aggregate Functions</h2>
<p>Hive supports the following built-in <b>aggregate functions</b>. The usage of these functions is as same as the SQL aggregate functions.</p>
<table class="table table-bordered">
<tr>
<th style="width:20%">Return Type</th>
<th>Signature</th>
<th>Description</th>
</tr>
<tr>
<td>BIGINT</td>
<td>count(*), count(expr),</td>
<td>count(*) - Returns the total number of retrieved rows.</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>sum(col), sum(DISTINCT col)</td>
<td>It returns the sum of the elements in the group or the sum of the distinct values of the column in the group.</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>avg(col), avg(DISTINCT col)</td>
<td>It returns the average of the elements in the group or the average of the distinct values of the column in the group.</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>min(col)</td>
<td>It returns the minimum value of the column in the group.</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>max(col)</td>
<td>It returns the maximum value of the column in the group.</td>
</tr>
</table>
<h1>Hive - View and Indexes</h1>
<p>This chapter describes how to create and manage views. Views are generated based on user requirements. You can save any result set data as a view. The usage of view in Hive is same as that of the view in SQL. It is a standard RDBMS concept. We can execute all DML operations on a view.</p>
<h2>Creating a View</h2>
<p>You can create a view at the time of executing a SELECT statement. The syntax is as follows:</p>
<pre class="prettyprint notranslate">
CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ]
[COMMENT table_comment]
AS SELECT ...
</pre>
<h2>Example</h2>
<p>Let us take an example for view. Assume employee table as given below, with the fields Id, Name, Salary, Designation, and Dept. Generate a query to retrieve the employee details who earn a salary of more than Rs 30000. We store the result in a view named <b>emp_30000.</b></p>
<pre class="prettyprint notranslate">
+------+--------------+-------------+-------------------+--------+
| ID   | Name         | Salary      | Designation       | Dept   |
+------+--------------+-------------+-------------------+--------+
|1201  | Gopal        | 45000       | Technical manager | TP     |
|1202  | Manisha      | 45000       | Proofreader       | PR     |
|1203  | Masthanvali  | 40000       | Technical writer  | TP     |
|1204  | Krian        | 40000       | Hr Admin          | HR     |
|1205  | Kranthi      | 30000       | Op Admin          | Admin  |
+------+--------------+-------------+-------------------+--------+
</pre>
<p>The following query retrieves the employee details using the above scenario:</p>
<pre class="prettyprint notranslate">
hive> CREATE VIEW emp_30000 AS
   > SELECT * FROM employee
   > WHERE salary>30000;
</pre>
<h2>Dropping a View</h2>
<p>Use the following syntax to drop a view:</p>
<pre class="prettyprint notranslate">
DROP VIEW view_name
</pre>
<p>The following query drops a view named as emp_30000:</p>
<pre class="prettyprint notranslate">
hive> DROP VIEW emp_30000;
</pre>
<h2>Creating an Index</h2>
<p>An Index is nothing but a pointer on a particular column of a table. Creating an index means creating a pointer on a particular column of a table. Its syntax is as follows:</p>
<pre class="prettyprint notranslate">
CREATE INDEX index_name
ON TABLE base_table_name (col_name, ...)
AS 'index.handler.class.name'
[WITH DEFERRED REBUILD]
[IDXPROPERTIES (property_name=property_value, ...)]
[IN TABLE index_table_name]
[PARTITIONED BY (col_name, ...)]
[
   [ ROW FORMAT ...] STORED AS ...
   | STORED BY ...
]
[LOCATION hdfs_path]
[TBLPROPERTIES (...)]
</pre>
<h2>Example</h2>
<p>Let us take an example for index. Use the same employee table that we have used earlier with the fields Id, Name, Salary, Designation, and Dept. Create an index named index_salary on the salary column of the employee table.</p>
<p>The following query creates an index:</p>
<pre class="prettyprint notranslate">
hive> CREATE INDEX inedx_salary ON TABLE employee(salary)
   > AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';
</pre>
<p>It is a pointer to the salary column. If the column is modified, the changes are stored using an index value.</p>
<h2>Dropping an Index</h2>
<p>The following syntax is used to drop an index:</p>
<pre class="prettyprint notranslate">
DROP INDEX &lt;index_name&gt; ON &lt;table_name&gt;
</pre>
<p>The following query drops an index named index_salary:</p>
<pre class="prettyprint notranslate">
hive> DROP INDEX index_salary ON employee;
</pre>
<div class="mui-container-fluid button-borders show">
<div class="pre-btn">
<a href="/hive/hive_questions_answers.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/hive/hive_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="google-bottom-ads">
<div>Advertisements</div>
<script><!--
var width = 580;
var height = 400;
var format = "580x400_as";
if( window.innerWidth < 468 ){
   width = 300;
   height = 250;
   format = "300x250_as";
}
google_ad_client = "pub-7133395778201029";
google_ad_width = width;
google_ad_height = height;
google_ad_format = format;
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
<div class="space-bottom"></div>
</div>
</div>
<!-- Tutorial Content Ends Here -->
<!-- Right Column Starts Here -->
<div class="mui-col-md-2 google-right-ads">
<div class="space-top"></div>
<div class="google-right-ad" style="margin: 0px auto !important;margin-top:5px;">
<script><!--
google_ad_client = "pub-2537027957187252";
google_ad_width = 300;
google_ad_height = 250;
google_ad_format = "300x250_as";
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9012177"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9012177")})</script>
</div>
<div class="space-bottom"></div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9013289"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9013289")})</script>
</div>
<div class="space-bottom" style="margin-bottom:15px;"></div>
</div>
<!-- Right Column Ends Here -->
</div>
</div>
<div class="clear"></div>
<footer id="footer">
<div class="mui--text-center">
<div class="mui--text-caption mui--text-light">
<a href="/index.htm" class="logo"><img class="img-responsive" src="/images/logo-black.png" alt="Tutorials Point" title="Tutorials Point"></a>
</div>
<ul class="mui-list--inline mui--text-body2 mui--text-light">
<li><a href="/about/index.htm"><i class="fal fa-globe"></i> About us</a></li>
<li><a href="/about/about_terms_of_use.htm"><i class="fal fa-asterisk"></i> Terms of use</a></li>
<li><a href="/about/about_privacy.htm#cookies"> <i class="fal fa-shield-check"></i> Cookies Policy</a></li>
<li><a href="/about/faq.htm"><i class="fal fa-question-circle"></i> FAQ's</a></li>
<li><a href="/about/about_helping.htm"><i class="fal fa-hands-helping"></i> Helping</a></li>
<li><a href="/about/contact_us.htm"><i class="fal fa-map-marker-alt"></i> Contact</a></li>
</ul>
<div class="mui--text-caption mui--text-light bottom-copyright-text">&copy; Copyright 2019. All Rights Reserved.</div>
</div>
<div id="privacy-banner">
  <div>
    <p>
      We use cookies to provide and improve our services. By using our site, you consent to our Cookies Policy.
      <a id="banner-accept" href="#">Accept</a>
      <a id="banner-learn" href="/about/about_cookies.htm" target="_blank">Learn more</a>
    </p>
  </div>
</div>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-232293-17"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-232293-6');
</script>
</footer>
</body>
</html>
