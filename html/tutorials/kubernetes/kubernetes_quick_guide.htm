<!DOCTYPE html>
<html lang="en-US">
<head>
<title>Kubernetes - Quick Guide</title>
<meta charset="utf-8">
<meta name="description" content="Kubernetes - Quick Guide - Kubernetes in an open source container management tool hosted by Cloud Native Computing Foundation (CNCF). This is also known as the enhanced version of Borg wh"/>
<meta name="keywords" content="C, C++, Python, Java, HTML, CSS, JavaScript, SQL, PHP, jQuery, XML, DOM, Bootstrap, Tutorials, Articles, Programming, training, learning, quiz, preferences, examples, code"/>
<link rel="canonical" href="https://www.tutorialspoint.com/kubernetes/kubernetes_quick_guide.htm" />




<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<meta name="viewport" content="width=device-width,initial-scale=1.0,user-scalable=yes">
<script src="/theme/js/script-min-v2.js?v=3"></script>
<link rel="stylesheet" href="/theme/css/style-min-v2.css?v=6">
<script src="//services.bilsyndication.com/adv1/?d=901" defer="" async=""></script>
<script> var vitag = vitag || {};</script>
<script> vitag.outStreamConfig = { enablePC: false, enableMobile: false };</script>  
<style>
.right-menu .mui-btn {
    background-color:#2B55F5;
}
a.demo {
    background:#2B55F5;
}
li.heading {
    background:#2B55F5;
}
.course-box{background:#2B55F5}
.home-intro-sub p{color:#2B55F5}
</style>
</head>
<body>
<header id="header">
<!-- Top sub-menu Starts Here -->
<div class="mui-appbar mui-container-fulid top-menu">
<div class="mui-container">
<div class="top-menu-item home">
<a href="https://www.tutorialspoint.com/index.htm" target="_blank" title="TutorialsPoint - Home"><i class="fal fa-home"></i> <span>Home</span></a>
</div>
<div class="top-menu-item qa">
<a href="https://www.tutorialspoint.com/about/about_careers.htm" target="_blank" title="Job @ Tutorials Point"><i class="fa fa-suitcase"></i> <span>Jobs</span></a>
</div>
<div class="top-menu-item tools">
<a href="https://www.tutorialspoint.com/online_dev_tools.htm" target="_blank" title="Tools - Online Development and Testing Tools"><i class="fal fa-cogs"></i> <span>Tools</span></a>
</div>
<div class="top-menu-item coding-ground">
<a href="https://www.tutorialspoint.com/codingground.htm" target="_blank" title="Coding Ground - Free Online IDE and Terminal"><i class="fal fa-code"></i> <span>Coding Ground </span></a> 
</div>
<div class="top-menu-item current-affairs">
<a href="https://www.tutorialspoint.com/current_affairs.htm" target="_blank" title="Daily Current Affairs"><i class="fal fa-layer-plus"></i> <span>Current Affairs</span></a>
</div>
<div class="top-menu-item upsc-notes">
<a href="https://www.tutorialspoint.com/upsc_ias_exams.htm" target="_blank" title="UPSC IAS Exams Notes - TutorialsPoint"><i class="fal fa-user-tie"></i> <span>UPSC Notes</span></a>
</div>      
<div class="top-menu-item online-tutoris">
<a href="https://www.tutorialspoint.com/tutor_connect/index.php" target="_blank" title="Top Online Tutors - Tutor Connect"><i class="fal fa-user"></i> <span>Online Tutors</span></a>
</div>
<div class="top-menu-item whiteboard">
<a href="https://www.tutorialspoint.com/whiteboard.htm" target="_blank" title="Free Online Whiteboard"><i class="fal fa-chalkboard"></i> <span>Whiteboard</span></a>
</div>
<div class="top-menu-item net-meeting">
<a href="https://www.tutorialspoint.com/netmeeting.php" target="_blank" title="A free tool for online video conferencing"><i class="fal fa-chalkboard-teacher"></i> <span>Net Meeting</span></a> 
</div>
<div class="top-menu-item articles">
<a href="https://www.tutorix.com" target="_blank" title="Tutorx - The Best Learning App" rel="nofollow"><i class="fal fa-video"></i> <span>Tutorix</span></a> 
</div>        
<div class="social-menu-item">
<a href="https://www.facebook.com/tutorialspointindia" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Facebook"><i class="fab fa-facebook-f"></i></a> 
<a href="https://www.twitter.com/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Twitter"><i class="fab fa-twitter"></i></a>
<a href="https://www.linkedin.com/company/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Linkedin"><i class="fab fa-linkedin-in"></i></a>
<a href="https://www.youtube.com/channel/UCVLbzhxVTiTLiVKeGV7WEBg" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint YouTube"><i class="fab fa-youtube"></i></a>
</div>        
</div>
</div>
<!-- Top sub-menu Ends Here -->
<!-- Top main-menu Starts Here -->
<div class="mui-appbar mui-container-fulid mui--appbar-line-height mui--z1" id="logo-menu">
<div class="mui-container">
<div class="left-menu">
<a href="https://www.tutorialspoint.com/index.htm" title="Tutorialspoint">
<img class="tp-logo" alt="tutorialspoint" src="/kubernetes/images/logo.png">
</a>
<div class="mui-dropdown">
<a class="mui-btn mui-btn--primary categories" data-mui-toggle="dropdown"><i class="fa fa-th-large"></i> 
<span>Categories <span class="mui-caret"></span></span></a>            
<ul class="mui-dropdown__menu cat-menu">
<li>
<ul>
<li><a href="/academic_tutorials.htm"><i class="fa fa-caret-right"></i> Academic Tutorials</a></li>
<li><a href="/big_data_tutorials.htm"><i class="fa fa-caret-right"></i> Big Data &amp; Analytics </a></li>
<li><a href="/computer_programming_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Programming </a></li>
<li><a href="/computer_science_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Science </a></li>
<li><a href="/database_tutorials.htm"><i class="fa fa-caret-right"></i> Databases </a></li>
<li><a href="/devops_tutorials.htm"><i class="fa fa-caret-right"></i> DevOps </a></li>
<li><a href="/digital_marketing_tutorials.htm"><i class="fa fa-caret-right"></i> Digital Marketing </a></li>
<li><a href="/engineering_tutorials.htm"><i class="fa fa-caret-right"></i> Engineering Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> Exams Syllabus </a></li>
<li><a href="/famous_monuments.htm"><i class="fa fa-caret-right"></i> Famous Monuments </a></li>
<li><a href="/gate_exams_tutorials.htm"><i class="fa fa-caret-right"></i> GATE Exams Tutorials</a></li>
<li><a href="/latest_technologies.htm"><i class="fa fa-caret-right"></i> Latest Technologies </a></li>
<li><a href="/machine_learning_tutorials.htm"><i class="fa fa-caret-right"></i> Machine Learning </a></li>
<li><a href="/mainframe_tutorials.htm"><i class="fa fa-caret-right"></i> Mainframe Development </a></li>
<li><a href="/management_tutorials.htm"><i class="fa fa-caret-right"></i> Management Tutorials </a></li>
<li><a href="/maths_tutorials.htm"><i class="fa fa-caret-right"></i> Mathematics Tutorials</a></li>
<li><a href="/microsoft_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Microsoft Technologies </a></li>
<li><a href="/misc_tutorials.htm"><i class="fa fa-caret-right"></i> Misc tutorials </a></li>
<li><a href="/mobile_development_tutorials.htm"><i class="fa fa-caret-right"></i> Mobile Development </a></li>
<li><a href="/java_technology_tutorials.htm"><i class="fa fa-caret-right"></i> Java Technologies </a></li>
<li><a href="/python_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Python Technologies </a></li>
<li><a href="/sap_tutorials.htm"><i class="fa fa-caret-right"></i> SAP Tutorials </a></li>
<li><a href="/scripting_lnaguage_tutorials.htm"><i class="fa fa-caret-right"></i>Programming Scripts </a></li>
<li><a href="/selected_reading.htm"><i class="fa fa-caret-right"></i> Selected Reading </a></li>
<li><a href="/software_quality_tutorials.htm"><i class="fa fa-caret-right"></i> Software Quality </a></li>
<li><a href="/soft_skill_tutorials.htm"><i class="fa fa-caret-right"></i> Soft Skills </a></li>
<li><a href="/telecom_tutorials.htm"><i class="fa fa-caret-right"></i> Telecom Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> UPSC IAS Exams </a></li>
<li><a href="/web_development_tutorials.htm"><i class="fa fa-caret-right"></i> Web Development </a></li>
<li><a href="/sports_tutorials.htm"><i class="fa fa-caret-right"></i> Sports Tutorials </a></li>
<li><a href="/xml_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> XML Technologies </a></li>
<li><a href="/multi_language_tutorials.htm"><i class="fa fa-caret-right"></i> Multi-Language Tutorials</a></li>
<li><a href="/questions_and_answers.htm"><i class="fa fa-caret-right"></i> Interview Questions</a></li>
</ul>
</li>
</ul>
<div class="clear"></div>
</div> 
</div>
<div class="right-menu">
<div class="toc-toggle">
<a href="javascript:void(0);"><i class="fa fa-bars"></i></a>
</div>
<div class="mobile-search-btn">
<a href="https://www.tutorialspoint.com/search.htm"><i class="fal fa-search"></i></a>
</div>
<div class="search-box">
<form method="get" class="" name="searchform" action="https://www.google.com/search" target="_blank" novalidate="">
<input type="hidden" name="sitesearch" value="www.tutorialspoint.com" class="user-valid valid">
<input class="header-search-box" type="text" id="search-string" name="q" placeholder="Search your favorite tutorials..." onfocus="if (this.value == 'Search your favorite tutorials...') {this.value = '';}" onblur="if (this.value == '') {this.value = 'Search your favorite tutorials...';}" autocomplete="off">
<button><i class="fal fa-search"></i></button>
</form>
</div>
<div class="menu-btn library-btn">
<a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a>
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a> 
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/questions/index.php"><i class="fa fa-location-arrow"></i> <span>Q/A</span></a>
</div>
<div class="menu-btn ebooks-btn">
<a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a>
</div>
<div class="mui-dropdown">
<button class="mui-btn mui-btn--primary" data-mui-toggle="dropdown">
<span class="mui-caret"></span>
</button>
<ul class="mui-dropdown__menu">
<li><a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a></li>
<li><a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a></li>
<li><a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a></li>
</ul>
</div>
</div>
</div>
</div>
<!-- Top main-menu Ends Here -->
</header>
<div class="mui-container-fluid content">
<div class="mui-container">
<!-- Tutorial ToC Starts Here -->
<div class="mui-col-md-3 tutorial-toc">
<div class="mini-logo">
<img src="/kubernetes/images/kubernetes-mini-logo.jpg" alt="Kubernetes Tutorial" />
</div>
<ul class="toc chapters">
<li class="heading">Kubernetes Tutorial</li>
<li><a href="/kubernetes/index.htm">Kubernetes - Home</a></li>
<li><a href="/kubernetes/kubernetes_overview.htm">Kubernetes - Overview</a></li>
<li><a href="/kubernetes/kubernetes_architecture.htm">Kubernetes - Architecture</a></li>
<li><a href="/kubernetes/kubernetes_setup.htm">Kubernetes - Setup</a></li>
<li><a href="/kubernetes/kubernetes_images.htm">Kubernetes - Images</a></li>
<li><a href="/kubernetes/kubernetes_jobs.htm">Kubernetes - Jobs</a></li>
<li><a href="/kubernetes/kubernetes_labels_selectors.htm">Kubernetes - Labels &amp; Selectors</a></li>
<li><a href="/kubernetes/kubernetes_namespace.htm">Kubernetes - Namespace</a></li>
<li><a href="/kubernetes/kubernetes_node.htm">Kubernetes - Node</a></li>
<li><a href="/kubernetes/kubernetes_service.htm">Kubernetes - Service</a></li>
<li><a href="/kubernetes/kubernetes_pod.htm">Kubernetes - Pod</a></li>
<li><a href="/kubernetes/kubernetes_replication_controller.htm">Kubernetes - Replication Controller</a></li>
<li><a href="/kubernetes/kubernetes_replica_sets.htm">Kubernetes - Replica Sets</a></li>
<li><a href="/kubernetes/kubernetes_deployments.htm">Kubernetes - Deployments</a></li>
<li><a href="/kubernetes/kubernetes_volumes.htm">Kubernetes - Volumes</a></li>
<li><a href="/kubernetes/kubernetes_secrets.htm">Kubernetes - Secrets</a></li>
<li><a href="/kubernetes/kubernetes_network_policy.htm">Kubernetes - Network Policy</a></li>
</ul>
<ul class="toc chapters">
<li class="heading">Advanced Kubernetes</li>
<li><a href="/kubernetes/kubernetes_api.htm">Kubernetes - API</a></li>
<li><a href="/kubernetes/kubernetes_kubectl.htm">Kubernetes - Kubectl</a></li>
<li><a href="/kubernetes/kubernetes_kubectl_commands.htm">Kubernetes - Kubectl Commands</a></li>
<li><a href="/kubernetes/kubernetes_creating_app.htm">Kubernetes - Creating an App</a></li>
<li><a href="/kubernetes/kubernetes_app_deployment.htm">Kubernetes - App Deployment</a></li>
<li><a href="/kubernetes/kubernetes_autoscaling.htm">Kubernetes - Autoscaling</a></li>
<li><a href="/kubernetes/kubernetes_dashboard_setup.htm">Kubernetes - Dashboard Setup</a></li>
<li><a href="/kubernetes/kubernetes_monitoring.htm">Kubernetes - Monitoring</a></li>
</ul>
<ul class="toc chapters">
<li class="heading">Kubernetes Useful Resources</li>
<li><a href="/kubernetes/kubernetes_quick_guide.htm">Kubernetes - Quick Guide</a></li>
<li><a href="/kubernetes/kubernetes_useful_resources.htm">Kubernetes - Useful Resources</a></li>
<li><a href="/kubernetes/kubernetes_discussion.htm">Kubernetes - Discussion</a></li>
</ul>
<ul class="toc reading">
<li class="sreading">Selected Reading</li>
<li><a target="_top" href="/upsc_ias_exams.htm">UPSC IAS Exams Notes</a></li>
<li><a target="_top" href="/developers_best_practices/index.htm">Developer's Best Practices</a></li>
<li><a target="_top" href="/questions_and_answers.htm">Questions and Answers</a></li>
<li><a target="_top" href="/effective_resume_writing.htm">Effective Resume Writing</a></li>
<li><a target="_top" href="/hr_interview_questions/index.htm">HR Interview Questions</a></li>
<li><a target="_top" href="/computer_glossary.htm">Computer Glossary</a></li>
<li><a target="_top" href="/computer_whoiswho.htm">Who is Who</a></li>
</ul>
</div>
<!-- Tutorial ToC Ends Here -->
<!-- Tutorial Content Starts Here -->
<div class="mui-col-md-6 tutorial-content">
<h1>Kubernetes - Quick Guide</h1>
<hr />
<div class="top-ad-heading">Advertisements</div>
<div style="text-align: center;">
<script><!--
google_ad_client = "pub-7133395778201029";
var width = document.getElementsByClassName("tutorial-content")[0].clientWidth - 40;
google_ad_width = width;
google_ad_height = 150;
google_ad_format = width + "x150_as";
google_ad_type = "image";
google_ad_channel = "";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="mui-container-fluid button-borders">
<div class="pre-btn">
<a href="/kubernetes/kubernetes_monitoring.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/kubernetes/kubernetes_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="clearer"></div>
<h1>Kubernetes - Overview</h1>
<p>Kubernetes in an open source container management tool hosted by Cloud Native Computing Foundation (CNCF). This is also known as the enhanced version of Borg which was developed at Google to manage both long running processes and batch jobs, which was earlier handled by separate systems.</p>
<p>Kubernetes comes with a capability of automating deployment, scaling of application, and operations of application containers across clusters. It is capable of creating container centric infrastructure.</p>
<h2>Features of Kubernetes</h2>
<p>Following are some of the important features of Kubernetes.</p>
<ul class="list">
<li><p>Continues development, integration and deployment</p></li>
<li><p>Containerized infrastructure</p></li>
<li><p>Application-centric management</p></li>
<li><p>Auto-scalable infrastructure</p></li>
<li><p>Environment consistency across development testing and production</p></li>
<li><p>Loosely coupled infrastructure, where each component can act as a separate unit</p></li>
<li><p>Higher density of resource utilization</p></li>
<li><p>Predictable infrastructure which is going to be created</p></li>
</ul>
<p>One of the key components of Kubernetes is, it can run application on clusters of physical and virtual machine infrastructure. It also has the capability to run applications on cloud. <b>It helps in moving from host-centric infrastructure to container-centric infrastructure.</b></p>
<h1>Kubernetes - Architecture</h1>
<p>In this chapter, we will discuss the basic architecture of Kubernetes.</p>
<h2>Kubernetes - Cluster Architecture</h2>
<p>As seen in the following diagram, Kubernetes follows client-server architecture. Wherein, we have master installed on one machine and the node on separate Linux machines.</p>
<img src="/kubernetes/images/cluster_architecture.jpg" alt ="Cluster Architecture"/>
<p>The key components of master and node are defined in the following section.</p>
<h2>Kubernetes - Master Machine Components</h2>
<p>Following are the components of Kubernetes Master Machine.</p>
<h3>etcd</h3>
<p>It stores the configuration information which can be used by each of the nodes in the cluster. It is a high availability key value store that can be distributed among multiple nodes. It is accessible only by Kubernetes API server as it may have some sensitive information. It is a distributed key value Store which is accessible to all.</p>
<h3>API Server</h3>
<p>Kubernetes is an API server which provides all the operation on cluster using the API. API server implements an interface, which means different tools and libraries can readily communicate with it. <b>Kubeconfig</b> is a package along with the server side tools that can be used for communication. It exposes Kubernetes API.</p>
<h3>Controller Manager</h3>
<p>This component is responsible for most of the collectors that regulates the state of cluster and performs a task. In general, it can be considered as a daemon which runs in nonterminating loop and is responsible for collecting and sending information to API server. It works toward getting the shared state of cluster and then make changes to bring the current status of the server to the desired state. The key controllers are replication controller, endpoint controller, namespace controller, and service account controller. The controller manager runs different kind of controllers to handle nodes, endpoints, etc.</p>
<h3>Scheduler</h3>
<p>This is one of the key components of Kubernetes master. It is a service in master responsible for distributing the workload. It is responsible for tracking utilization of working load on cluster nodes and then placing the workload on which resources are available and accept the workload. In other words, this is the mechanism responsible for allocating pods to available nodes. The scheduler is responsible for workload utilization and allocating pod to new node.</p>
<h2>Kubernetes - Node Components</h2>
<p>Following are the key components of Node server which are necessary to communicate with Kubernetes master.</p>
<h3>Docker</h3>
<p>The first requirement of each node is Docker which helps in running the encapsulated application containers in a relatively isolated but lightweight operating environment.</p>
<h3>Kubelet Service</h3>
<p>This is a small service in each node responsible for relaying information to and from control plane service. It interacts with <b>etcd</b> store to read configuration details and wright values. This communicates with the master component to receive commands and work. The <b>kubelet</b> process then assumes responsibility for maintaining the state of work and the node server. It manages network rules, port forwarding, etc.</p>
<h3>Kubernetes Proxy Service</h3>
<p>This is a proxy service which runs on each node and helps in making services available to the external host. It helps in forwarding the request to correct containers and is capable of performing primitive load balancing. It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. It manages pods on node, volumes, secrets, creating new containers’ health checkup, etc.</p>
<h2>Kubernetes - Master and Node Structure</h2>
<p>The following illustrations show the structure of Kubernetes Master and Node.</p>
<img src="/kubernetes/images/master_node_structure.jpg" alt="Master and Node Structure"/>
<h1>Kubernetes - Setup</h1>
<p>It is important to set up the Virtual Datacenter (vDC) before setting up Kubernetes. This can be considered as a set of machines where they can communicate with each other via the network. For hands-on approach, you can set up vDC on <b>PROFITBRICKS</b> if you do not have a physical or cloud infrastructure set up.</p>
<p>Once the IaaS setup on any cloud is complete, you need to configure the <b>Master</b> and the <b>Node</b>.</p>
<p><b>Note</b> &minus; The setup is shown for Ubuntu machines. The same can be set up on other Linux machines as well.</p>
<h2>Prerequisites</h2>
<p><b>Installing Docker</b> &minus; Docker is required on all the instances of Kubernetes. Following are the steps to install the Docker.</p>
<p><b>Step 1</b> &minus; Log on to the machine with the root user account.</p>
<p><b>Step 2</b> &minus; Update the package information. Make sure that the apt package is working.</p>
<p><b>Step 3</b> &minus; Run the following commands.</p>
<pre class="result notranslate">
$ sudo apt-get update
$ sudo apt-get install apt-transport-https ca-certificates
</pre>
<p><b>Step 4</b> &minus; Add the new GPG key.</p>
<pre class="result notranslate">
$ sudo apt-key adv \
   --keyserver hkp://ha.pool.sks-keyservers.net:80 \
   --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
$ echo "deb https://apt.dockerproject.org/repo ubuntu-trusty main" | sudo tee
/etc/apt/sources.list.d/docker.list
</pre>
<p><b>Step 5</b> &minus; Update the API package image.</p>
<pre class="result notranslate">
$ sudo apt-get update
</pre>
<p>Once all the above tasks are complete, you can start with the actual installation of the Docker engine. However, before this you need to verify that the kernel version you are using is correct.</p>
<h2>Install Docker Engine</h2>
<p>Run the following commands to install the Docker engine.</p>
<p><b>Step 1</b> &minus; Logon to the machine.</p>
<p><b>Step 2</b> &minus; Update the package index.</p>
<pre class="result notranslate">
$ sudo apt-get update
</pre>
<p><b>Step 3</b> &minus; Install the Docker Engine using the following command.</p>
<pre class="result notranslate">
$ sudo apt-get install docker-engine
</pre>
<p><b>Step 4</b> &minus; Start the Docker daemon.</p>
<pre class="result notranslate">
$ sudo apt-get install docker-engine
</pre>
<p><b>Step 5</b> &minus; To very if the Docker is installed, use the following command.</p>
<pre class="result notranslate">
$ sudo docker run hello-world
</pre>
<h2>Install etcd 2.0</h2>
<p>This needs to be installed on Kubernetes Master Machine. In order to install it, run the following commands.</p>
<pre class="result notranslate">
$ curl -L https://github.com/coreos/etcd/releases/download/v2.0.0/etcd
-v2.0.0-linux-amd64.tar.gz -o etcd-v2.0.0-linux-amd64.tar.gz -&gt;1
$ tar xzvf etcd-v2.0.0-linux-amd64.tar.gz ------&gt;2
$ cd etcd-v2.0.0-linux-amd64 ------------&gt;3
$ mkdir /opt/bin -------------&gt;4
$ cp etcd* /opt/bin -----------&gt;5
</pre>
<p>In the above set of command &minus;</p>
<ul class="list">
<li>First, we download the <b>etcd</b>. Save this with specified name.</li>
<li>Then, we have to un-tar the tar package.</li>
<li>We make a dir. inside the /opt named bin.</li>
<li>Copy the extracted file to the target location.</li>
</ul>
<p>Now we are ready to build Kubernetes. We need to install Kubernetes on all the machines on the cluster.</p>
<pre class="result notranslate">
$ git clone https://github.com/GoogleCloudPlatform/kubernetes.git
$ cd kubernetes
$ make release
</pre>
<p>The above command will create a <b>_output</b> dir in the root of the kubernetes folder. Next, we can extract the directory into any of the directory of our choice /opt/bin, etc.</p>
<p>Next, comes the networking part wherein we need to actually start with the setup of Kubernetes master and node. In order to do this, we will make an entry in the host file which can be done on the node machine.</p>
<pre class="result notranslate">
$ echo "&lt;IP address of master machine&gt; kube-master
&lt; IP address of Node Machine&gt;" &gt;&gt; /etc/hosts
</pre>
<p>Following will be the output of the above command.</p>
<img src="/kubernetes/images/output.jpg" alt="Output"/>
<p>Now, we will start with the actual configuration on Kubernetes Master.</p>
<p>First, we will start copying all the configuration files to their correct location.</p>
<pre class="result notranslate">
$ cp &lt;Current dir. location&gt;/kube-apiserver /opt/bin/
$ cp &lt;Current dir. location&gt;/kube-controller-manager /opt/bin/
$ cp &lt;Current dir. location&gt;/kube-kube-scheduler /opt/bin/
$ cp &lt;Current dir. location&gt;/kubecfg /opt/bin/
$ cp &lt;Current dir. location&gt;/kubectl /opt/bin/
$ cp &lt;Current dir. location&gt;/kubernetes /opt/bin/
</pre>
<p>The above command will copy all the configuration files to the required location. Now we will come back to the same directory where we have built the Kubernetes folder.</p>
<pre class="result notranslate">
$ cp kubernetes/cluster/ubuntu/init_conf/kube-apiserver.conf /etc/init/
$ cp kubernetes/cluster/ubuntu/init_conf/kube-controller-manager.conf /etc/init/
$ cp kubernetes/cluster/ubuntu/init_conf/kube-kube-scheduler.conf /etc/init/

$ cp kubernetes/cluster/ubuntu/initd_scripts/kube-apiserver /etc/init.d/
$ cp kubernetes/cluster/ubuntu/initd_scripts/kube-controller-manager /etc/init.d/
$ cp kubernetes/cluster/ubuntu/initd_scripts/kube-kube-scheduler /etc/init.d/

$ cp kubernetes/cluster/ubuntu/default_scripts/kubelet /etc/default/
$ cp kubernetes/cluster/ubuntu/default_scripts/kube-proxy /etc/default/
$ cp kubernetes/cluster/ubuntu/default_scripts/kubelet /etc/default/
</pre>
<p>The next step is to update the copied configuration file under /etc. dir.</p>
<p>Configure etcd on master using the following command.</p>
<pre class="result notranslate">
$ ETCD_OPTS = "-listen-client-urls = http://kube-master:4001"
</pre>
<h2>Configure kube-apiserver</h2>
<p>For this on the master, we need to edit the <b>/etc/default/kube-apiserver</b> file which we copied earlier.</p>
<pre class="result notranslate">
$ KUBE_APISERVER_OPTS = "--address = 0.0.0.0 \
--port = 8080 \
--etcd_servers = &lt;The path that is configured in ETCD_OPTS&gt; \
--portal_net = 11.1.1.0/24 \
--allow_privileged = false \
--kubelet_port = &lt; Port you want to configure&gt; \
--v = 0"
</pre>
<h2>Configure the kube Controller Manager</h2>
<p>We need to add the following content in <b>/etc/default/kube-controller-manager</b>.</p>
<pre class="result notranslate">
$ KUBE_CONTROLLER_MANAGER_OPTS = "--address = 0.0.0.0 \
--master = 127.0.0.1:8080 \
--machines = kube-minion \ -----&gt; #this is the kubernatics node
--v = 0
</pre>
<p>Next, configure the kube scheduler in the corresponding file.</p>
<pre class="result notranslate">
$ KUBE_SCHEDULER_OPTS = "--address = 0.0.0.0 \
--master = 127.0.0.1:8080 \
--v = 0"
</pre>
<p>Once all the above tasks are complete, we are good to go ahead by bring up the Kubernetes Master. In order to do this, we will restart the Docker.</p>
<pre class="result notranslate">
$ service docker restart
</pre>
<h2>Kubernetes Node Configuration</h2>
<p>Kubernetes node will run two services the <b>kubelet and the kube-proxy</b>. Before moving ahead, we need to copy the binaries we downloaded to their required folders where we want to configure the kubernetes node.</p>
<p>Use the same method of copying the files that we did for kubernetes master. As it will only run the kubelet and the kube-proxy, we will configure them.</p>
<pre class="result notranslate">
$ cp &lt;Path of the extracted file&gt;/kubelet /opt/bin/
$ cp &lt;Path of the extracted file&gt;/kube-proxy /opt/bin/
$ cp &lt;Path of the extracted file&gt;/kubecfg /opt/bin/
$ cp &lt;Path of the extracted file&gt;/kubectl /opt/bin/
$ cp &lt;Path of the extracted file&gt;/kubernetes /opt/bin/
</pre>
<p>Now, we will copy the content to the appropriate dir.</p>
<pre class="result notranslate">
$ cp kubernetes/cluster/ubuntu/init_conf/kubelet.conf /etc/init/
$ cp kubernetes/cluster/ubuntu/init_conf/kube-proxy.conf /etc/init/
$ cp kubernetes/cluster/ubuntu/initd_scripts/kubelet /etc/init.d/
$ cp kubernetes/cluster/ubuntu/initd_scripts/kube-proxy /etc/init.d/
$ cp kubernetes/cluster/ubuntu/default_scripts/kubelet /etc/default/
$ cp kubernetes/cluster/ubuntu/default_scripts/kube-proxy /etc/default/
</pre>
<p>We will configure the <b>kubelet</b> and <b>kube-proxy conf</b> files.</p>
<p>We will configure the <b>/etc/init/kubelet.conf</b>.</p>
<pre class="result notranslate">
$ KUBELET_OPTS = "--address = 0.0.0.0 \
--port = 10250 \
--hostname_override = kube-minion \
--etcd_servers = http://kube-master:4001 \
--enable_server = true
--v = 0"
/
</pre>
<p>For kube-proxy, we will configure using the following command.</p>
<pre class="result notranslate">
$ KUBE_PROXY_OPTS = "--etcd_servers = http://kube-master:4001 \
--v = 0"
/etc/init/kube-proxy.conf
</pre>
<p>Finally, we will restart the Docker service.</p>
<pre class="result notranslate">
$ service docker restart
</pre>
<p>Now we are done with the configuration. You can check by running the following commands.</p>
<pre class="result notranslate">
$ /opt/bin/kubectl get minions
</pre>
<h1>Kubernetes - Images</h1>
<p>Kubernetes (Docker) images are the key building blocks of Containerized Infrastructure. As of now, we are only supporting Kubernetes to support Docker images. Each container in a pod has its Docker image running inside it.</p>
<p>When we are configuring a pod, the image property in the configuration file has the same syntax as the Docker command does. The configuration file has a field to define the image name, which we are planning to pull from the registry.</p>
<p>Following is the common configuration structure which will pull image from Docker registry and deploy in to Kubernetes container.</p>
<pre class="result notranslate">
apiVersion: v1
kind: pod
metadata:
   name: Tesing_for_Image_pull -----------&gt; 1
   spec:
      containers:
         - name: neo4j-server ------------------------&gt; 2
         image: &lt;Name of the Docker image&gt;----------> 3
         imagePullPolicy: Always -------------&gt;4
         command: ["echo", "SUCCESS"] -------------------&gt;
</pre>
<p>In the above code, we have defined &minus;</p>
<ul class="list">
<li><p><b>name: Tesing_for_Image_pull</b> &minus; This name is given to identify and check what is the name of the container that would get created after pulling the images from Docker registry.</p></li>
<li><p><b>name: neo4j-server</b> &minus; This is the name given to the container that we are trying to create. Like we have given neo4j-server.</p></li>
<li><p><b>image: &lt;Name of the Docker image&gt;</b> &minus; This is the name of the image which we are trying to pull from the Docker or internal registry of images. We need to define a complete registry path along with the image name that we are trying to pull.</p></li>
<li><p><b>imagePullPolicy</b> &minus; Always - This image pull policy defines that whenever we run this file to create the container, it will pull the same name again.</p></li>
<li><p><b>command: [“echo”, “SUCCESS”]</b> &minus; With this, when we create the container and if everything goes fine, it will display a message when we will access the container.</p></li>
</ul>
<p>In order to pull the image and create a container, we will run the following command.</p>
<pre class="result notranslate">
$ kubectl create –f Tesing_for_Image_pull
</pre>
<p>Once we fetch the log, we will get the output as successful.</p>
<pre class="result notranslate">
$ kubectl log Tesing_for_Image_pull
</pre>
<p>The above command will produce an output of success or we will get an output as failure.</p>
<p><b>Note</b> &minus; It is recommended that you try all the commands yourself.</p>
<h1>Kubernetes - Jobs</h1>
<p>The main function of a job is to create one or more pod and tracks about the success of pods. They ensure that the specified number of pods are completed successfully. When a specified number of successful run of pods is completed, then the job is considered complete.</p>
<h2>Creating a Job</h2>
<p>Use the following command to create a job &minus;</p>
<pre class="result notranslate">
apiVersion: v1
kind: Job ------------------------&gt; 1
metadata:
   name: py
   spec:
   template:
      metadata
      name: py -------&gt; 2
      spec:
         containers:
            - name: py ------------------------&gt; 3
            image: python----------&gt; 4
            command: ["python", "SUCCESS"]
            restartPocliy: Never --------&gt; 5
</pre>
<p>In the above code, we have defined &minus;</p>
<ul class="list">
<li><p><b>kind: Job &rarr;</b> We have defined the kind as Job which will tell <b>kubectl</b> that the <b>yaml</b> file being used is to create a job type pod.</p></li>
<li><p><b>Name:py &rarr;</b> This is the name of the template that we are using and the spec defines the template.</p></li>
<li><p><b>name: py &rarr;</b> we have given a name as <b>py</b> under container spec which helps to identify the Pod which is going to be created out of it.</p></li>
<li><p><b>Image: python &rarr;</b> the image which we are going to pull to create the container which will run inside the pod.</p></li>
<li><p><b>restartPolicy: Never &rarr;</b>This condition of image restart is given as never which means that if the container is killed or if it is false, then it will not restart itself.</p></li>
</ul>
<p>We will create the job using the following command with yaml which is saved with the name <b>py.yaml</b>.</p>
<pre class="result notranslate">
$ kubectl create –f py.yaml
</pre>
<p>The above command will create a job. If you want to check the status of a job, use the following command.</p>
<pre class="result notranslate">
$ kubectl describe jobs/py
</pre>
<p>The above command will create a job. If you want to check the status of a job, use the following command.</p>
<h2>Scheduled Job</h2>
<p>Scheduled job in Kubernetes uses <b>Cronetes</b>, which takes Kubernetes job and launches them in Kubernetes cluster.</p>
<ul class="list">
<li>Scheduling a job will run a pod at a specified point of time.</li>
<li>A parodic job is created for it which invokes itself automatically.</li>
</ul>
<p><b>Note</b> &minus; The feature of a scheduled job is supported by version 1.4 and the betch/v2alpha 1 API is turned on by passing the <b>–runtime-config=batch/v2alpha1</b> while bringing up the API server.</p>
<p>We will use the same yaml which we used to create the job and make it a scheduled job.</p>
<pre class="result notranslate">
apiVersion: v1
kind: Job
metadata:
   name: py
spec:
   schedule: h/30 * * * * ? -------------------&gt; 1
   template:
      metadata
         name: py
      spec:
         containers:
         - name: py
         image: python
         args:
/bin/sh -------&gt; 2
-c
ps –eaf ------------&gt; 3
restartPocliy: OnFailure
</pre>
<p>In the above code, we have defined &minus;</p>
<ul class="list">
<li><p><b>schedule: h/30 * * * * ?</b> &rarr; To schedule the job to run in every 30 minutes.</p></li>
<li><p><b>/bin/sh:</b> This will enter in the container with /bin/sh</p></li>
<li><p><b>ps –eaf &rarr;</b> Will run ps -eaf command on the machine and list all the running process inside a container.</p></li>
</ul>
<p>This scheduled job concept is useful when we are trying to build and run a set of tasks at a specified point of time and then complete the process.</p>
<h1>Kubernetes - Labels &amp; Selectors</h1>
<h2>Labels</h2>
<p>Labels are key-value pairs which are attached to pods, replication controller and services. They are used as identifying attributes for objects such as pods and replication controller. They can be added to an object at creation time and can be added or modified at the run time.</p>
<h2>Selectors</h2>
<p>Labels do not provide uniqueness. In general, we can say many objects can carry the same labels. Labels selector are core grouping primitive in Kubernetes. They are used by the users to select a set of objects.</p>
<p>Kubernetes API currently supports two type of selectors &minus;</p>
<ul class="list">
<li>Equality-based selectors</li>
<li>Set-based selectors</li>
</ul>
<h3>Equality-based Selectors</h3>
<p>They allow filtering by key and value. Matching objects should satisfy all the specified labels.</p>
<h3>Set-based Selectors</h3>
<p>Set-based selectors allow filtering of keys according to a set of values.</p>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: sp-neo4j-standalone
spec:
   ports:
      - port: 7474
      name: neo4j
   type: NodePort
   selector:
      app: salesplatform ---------&gt; 1
      component: neo4j -----------&gt; 2
</pre>
<p>In the above code, we are using the label selector as <b>app: salesplatform</b> and component as <b>component: neo4j</b>.</p>
<p>Once we run the file using the <b>kubectl</b> command, it will create a service with the name <b>sp-neo4j-standalone</b> which will communicate on port 7474. The ype is <b>NodePort</b> with the new label selector as <b>app: salesplatform</b> and <b>component: neo4j</b>.</p>
<h1>Kubernetes - Namespace</h1>
<p>Namespace provides an additional qualification to a resource name. This is helpful when multiple teams are using the same cluster and there is a potential of name collision. It can be as a virtual wall between multiple clusters.</p>
<h2>Functionality of Namespace</h2>
<p>Following are some of the important functionalities of a Namespace in Kubernetes &minus;</p>
<ul class="list">
<li><p>Namespaces help pod-to-pod communication using the same namespace.</p></li>
<li><p>Namespaces are virtual clusters that can sit on top of the same physical cluster.</p></li>
<li><p>They provide logical separation between the teams and their environments.</p></li>
</ul>
<h2>Create a Namespace</h2>
<p>The following command is used to create a namespace.</p>
<pre class="result notranslate">
apiVersion: v1
kind: Namespce
metadata
   name: elk
</pre>
<h2>Control the Namespace</h2>
<p>The following command is used to control the namespace.</p>
<pre class="result notranslate">
$ kubectl create –f namespace.yml ---------&gt; 1
$ kubectl get namespace -----------------&gt; 2
$ kubectl get namespace &lt;Namespace name&gt; ------->3
$ kubectl describe namespace &lt;Namespace name&gt; ----&gt;4
$ kubectl delete namespace &lt;Namespace name&gt;
</pre>
<p>In the above code,</p>
<ul class="list">
<li>We are using the command to create a namespace.</li>
<li>This will list all the available namespace.</li>
<li>This will get a particular namespace whose name is specified in the command.</li>
<li>This will describe the complete details about the service.</li>
<li>This will delete a particular namespace present in the cluster.</li>
</ul>
<h2>Using Namespace in Service - Example</h2>
<p>Following is an example of a sample file for using namespace in service.</p>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: elasticsearch
   namespace: elk
   labels:
      component: elasticsearch
spec:
   type: LoadBalancer
   selector:
      component: elasticsearch
   ports:
   - name: http
      port: 9200
      protocol: TCP
   - name: transport
      port: 9300
      protocol: TCP
</pre>
<p>In the above code, we are using the same namespace under service metadata with the name of <b>elk</b>.</p>
<h1>Kubernetes - Node</h1>
<p>A node is a working machine in Kubernetes cluster which is also known as a minion. They are working units which can be physical, VM, or a cloud instance.</p>
<p>Each node has all the required configuration required to run a pod on it such as the proxy service and kubelet service along with the Docker, which is used to run the Docker containers on the pod created on the node.</p>
<p>They are not created by Kubernetes but they are created externally either by the cloud service provider or the Kubernetes cluster manager on physical or VM machines.</p>
<p>The key component of Kubernetes to handle multiple nodes is the controller manager, which runs multiple kind of controllers to manage nodes. To manage nodes, Kubernetes creates an object of kind node which will validate that the object which is created is a valid node.</p>
<h2>Service with Selector</h2>
<pre class="result notranslate">
apiVersion: v1
kind: node
metadata:
   name: &lt; ip address of the node&gt;
   labels:
      name: &lt;lable name&gt;
</pre>
<p>In JSON format the actual object is created which looks as follows &minus;</p>
<pre class="result notranslate">
{
   Kind: node
   apiVersion: v1
   "metadata": 
   {
      "name": "10.01.1.10",
      "labels"
      {
         "name": "cluster 1 node"
      }
   }
}
</pre>
<h2>Node Controller</h2>
<p>They are the collection of services which run in the Kubernetes master and continuously monitor the node in the cluster on the basis of metadata.name. If all the required services are running, then the node is validated and a newly created pod will be assigned to that node by the controller. If it is not valid, then the master will not assign any pod to it and will wait until it becomes valid.</p>
<p>Kubernetes master registers the node automatically, if <b>–register-node</b> flag is true.</p>
<pre class="result notranslate">
–register-node = true
</pre>
<p>However, if the cluster administrator wants to manage it manually then it could be done by turning the flat of &minus;</p>
<pre class="result notranslate">
–register-node = false
</pre>
<h1>Kubernetes - Service</h1>
<p>A service can be defined as a logical set of pods. It can be defined as an abstraction on the top of the pod which provides a single IP address and DNS name by which pods can be accessed. With Service, it is very easy to manage load balancing configuration. It helps pods to scale very easily.</p>
<p>A service is a REST object in Kubernetes whose definition can be posted to Kubernetes apiServer on the Kubernetes master to create a new instance.</p>
<h2>Service without Selector</h2>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: Tutorial_point_service
spec:
   ports:
   - port: 8080
   targetPort: 31999
</pre>
<p>The above configuration will create a service with the name Tutorial_point_service.</p>
<h2>Service Config File with Selector</h2>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: Tutorial_point_service
spec:
   selector:
      application: "My Application" -------------------&gt; (Selector)
   ports:
   - port: 8080
   targetPort: 31999
</pre>
<p>In this example, we have a selector; so in order to transfer traffic, we need to create an endpoint manually.</p>
<pre class="result notranslate">
apiVersion: v1
kind: Endpoints
metadata:
   name: Tutorial_point_service
subnets:
   address:
      "ip": "192.168.168.40" -------------------> (Selector)
   ports:
      - port: 8080
</pre>
<p>In the above code, we have created an endpoint which will route the traffic to the endpoint defined as “192.168.168.40:8080”.</p>
<h2>Multi-Port Service Creation</h2>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: Tutorial_point_service
spec:
   selector:
      application: “My Application” -------------------> (Selector)
   ClusterIP: 10.3.0.12
   ports:
      -name: http
      protocol: TCP
      port: 80
      targetPort: 31999
   -name:https
      Protocol: TCP
      Port: 443
      targetPort: 31998
</pre>
<h2>Types of Services</h2>
<p><b>ClusterIP</b> &minus; This helps in restricting the service within the cluster. It exposes the service within the defined Kubernetes cluster.</p>
<pre class="result notranslate">
spec:
   type: NodePort
   ports:
   - port: 8080
      nodePort: 31999
      name: NodeportService
</pre>
<p><b>NodePort</b> &minus; It will expose the service on a static port on the deployed node. A <b>ClusterIP</b> service, to which <b>NodePort</b> service will route, is automatically created. The service can be accessed from outside the cluster using the <b>NodeIP:nodePort</b>.</p>
<pre class="result notranslate">
spec:
   ports:
   - port: 8080
      nodePort: 31999
      name: NodeportService
      clusterIP: 10.20.30.40
</pre>
<p><b>Load Balancer</b> &minus; It uses cloud providers’ load balancer. <b>NodePort</b> and <b>ClusterIP</b> services are created automatically to which the external load balancer will route.</p>
<p>A full service <b>yaml</b> file with service type as Node Port. Try to create one yourself.</p>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: appname
   labels:
      k8s-app: appname
spec:
   type: NodePort
   ports:
   - port: 8080
      nodePort: 31999
      name: omninginx
   selector:
      k8s-app: appname
      component: nginx
      env: env_name
</pre>
<h1>Kubernetes - Pod</h1>
<p>A pod is a collection of containers and its storage inside a node of a Kubernetes cluster. It is possible to create a pod with multiple containers inside it. For example, keeping a database container and data container in the same pod.</p>
<h2>Types of Pod</h2>
<p>There are two types of Pods &minus;</p>
<ul class="list">
<li>Single container pod</li>
<li>Multi container pod</li>
</ul>
<h3>Single Container Pod</h3>
<p>They can be simply created with the kubctl run command, where you have a defined image on the Docker registry which we will pull while creating a pod.</p>
<pre class="result notranslate">
$ kubectl run &lt;name of pod&gt; --image=&lt;name of the image from registry&gt;
</pre>
<p><b>Example</b> &minus; We will create a pod with a tomcat image which is available on the Docker hub.</p>
<pre class="result notranslate">
$ kubectl run tomcat --image = tomcat:8.0
</pre>
<p>This can also be done by creating the <b>yaml</b> file and then running the <b>kubectl create</b> command.</p>
<pre class="result notranslate">
apiVersion: v1
kind: Pod
metadata:
   name: Tomcat
spec:
   containers:
   - name: Tomcat
      image: tomcat: 8.0
      ports:
containerPort: 7500
   imagePullPolicy: Always
</pre>
<p>Once the above <b>yaml</b> file is created, we will save the file with the name of <b>tomcat.yml</b> and run the create command to run the document.</p>
<pre class="result notranslate">
$ kubectl create –f tomcat.yml
</pre>
<p>It will create a pod with the name of tomcat. We can use the describe command along with <b>kubectl</b> to describe the pod.</p>
<h3>Multi Container Pod</h3>
<p>Multi container pods are created using <b>yaml mail</b> with the definition of the containers.</p>
<pre class="result notranslate">
apiVersion: v1
kind: Pod
metadata:
   name: Tomcat
spec:
   containers:
   - name: Tomcat
      image: tomcat: 8.0
      ports:
containerPort: 7500
   imagePullPolicy: Always
   -name: Database
      Image: mongoDB
      Ports:
containerPort: 7501
   imagePullPolicy: Always
</pre>
<p>In the above code, we have created one pod with two containers inside it, one for tomcat and the other for MongoDB.</p>
<h1>Kubernetes - Replication Controller</h1>
<p>Replication Controller is one of the key features of Kubernetes, which is responsible for managing the pod lifecycle. It is responsible for making sure that the specified number of pod replicas are running at any point of time. It is used in time when one wants to make sure that the specified number of pod or at least one pod is running. It has the capability to bring up or down the specified no of pod.</p>
<p>It is a best practice to use the replication controller to manage the pod life cycle rather than creating a pod again and again.</p>
<pre class="result notranslate">
apiVersion: v1
kind: ReplicationController --------------------------&gt; 1
metadata:
   name: Tomcat-ReplicationController --------------------------&gt; 2
spec:
   replicas: 3 ------------------------&gt; 3
   template:
      metadata:
         name: Tomcat-ReplicationController
      labels:
         app: App
         component: neo4j
      spec:
         containers:
         - name: Tomcat- -----------------------&gt; 4
         image: tomcat: 8.0
         ports:
            - containerPort: 7474 ------------------------&gt; 5
</pre>
<h2>Setup Details</h2>
<ul class ="list">
<li><p><b>Kind: ReplicationController</b> &rarr; In the above code, we have defined the kind as replication controller which tells the <b>kubectl</b> that the <b>yaml</b> file is going to be used for creating the replication controller.</p></li>
<li><p><b>name: Tomcat-ReplicationController</b> &rarr; This helps in identifying the name with which the replication controller will be created. If we run the kubctl, get <b>rc &lt; Tomcat-ReplicationController &gt;</b> it will show the replication controller details.</p></li>
<li><p><b>replicas: 3</b> &rarr; This helps the replication controller to understand that it needs to maintain three replicas of a pod at any point of time in the pod lifecycle.</p></li>
<li><p><b>name: Tomcat</b> &rarr; In the spec section, we have defined the name as tomcat which will tell the replication controller that the container present inside the pods is tomcat.</p></li>
<li><p><b>containerPort: 7474</b> &rarr; It helps in making sure that all the nodes in the cluster where the pod is running the container inside the pod will be exposed on the same port 7474.</p></li>
</ul>
<img src="/kubernetes/images/kube_service_replicas.jpg" alt="Kube Service for Replicas"/>
<p>Here, the Kubernetes service is working as a load balancer for three tomcat replicas.</p>
<h1>Kubernetes - Replica Sets</h1>
<p>Replica Set ensures how many replica of pod should be running. It can be considered as a replacement of replication controller. The key difference between the replica set and the replication controller is, the replication controller only supports equality-based selector whereas the replica set supports set-based selector.</p>
<pre class="result notranslate">
apiVersion: extensions/v1beta1 ---------------------&gt;1
kind: ReplicaSet --------------------------&gt; 2
metadata:
   name: Tomcat-ReplicaSet
spec:
   replicas: 3
   selector:
      matchLables:
         tier: Backend ------------------&gt; 3
      matchExpression:
{ key: tier, operation: In, values: [Backend]} --------------&gt; 4
template:
   metadata:
      lables:
         app: Tomcat-ReplicaSet
         tier: Backend
      labels:
         app: App
         component: neo4j
   spec:
      containers:
      - name: Tomcat
      image: tomcat: 8.0
      ports:
      - containerPort: 7474
</pre>
<h2>Setup Details</h2>
<ul class ="list">
<li><p><b>apiVersion: extensions/v1beta1</b> &rarr; In the above code, the API version is the advanced beta version of Kubernetes which supports the concept of replica set.</p></li>
<li><p><b>kind: ReplicaSet</b> &rarr; We have defined the kind as the replica set which helps kubectl to understand that the file is used to create a replica set.</p></li>
<li><p><b>tier: Backend</b> &rarr; We have defined the label tier as backend which creates a matching selector.</p></li>
<li><p><b>{key: tier, operation: In, values: [Backend]}</b> &rarr; This will help <b>matchExpression</b> to understand the matching condition we have defined and in the operation which is used by <b>matchlabel</b> to find details.</p></li>
</ul>
<p>Run the above file using <b>kubectl</b> and create the backend replica set with the provided definition in the <b>yaml</b> file.</p>
<img src="/kubernetes/images/kube_service__backend_replicaset.jpg" alt="Kube Service Backend Replicaset"/>
<h1>Kubernetes - Deployments</h1>
<p>Deployments are upgraded and higher version of replication controller. They manage the deployment of replica sets which is also an upgraded version of the replication controller. They have the capability to update the replica set and are also capable of rolling back to the previous version.</p>
<p>They provide many updated features of <b>matchLabels</b> and <b>selectors</b>. We have got a new controller in the Kubernetes master called the deployment controller which makes it happen. It has the capability to change the deployment midway.</p>
<h2>Changing the Deployment</h2>
<p><b>Updating</b> &minus; The user can update the ongoing deployment before it is completed. In this, the existing deployment will be settled and new deployment will be created.</p>
<p><b>Deleting</b> &minus; The user can pause/cancel the deployment by deleting it before it is completed. Recreating the same deployment will resume it.</p>
<p><b>Rollback</b> &minus; We can roll back the deployment or the deployment in progress. The user can create or update the deployment by using <b>DeploymentSpec.PodTemplateSpec = oldRC.PodTemplateSpec.</b></p>
<h2>Deployment Strategies</h2>
<p>Deployment strategies help in defining how the new RC should replace the existing RC.</p>
<p><b>Recreate</b> &minus; This feature will kill all the existing RC and then bring up the new ones. This results in quick deployment however it will result in downtime when the old pods are down and the new pods have not come up.</p>
<p><b>Rolling Update</b> &minus; This feature gradually brings down the old RC and brings up the new one. This results in slow deployment, however there is no deployment. At all times, few old pods and few new pods are available in this process.</p>
<p>The configuration file of Deployment looks like this.</p>
<pre class="result notranslate">
apiVersion: extensions/v1beta1 ---------------------&gt;1
kind: Deployment --------------------------&gt; 2
metadata:
   name: Tomcat-ReplicaSet
spec:
   replicas: 3
   template:
      metadata:
         lables:
            app: Tomcat-ReplicaSet
            tier: Backend
   spec:
      containers:
         - name: Tomcatimage:
            tomcat: 8.0
            ports:
               - containerPort: 7474
</pre>
<p>In the above code, the only thing which is different from the replica set is we have defined the kind as deployment.</p>
<h2>Create Deployment</h2>
<pre class="result notranslate">
$ kubectl create –f Deployment.yaml -–record
deployment "Deployment" created Successfully.
</pre>
<h2>Fetch the Deployment</h2>
<pre class="result notranslate">
$ kubectl get deployments
NAME           DESIRED     CURRENT     UP-TO-DATE     AVILABLE    AGE
Deployment        3           3           3              3        20s
</pre>
<h2>Check the Status of Deployment</h2>
<pre class="result notranslate">
$ kubectl rollout status deployment/Deployment
</pre>
<h2>Updating the Deployment</h2>
<pre class="result notranslate">
$ kubectl set image deployment/Deployment tomcat=tomcat:6.0
</pre>
<h2>Rolling Back to Previous Deployment</h2>
<pre class="result notranslate">
$ kubectl rollout undo deployment/Deployment –to-revision=2
</pre>
<h1>Kubernetes - Volumes</h1>
<p>In Kubernetes, a volume can be thought of as a directory which is accessible to the containers in a pod. We have different types of volumes in Kubernetes and the type defines how the volume is created and its content.</p>
<p>The concept of volume was present with the Docker, however the only issue was that the volume was very much limited to a particular pod. As soon as the life of a pod ended, the volume was also lost.</p>
<p>On the other hand, the volumes that are created through Kubernetes is not limited to any container. It supports any or all the containers deployed inside the pod of Kubernetes. A key advantage of Kubernetes volume is, it supports different kind of storage wherein the pod can use multiple of them at the same time.</p>
<h3>Types of Kubernetes Volume</h3>
<p>Here is a list of some popular Kubernetes Volumes &minus;</p>
<ul class ="list">
<li><p><b>emptyDir</b> &minus; It is a type of volume that is created when a Pod is first assigned to a Node. It remains active as long as the Pod is running on that node. The volume is initially empty and the containers in the pod can read and write the files in the emptyDir volume. Once the Pod is removed from the node, the data in the emptyDir is erased.</p></li>
<li><p><b>hostPath</b> &minus; This type of volume mounts a file or directory from the host node’s filesystem into your pod.</p></li>
<li><p><b>gcePersistentDisk</b> &minus; This type of volume mounts a Google Compute Engine (GCE) Persistent Disk into your Pod. The data in a <b>gcePersistentDisk</b> remains intact when the Pod is removed from the node.</p></li>
<li><p><b>awsElasticBlockStore</b> &minus; This type of volume mounts an Amazon Web Services (AWS) Elastic Block Store into your Pod. Just like <b>gcePersistentDisk</b>, the data in an <b>awsElasticBlockStore</b> remains intact when the Pod is removed from the node.</p></li>
<li><p><b>nfs</b> &minus; An <b>nfs</b> volume allows an existing NFS (Network File System) to be mounted into your pod. The data in an <b>nfs</b> volume is not erased when the Pod is removed from the node. The volume is only unmounted.</p></li>
<li><p><b>iscsi</b> &minus; An <b>iscsi</b> volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your pod.</p></li>
<li><p><b>flocker</b> &minus; It is an open-source clustered container data volume manager. It is used for managing data volumes. A <b>flocker</b> volume allows a Flocker dataset to be mounted into a pod. If the dataset does not exist in Flocker, then you first need to create it by using the Flocker API.</p></li>
<li><p><b>glusterfs</b> &minus; Glusterfs is an open-source networked filesystem. A glusterfs volume allows a glusterfs volume to be mounted into your pod.</p></li>
<li><p><b>rbd</b> &minus; RBD stands for Rados Block Device. An <b>rbd</b> volume allows a Rados Block Device volume to be mounted into your pod. Data remains preserved after the Pod is removed from the node.</p></li>
<li><p><b>cephfs</b> &minus; A <b>cephfs</b> volume allows an existing CephFS volume to be mounted into your pod. Data remains intact after the Pod is removed from the node.</p></li>
<li><p><b>gitRepo</b> &minus; A <b>gitRepo</b> volume mounts an empty directory and clones a <b>git</b> repository into it for your pod to use.</p></li>
<li><p><b>secret</b> &minus; A <b>secret</b> volume is used to pass sensitive information, such as passwords, to pods.</p></li>
<li><p><b>persistentVolumeClaim</b> &minus; A <b>persistentVolumeClaim</b> volume is used to mount a PersistentVolume into a pod. PersistentVolumes are a way for users to “claim” durable storage (such as a GCE PersistentDisk or an iSCSI volume) without knowing the details of the particular cloud environment.</p></li>
<li><p><b>downwardAPI</b> &minus; A <b>downwardAPI</b> volume is used to make downward API data available to applications. It mounts a directory and writes the requested data in plain text files.</p></li>
<li><p><b>azureDiskVolume</b> &minus; An <b>AzureDiskVolume</b> is used to mount a Microsoft Azure Data Disk into a Pod.</p></li>
</ul>
<h2>Persistent Volume and Persistent Volume Claim</h2>
<p><b>Persistent Volume (PV)</b> &minus; It’s a piece of network storage that has been provisioned by the administrator. It’s a resource in the cluster which is independent of any individual pod that uses the PV.</p>
<p><b>Persistent Volume Claim (PVC)</b> &minus; The storage requested by Kubernetes for its pods is known as PVC. The user does not need to know the underlying provisioning. The claims must be created in the same namespace where the pod is created.</p>
<h3>Creating Persistent Volume</h3>
<pre class="result notranslate">
kind: PersistentVolume ---------&gt; 1
apiVersion: v1
metadata:
   name: pv0001 ------------------&gt; 2
   labels:
      type: local
spec:
   capacity: -----------------------&gt; 3
      storage: 10Gi ----------------------&gt; 4
   accessModes:
      - ReadWriteOnce -------------------&gt; 5
      hostPath:
         path: "/tmp/data01" --------------------------&gt; 6
</pre>
<p>In the above code, we have defined &minus;</p>
<ul class="list">
<li><p><b>kind: PersistentVolume</b> &rarr; We have defined the kind as PersistentVolume which tells kubernetes that the yaml file being used is to create the Persistent Volume.</p></li>
<li><p><b>name: pv0001</b> &rarr; Name of PersistentVolume that we are creating.</p></li>
<li><p><b>capacity:</b> &rarr; This spec will define the capacity of PV that we are trying to create.</p></li>
<li><p><b>storage: 10Gi</b> &rarr; This tells the underlying infrastructure that we are trying to claim 10Gi space on the defined path.</p></li>
<li><p><b>ReadWriteOnce</b> &rarr; This tells the access rights of the volume that we are creating.</p></li>
<li><p><b>path: "/tmp/data01"</b> &rarr; This definition tells the machine that we are trying to create volume under this path on the underlying infrastructure.</p></li>
</ul>
<h3>Creating PV</h3>
<pre class="result notranslate">
$ kubectl create –f local-01.yaml
persistentvolume "pv0001" created
</pre>
<h3>Checking PV</h3>
<pre class="result notranslate">
$ kubectl get pv
NAME        CAPACITY      ACCESSMODES       STATUS       CLAIM      REASON     AGE
pv0001        10Gi            RWO         Available                            14s
</pre>
<h3>Describing PV</h3>
<pre class="result notranslate">
$ kubectl describe pv pv0001
</pre>
<h3>Creating Persistent Volume Claim</h3>
<pre class="result notranslate">
kind: PersistentVolumeClaim --------------&gt; 1
apiVersion: v1
metadata:
   name: myclaim-1 --------------------&gt; 2
spec:
   accessModes:
      - ReadWriteOnce ------------------------> 3
   resources:
      requests:
         storage: 3Gi ---------------------> 4
</pre>
<p>In the above code, we have defined &minus;</p>
<ul class="list">
<li><p><b>kind: PersistentVolumeClaim</b> &rarr; It instructs the underlying infrastructure that we are trying to claim a specified amount of space.</p></li>
<li><p><b>name: myclaim-1</b> &rarr; Name of the claim that we are trying to create.</p></li>
<li><p><b>ReadWriteOnce</b> &rarr; This specifies the mode of the claim that we are trying to create.</p></li>
<li><p><b>storage: 3Gi</b> &rarr; This will tell kubernetes about the amount of space we are trying to claim.</p></li>
</ul>
<h3>Creating PVC</h3>
<pre class="result notranslate">
$ kubectl create –f myclaim-1
persistentvolumeclaim "myclaim-1" created
</pre>
<h3>Getting Details About PVC</h3>
<pre class="result notranslate">
$ kubectl get pvc
NAME        STATUS   VOLUME   CAPACITY   ACCESSMODES   AGE
myclaim-1   Bound    pv0001     10Gi         RWO       7s
</pre>
<h3>Describe PVC</h3>
<pre class="result notranslate">
$ kubectl describe pv pv0001
</pre>
<h3>Using PV and PVC with POD</h3>
<pre class="result notranslate">
kind: Pod
apiVersion: v1
metadata:
   name: mypod
   labels:
      name: frontendhttp
spec:
   containers:
   - name: myfrontend
      image: nginx
      ports:
      - containerPort: 80
         name: "http-server"
      volumeMounts: ----------------------------> 1
      - mountPath: "/usr/share/tomcat/html"
         name: mypd
   volumes: -----------------------> 2
      - name: mypd
         persistentVolumeClaim: ------------------------->3
         claimName: myclaim-1
</pre>
<p>In the above code, we have defined &minus;</p>
<ul class="list">
<li><p><b>volumeMounts:</b> &rarr; This is the path in the container on which the mounting will take place.</p></li>
<li><p><b>Volume:</b> &rarr; This definition defines the volume definition that we are going to claim.</p></li>
<li><p><b>persistentVolumeClaim:</b> &rarr; Under this, we define the volume name which we are going to use in the defined pod.</p></li>
</ul>
<h1>Kubernetes - Secrets</h1>
<p>Secrets can be defined as Kubernetes objects used to store sensitive data such as user name and passwords with encryption.</p>
<p>There are multiple ways of creating secrets in Kubernetes.</p>
<ul class="list">
<li>Creating from txt files.</li>
<li>Creating from yaml file.</li>
</ul>
<h3>Creating From Text File</h3>
<p>In order to create secrets from a text file such as user name and password, we first need to store them in a txt file and use the following command.</p>
<pre class="result notranslate">
$ kubectl create secret generic tomcat-passwd –-from-file = ./username.txt –fromfile = ./.
password.txt
</pre>
<h3>Creating From Yaml File</h3>
<pre class="result notranslate">
apiVersion: v1
kind: Secret
metadata:
name: tomcat-pass
type: Opaque
data:
   password: &lt;User Password&gt;
   username: &lt;User Name&gt;
</pre>
<h3>Creating the Secret</h3>
<pre class="result notranslate">
$ kubectl create –f Secret.yaml
secrets/tomcat-pass
</pre>
<h2>Using Secrets</h2>
<p>Once we have created the secrets, it can be consumed in a pod or the replication controller as &minus;</p>
<ul class="list">
<li>Environment Variable</li>
<li>Volume</li>
</ul>
<h3>As Environment Variable</h3>
<p>In order to use the secret as environment variable, we will use <b>env</b> under the spec section of pod yaml file.</p>
<pre class="result notranslate">
env:
- name: SECRET_USERNAME
   valueFrom:
      secretKeyRef:
         name: mysecret
         key: tomcat-pass
</pre>
<h3>As Volume</h3>
<pre class="result notranslate">
spec:
   volumes:
      - name: "secretstest"
         secret:
            secretName: tomcat-pass
   containers:
      - image: tomcat:7.0
         name: awebserver
         volumeMounts:
            - mountPath: "/tmp/mysec"
            name: "secretstest"
</pre>
<h3>Secret Configuration As Environment Variable</h3>
<pre class="result notranslate">
apiVersion: v1
kind: ReplicationController
metadata:
   name: appname
spec:
replicas: replica_count
template:
   metadata:
      name: appname
   spec:
      nodeSelector:
         resource-group:
      containers:
         - name: appname
            image:
            imagePullPolicy: Always
            ports:
            - containerPort: 3000
            env: -----------------------------> 1
               - name: ENV
                  valueFrom:
                     configMapKeyRef:
                        name: appname
                        key: tomcat-secrets
</pre>
<p>In the above code, under the <b>env</b> definition, we are using secrets as environment variable in the replication controller.</p>
<h3>Secrets As Volume Mount</h3>
<pre class="result notranslate">
apiVersion: v1
kind: pod
metadata:
   name: appname
spec:
   metadata:
      name: appname
   spec:
   volumes:
      - name: "secretstest"
         secret:
            secretName: tomcat-pass
   containers:
      - image: tomcat: 8.0
         name: awebserver
         volumeMounts:
            - mountPath: "/tmp/mysec"
            name: "secretstest"
</pre>
<h1>Kubernetes - Network Policy</h1>
<p>Network Policy defines how the pods in the same namespace will communicate with each other and the network endpoint. It requires <b>extensions/v1beta1/networkpolicies</b> to be enabled in the runtime configuration in the API server. Its resources use labels to select the pods and define rules to allow traffic to a specific pod in addition to which is defined in the namespace.</p>
<p>First, we need to configure Namespace Isolation Policy. Basically, this kind of networking policies are required on the load balancers.</p>
<pre class="result notranslate">
kind: Namespace
apiVersion: v1
metadata:
   annotations:
      net.beta.kubernetes.io/network-policy: |
      {
         "ingress": 
         {
            "isolation": "DefaultDeny"
         }
      }
</pre>
<p></p>
<pre class="result notranslate">
$ kubectl annotate ns &lt;namespace&gt; "net.beta.kubernetes.io/network-policy = 
{\"ingress\": {\"isolation\": \"DefaultDeny\"}}"
</pre>
<p>Once the namespace is created, we need to create the Network Policy.</p>
<h2>Network Policy Yaml</h2>
<pre class="result notranslate">
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
   name: allow-frontend
   namespace: myns
spec:
   podSelector:
      matchLabels:
         role: backend
   ingress:
   - from:
      - podSelector:
         matchLabels:
            role: frontend
   ports:
      - protocol: TCP
         port: 6379
</pre>
<h1>Kubernetes - API</h1>
<p>Kubernetes API serves as a foundation for declarative configuration schema for the system. <b>Kubectl</b> command-line tool can be used to create, update, delete, and get API object. Kubernetes API acts a communicator among different components of Kubernetes.</p>
<h2>Adding API to Kubernetes</h2>
<p>Adding a new API to Kubernetes will add new features to Kubernetes, which will increase the functionality of Kubernetes. However, alongside it will also increase the cost and maintainability of the system. In order to create a balance between the cost and complexity, there are a few sets defined for it.</p>
<p>The API which is getting added should be useful to more than 50% of the users. There is no other way to implement the functionality in Kubernetes. Exceptional circumstances are discussed in the community meeting of Kubernetes, and then API is added.</p>
<h2>API Changes</h2>
<p>In order to increase the capability of Kubernetes, changes are continuously introduced to the system. It is done by Kubernetes team to add the functionality to Kubernetes without removing or impacting the existing functionality of the system.</p>
<p>To demonstrate the general process, here is an (hypothetical) example &minus;</p>
<ul class="list">
<li><p>A user POSTs a Pod object to <b>/api/v7beta1/...</b></p></li>
<li><p>The JSON is unmarshalled into a <b>v7beta1.Pod</b> structure</p></li>
<li><p>Default values are applied to the <b>v7beta1.Pod</b></p></li>
<li><p>The <b>v7beta1.Pod</b> is converted to an <b>api.Pod</b> structure</p></li>
<li><p>The <b>api.Pod</b> is validated, and any errors are returned to the user</p></li>
<li><p>The <b>api.Pod</b> is converted to a v6.Pod (because v6 is the latest stable version)</p></li>
<li><p>The <b>v6.Pod</b> is marshalled into JSON and written to <b>etcd</b></p></li>
</ul>
<p>Now that we have the Pod object stored, a user can GET that object in any supported API version. For example &minus;</p>
<ul class="list">
<li><p>A user GETs the Pod from <b>/api/v5/...</b></p></li>
<li><p>The JSON is read from <b>etcd</b> and <b>unmarshalled</b> into a <b>v6.Pod</b> structure</p></li>
<li><p>Default values are applied to the <b>v6.Pod</b></p></li>
<li><p>The <b>v6.Pod</b> is converted to an api.Pod structure</p></li>
<li><p>The <b>api.Pod</b> is converted to a <b>v5.Pod</b> structure</p></li>
<li><p>The <b>v5.Pod</b> is marshalled into JSON and sent to the user</p></li>
</ul>
<p>The implication of this process is that API changes must be done carefully and backward compatibly.</p>
<h2>API Versioning</h2>
<p>To make it easier to support multiple structures, Kubernetes supports multiple API versions each at different API path such as <b>/api/v1</b> or <b>/apsi/extensions/v1beta1</b></p>
<p>Versioning standards at Kubernetes are defined in multiple standards.</p>
<h3>Alpha Level</h3>
<ul class="list">
<li><p>This version contains alpha (e.g. v1alpha1)</p></li>
<li><p>This version may be buggy; the enabled version may have bugs</p></li>
<li><p>Support for bugs can be dropped at any point of time.</p></li>
<li><p>Recommended to be used in short term testing only as the support may not be present all the time.</p></li>
</ul>
<h3>Beta Level</h3>
<ul class="list">
<li><p>The version name contains beta (e.g. v2beta3)</p></li>
<li><p>The code is fully tested and the enabled version is supposed to be stable.</p></li>
<li><p>The support of the feature will not be dropped; there may be some small changes.</p></li>
<li><p>Recommended for only non-business-critical uses because of the potential for incompatible changes in subsequent releases.</p></li>
</ul>
<h3>Stable Level</h3>
<ul class="list">
<li><p>The version name is <b>vX</b> where <b>X</b> is an integer.</p></li>
<li><p>Stable versions of features will appear in the released software for many subsequent versions.</p></li>
</ul>
<h1>Kubernetes - Kubectl</h1>
<p>Kubectl is the command line utility to interact with Kubernetes API. It is an interface which is used to communicate and manage pods in Kubernetes cluster.</p>
<p>One needs to set up kubectl to local in order to interact with Kubernetes cluster.</p>
<h2>Setting Kubectl</h2>
<p>Download the executable to the local workstation using the curl command.</p>
<h3>On Linux</h3>
<pre class="result notranslate">
$ curl -O https://storage.googleapis.com/kubernetesrelease/
release/v1.5.2/bin/linux/amd64/kubectl
</pre>
<h3>On OS X workstation</h3>
<pre class="result notranslate">
$ curl -O https://storage.googleapis.com/kubernetesrelease/
release/v1.5.2/bin/darwin/amd64/kubectl
</pre>
<p>After download is complete, move the binaries in the path of the system.</p>
<pre class="result notranslate">
$ chmod +x kubectl
$ mv kubectl /usr/local/bin/kubectl
</pre>
<h2>Configuring Kubectl</h2>
<p>Following are the steps to perform the configuration operation.</p>
<pre class="result notranslate">
$ kubectl config set-cluster default-cluster --server = https://${MASTER_HOST} --
certificate-authority = ${CA_CERT}

$ kubectl config set-credentials default-admin --certificateauthority = ${
CA_CERT} --client-key = ${ADMIN_KEY} --clientcertificate = ${
ADMIN_CERT}

$ kubectl config set-context default-system --cluster = default-cluster --
user = default-admin
$ kubectl config use-context default-system
</pre>
<ul class="list">
<li><p>Replace <b>${MASTER_HOST}</b> with the master node address or name used in the previous steps.</p></li>
<li><p>Replace <b>${CA_CERT}</b> with the absolute path to the <b>ca.pem</b> created in the previous steps.</p></li>
<li><p>Replace <b>${ADMIN_KEY}</b> with the absolute path to the <b>admin-key.pem</b> created in the previous steps.</p></li>
<li><p>Replace <b>${ADMIN_CERT}</b> with the absolute path to the <b>admin.pem</b> created in the previous steps.</p></li>
</ul>
<h2>Verifying the Setup</h2>
<p>To verify if the <b>kubectl</b> is working fine or not, check if the Kubernetes client is set up correctly.</p>
<pre class="result notranslate">
$ kubectl get nodes

NAME       LABELS                                     STATUS
Vipin.com  Kubernetes.io/hostname = vipin.mishra.com    Ready
</pre>
<h1>Kubernetes - Kubectl Commands</h1>
<p><b>Kubectl</b> controls the Kubernetes Cluster. It is one of the key components of Kubernetes which runs on the workstation on any machine when the setup is done. It has the capability to manage the nodes in the cluster.</p>
<p><b>Kubectl</b> commands are used to interact and manage Kubernetes objects and the cluster. In this chapter, we will discuss a few commands used in Kubernetes via kubectl.</p>
<p><b>kubectl annotate</b> &minus; It updates the annotation on a resource.</p>
<pre class="result notranslate">
$kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ...
KEY_N = VAL_N [--resource-version = version]
</pre>
<p>For example,</p>
<pre class="result notranslate">
kubectl annotate pods tomcat description = 'my frontend'
</pre>
<p><b>kubectl api-versions</b> &minus; It prints the supported versions of API on the cluster.</p>
<pre class="result notranslate">
$ kubectl api-version;
</pre>
<p><b>kubectl apply</b> &minus; It has the capability to configure a resource by file or stdin.</p>
<pre class="result notranslate">
$ kubectl apply –f &lt;filename&gt;
</pre>
<p><b>kubectl attach</b> &minus; This attaches things to the running container.</p>
<pre class="result notranslate">
$ kubectl attach &lt;pod&gt; –c &lt;container&gt;
$ kubectl attach 123456-7890 -c tomcat-conatiner
</pre>
<p><b>kubectl autoscale</b> &minus; This is used to auto scale pods which are defined such as Deployment, replica set, Replication Controller.</p>
<pre class="result notranslate">
$ kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min = MINPODS] --
max = MAXPODS [--cpu-percent = CPU] [flags]
$ kubectl autoscale deployment foo --min = 2 --max = 10
</pre>
<p><b>kubectl cluster-info</b> &minus; It displays the cluster Info.</p>
<pre class="result notranslate">
$ kubectl cluster-info
</pre>
<p><b>kubectl cluster-info dump</b> &minus; It dumps relevant information regarding cluster for debugging and diagnosis.</p>
<pre class="result notranslate">
$ kubectl cluster-info dump
$ kubectl cluster-info dump --output-directory = /path/to/cluster-state
</pre>
<p><b>kubectl config</b> &minus; Modifies the kubeconfig file.</p>
<pre class="result notranslate">
$ kubectl config &lt;SUBCOMMAD&gt;
$ kubectl config –-kubeconfig &lt;String of File name&gt;
</pre>
<p><b>kubectl config current-context</b> &minus; It displays the current context.</p>
<pre class="result notranslate">
$ kubectl config current-context
#deploys the current context
</pre>
<p><b>kubectl config delete-cluster</b> &minus; Deletes the specified cluster from kubeconfig.</p>
<pre class="result notranslate">
$ kubectl config delete-cluster &lt;Cluster Name&gt;
</pre>
<p><b>kubectl config delete-context</b> &minus; Deletes a specified context from kubeconfig.</p>
<pre class="result notranslate">
$ kubectl config delete-context &lt;Context Name&gt;
</pre>
<p><b>kubectl config get-clusters</b> &minus; Displays cluster defined in the kubeconfig.</p>
<pre class="result notranslate">
$ kubectl config get-cluster
$ kubectl config get-cluster &lt;Cluser Name&gt;
</pre>
<p><b>kubectl config get-contexts</b> &minus; Describes one or many contexts.</p>
<pre class="result notranslate">
$ kubectl config get-context &lt;Context Name&gt;
</pre>
<p><b>kubectl config set-cluster</b> &minus; Sets the cluster entry in Kubernetes.</p>
<pre class="result notranslate">
$ kubectl config set-cluster NAME [--server = server] [--certificateauthority =
path/to/certificate/authority] [--insecure-skip-tls-verify = true]
</pre>
<p><b>kubectl config set-context</b> &minus; Sets a context entry in kubernetes entrypoint.</p>
<pre class="result notranslate">
$ kubectl config set-context NAME [--cluster = cluster_nickname] [--
user = user_nickname] [--namespace = namespace]
$ kubectl config set-context prod –user = vipin-mishra
</pre>
<p><b>kubectl config set-credentials</b> &minus; Sets a user entry in kubeconfig.</p>
<pre class="result notranslate">
$ kubectl config set-credentials cluster-admin --username = vipin --
password = uXFGweU9l35qcif
</pre>
<p><b>kubectl config set</b> &minus; Sets an individual value in kubeconfig file.</p>
<pre class="result notranslate">
$ kubectl config set PROPERTY_NAME PROPERTY_VALUE
</pre>
<p><b>kubectl config unset</b> &minus; It unsets a specific component in kubectl.</p>
<pre class="result notranslate">
$ kubectl config unset PROPERTY_NAME PROPERTY_VALUE
</pre>
<p><b>kubectl config use-context</b> &minus; Sets the current context in kubectl file.</p>
<pre class="result notranslate">
$ kubectl config use-context &lt;Context Name&gt;
</pre>
<p><b>kubectl config view</b></p>
<pre class="result notranslate">
$ kubectl config view
$ kubectl config view –o jsonpath='{.users[?(@.name == "e2e")].user.password}'
</pre>
<p><b>kubectl cp</b> &minus; Copy files and directories to and from containers.</p>
<pre class="result notranslate">
$ kubectl cp &lt;Files from source&gt; &lt;Files to Destinatiion&gt;
$ kubectl cp /tmp/foo &lt;some-pod>:/tmp/bar -c &lt;specific-container&gt;
</pre>
<p><b>kubectl create</b> &minus; To create resource by filename of or stdin. To do this, JSON or YAML formats are accepted.</p>
<pre class="result notranslate">
$ kubectl create –f &lt;File Name&gt;
$ cat &lt;file name&gt; | kubectl create –f -
</pre>
<p>In the same way, we can create multiple things as listed using the <b>create</b> command along with <b>kubectl</b>.</p>
<ul class="list">
<li>deployment</li>
<li>namespace</li>
<li>quota</li>
<li>secret docker-registry</li>
<li>secret</li>
<li>secret generic</li>
<li>secret tls</li>
<li>serviceaccount</li>
<li>service clusterip</li>
<li>service loadbalancer</li>
<li>service nodeport</li>
</ul>
<p><b>kubectl delete</b> &minus; Deletes resources by file name, stdin, resource and names.</p>
<pre class="result notranslate">
$ kubectl delete –f ([-f FILENAME] | TYPE [(NAME | -l label | --all)])
</pre>
<p><b>kubectl describe</b> &minus; Describes any particular resource in kubernetes. Shows details of resource or a group of resources.</p>
<pre class="result notranslate">
$ kubectl describe &lt;type&gt; &lt;type name&gt;
$ kubectl describe pod tomcat
</pre>
<p><b>kubectl drain</b> &minus; This is used to drain a node for maintenance purpose. It prepares the node for maintenance. This will mark the node as unavailable so that it should not be assigned with a new container which will be created.</p>
<pre class="result notranslate">
$ kubectl drain tomcat –force
</pre>
<p><b>kubectl edit</b> &minus; It is used to end the resources on the server. This allows to directly edit a resource which one can receive via the command line tool.</p>
<pre class="result notranslate">
$ kubectl edit &lt;Resource/Name | File Name)
Ex.
$ kubectl edit rc/tomcat
</pre>
<p><b>kubectl exec</b> &minus; This helps to execute a command in the container.</p>
<pre class="result notranslate">
$ kubectl exec POD &lt;-c CONTAINER &gt; -- COMMAND &lt; args...&gt;
$ kubectl exec tomcat 123-5-456 date
</pre>
<p><b>kubectl expose</b> &minus; This is used to expose the Kubernetes objects such as pod, replication controller, and service as a new Kubernetes service. This has the capability to expose it via a running container or from a <b>yaml</b> file.</p>
<pre class="result notranslate">
$ kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol = TCP|UDP]
[--target-port = number-or-name] [--name = name] [--external-ip = external-ip-ofservice]
[--type = type]
$ kubectl expose rc tomcat –-port=80 –target-port = 30000
$ kubectl expose –f tomcat.yaml –port = 80 –target-port =
</pre>
<p><b>kubectl get</b> &minus; This command is capable of fetching data on the cluster about the Kubernetes resources.</p>
<pre class="result notranslate">
$ kubectl get [(-o|--output=)json|yaml|wide|custom-columns=...|custom-columnsfile=...|
go-template=...|go-template-file=...|jsonpath=...|jsonpath-file=...]
(TYPE [NAME | -l label] | TYPE/NAME ...) [flags]
</pre>
<p>For example,</p>
<pre class="result notranslate">
$ kubectl get pod &lt;pod name&gt;
$ kubectl get service &lt;Service name&gt;
</pre>
<p><b>kubectl logs</b> &minus; They are used to get the logs of the container in a pod. Printing the logs can be defining the container name in the pod. If the POD has only one container there is no need to define its name.</p>
<pre class="result notranslate">
$ kubectl logs [-f] [-p] POD [-c CONTAINER]
Example
$ kubectl logs tomcat.
$ kubectl logs –p –c tomcat.8
</pre>
<p><b>kubectl port-forward</b> &minus; They are used to forward one or more local port to pods.</p>
<pre class="result notranslate">
$ kubectl port-forward POD [LOCAL_PORT:]REMOTE_PORT
[...[LOCAL_PORT_N:]REMOTE_PORT_N]
$ kubectl port-forward tomcat 3000 4000
$ kubectl port-forward tomcat 3000:5000
</pre>
<p><b>kubectl replace</b> &minus; Capable of replacing a resource by file name or <b>stdin</b>.</p>
<pre class="result notranslate">
$ kubectl replace -f FILENAME
$ kubectl replace –f tomcat.yml
$ cat tomcat.yml | kubectl replace –f -
</pre>
<p><b>kubectl rolling-update</b> &minus; Performs a rolling update on a replication controller. Replaces the specified replication controller with a new replication controller by updating a POD at a time.</p>
<pre class="result notranslate">
$ kubectl rolling-update OLD_CONTROLLER_NAME ([NEW_CONTROLLER_NAME] --
image = NEW_CONTAINER_IMAGE | -f NEW_CONTROLLER_SPEC)
$ kubectl rolling-update frontend-v1 –f freontend-v2.yaml
</pre>
<p><b>kubectl rollout</b> &minus; It is capable of managing the rollout of deployment.</p>
<pre class="result notranslate">
$ Kubectl rollout &lt;Sub Command&gt;
$ kubectl rollout undo deployment/tomcat
</pre>
<p>Apart from the above, we can perform multiple tasks using the rollout such as &minus;</p>
<ul class="list">
<li>rollout history</li>
<li>rollout pause</li>
<li>rollout resume</li>
<li>rollout status</li>
<li>rollout undo</li>
</ul>
<p><b>kubectl run</b> &minus; Run command has the capability to run an image on the Kubernetes cluster.</p>
<pre class="result notranslate">
$ kubectl run NAME --image = image [--env = "key = value"] [--port = port] [--
replicas = replicas] [--dry-run = bool] [--overrides = inline-json] [--command] --
[COMMAND] [args...]
$ kubectl run tomcat --image = tomcat:7.0
$ kubectl run tomcat –-image = tomcat:7.0 –port = 5000
</pre>
<p><b>kubectl scale</b> &minus; It will scale the size of Kubernetes Deployments, ReplicaSet, Replication Controller, or job.</p>
<pre class="result notranslate">
$ kubectl scale [--resource-version = version] [--current-replicas = count] --
replicas = COUNT (-f FILENAME | TYPE NAME )
$ kubectl scale –-replica = 3 rs/tomcat
$ kubectl scale –replica = 3 tomcat.yaml
</pre>
<p><b>kubectl set image</b> &minus; It updates the image of a pod template.</p>
<pre class="result notranslate">
$ kubectl set image (-f FILENAME | TYPE NAME)
CONTAINER_NAME_1 = CONTAINER_IMAGE_1 ... CONTAINER_NAME_N = CONTAINER_IMAGE_N
$ kubectl set image deployment/tomcat busybox = busybox ngnix = ngnix:1.9.1
$ kubectl set image deployments, rc tomcat = tomcat6.0 --all
</pre>
<p><b>kubectl set resources</b> &minus; It is used to set the content of the resource. It updates resource/limits on object with pod template.</p>
<pre class="result notranslate">
$ kubectl set resources (-f FILENAME | TYPE NAME) ([--limits = LIMITS &amp; --
requests = REQUESTS]
$ kubectl set resources deployment tomcat -c = tomcat --
limits = cpu = 200m,memory = 512Mi
</pre>
<p><b>kubectl top node</b> &minus; It displays CPU/Memory/Storage usage. The top command allows you to see the resource consumption for nodes.</p>
<pre class="result notranslate">
$ kubectl top node [node Name]
</pre>
<p>The same command can be used with a pod as well.</p>
<h1>Kubernetes - Creating an App</h1>
<p>In order to create an application for Kubernetes deployment, we need to first create the application on the Docker. This can be done in two ways &minus;</p>
<ul class="list">
<li>By downloading</li>
<li>From Docker file</li>
</ul>
<h2>By Downloading</h2>
<p>The existing image can be downloaded from Docker hub and can be stored on the local Docker registry.</p>
<p>In order to do that, run the Docker <b>pull</b> command.</p>
<pre class="result notranslate">
$ docker pull --help
Usage: docker pull [OPTIONS] NAME[:TAG|@DIGEST]
Pull an image or a repository from the registry
   -a, --all-tags = false     Download all tagged images in the repository
   --help = false             Print usage
</pre>
<p>Following will be the output of the above code.</p>
<img src= "/kubernetes/images/app_output.jpg" alt="App Output"/>
<p>The above screenshot shows a set of images which are stored in our local Docker registry.</p>
<p>If we want to build a container from the image which consists of an application to test, we can do it using the Docker run command.</p>
<pre class="result notranslate">
$ docker run –i –t unbunt /bin/bash
</pre>
<h2>From Docker File</h2>
<p>In order to create an application from the Docker file, we need to first create a Docker file.</p>
<p>Following is an example of Jenkins Docker file.</p>
<pre class="result notranslate">
FROM ubuntu:14.04
MAINTAINER vipinkumarmishra@virtusapolaris.com
ENV REFRESHED_AT 2017-01-15
RUN apt-get update -qq &amp;&amp; apt-get install -qqy curl
RUN curl https://get.docker.io/gpg | apt-key add -
RUN echo deb http://get.docker.io/ubuntu docker main > /etc/apt/↩
sources.list.d/docker.list
RUN apt-get update -qq &amp;&amp; apt-get install -qqy iptables ca-↩
certificates lxc openjdk-6-jdk git-core lxc-docker
ENV JENKINS_HOME /opt/jenkins/data
ENV JENKINS_MIRROR http://mirrors.jenkins-ci.org
RUN mkdir -p $JENKINS_HOME/plugins
RUN curl -sf -o /opt/jenkins/jenkins.war -L $JENKINS_MIRROR/war-↩
stable/latest/jenkins.war
RUN for plugin in chucknorris greenballs scm-api git-client git ↩
ws-cleanup ;\
do curl -sf -o $JENKINS_HOME/plugins/${plugin}.hpi \
-L $JENKINS_MIRROR/plugins/${plugin}/latest/${plugin}.hpi ↩
; done
ADD ./dockerjenkins.sh /usr/local/bin/dockerjenkins.sh
RUN chmod +x /usr/local/bin/dockerjenkins.sh
VOLUME /var/lib/docker
EXPOSE 8080
ENTRYPOINT [ "/usr/local/bin/dockerjenkins.sh" ]
</pre>
<p>Once the above file is created, save it with the name of Dockerfile and cd to the file path. Then, run the following command.</p>
<img src= "/kubernetes/images/run_command.jpg" alt="Run Command"/>
<pre class="result notranslate">
$ sudo docker build -t jamtur01/Jenkins .
</pre>
<p>Once the image is built, we can test if the image is working fine and can be converted to a container.</p>
<pre class="result notranslate">
$ docker run –i –t jamtur01/Jenkins /bin/bash
</pre>
<h1>Kubernetes - App Deployment</h1>
<p>Deployment is a method of converting images to containers and then allocating those images to pods in the Kubernetes cluster. This also helps in setting up the application cluster which includes deployment of service, pod, replication controller and replica set. The cluster can be set up in such a way that the applications deployed on the pod can communicate with each other.</p>
<p>In this setup, we can have a load balancer setting on top of one application diverting traffic to a set of pods and later they communicate to backend pods. The communication between pods happen via the service object built in Kubernetes.</p>
<img src= "/kubernetes/images/application_cluster_view.jpg" alt="Application Cluster View"/>
<h2>Ngnix Load Balancer Yaml File</h2>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: oppv-dev-nginx
      labels:
         k8s-app: omni-ppv-api
spec:
   type: NodePort
   ports:
   - port: 8080
      nodePort: 31999
      name: omninginx
   selector:
      k8s-app: appname
      component: nginx
      env: dev
</pre>
<h2>Ngnix Replication Controller Yaml</h2>
<pre class="result notranslate">
apiVersion: v1
kind: ReplicationController
metadata:
   name: appname
spec:
   replicas: replica_count
   template:
      metadata:
         name: appname
         labels:
            k8s-app: appname
            component: nginx
               env: env_name
spec:
   nodeSelector:
      resource-group: oppv
   containers:
      - name: appname
      image: IMAGE_TEMPLATE
      imagePullPolicy: Always
      ports:
         - containerPort: 8080
         resources:
            requests:
               memory: "request_mem"
               cpu: "request_cpu"
            limits:
               memory: "limit_mem"
               cpu: "limit_cpu"
            env:
            - name: BACKEND_HOST
               value: oppv-env_name-node:3000
</pre>
<h2>Frontend Service Yaml File</h2>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: appname
   labels:
      k8s-app: appname
spec:
   type: NodePort
   ports:
   - name: http
      port: 3000
      protocol: TCP
      targetPort: 3000
   selector:
      k8s-app: appname
      component: nodejs
      env: dev
</pre>
<h2>Frontend Replication Controller Yaml File</h2>
<pre class="result notranslate">
apiVersion: v1
kind: ReplicationController
metadata:
   name: Frontend
spec:
   replicas: 3
   template:
      metadata:
         name: frontend
         labels:
            k8s-app: Frontend
            component: nodejs
            env: Dev
spec:
   nodeSelector:
      resource-group: oppv
   containers:
      - name: appname
         image: IMAGE_TEMPLATE
         imagePullPolicy: Always
         ports:
            - containerPort: 3000
            resources:
               requests:
                  memory: "request_mem"
                  cpu: "limit_cpu"
                  limits:
                  memory: "limit_mem"
                  cpu: "limit_cpu"
            env:
               - name: ENV
               valueFrom:
               configMapKeyRef:
               name: appname
               key: config-env
</pre>
<h2>Backend Service Yaml File</h2>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: backend
   labels:
      k8s-app: backend
spec:
   type: NodePort
   ports:
   - name: http
      port: 9010
      protocol: TCP
      targetPort: 9000
   selector:
      k8s-app: appname
      component: play
      env: dev
</pre>
<h2>Backed Replication Controller Yaml File</h2>
<pre class="result notranslate">
apiVersion: v1
kind: ReplicationController
metadata:
   name: backend
spec:
   replicas: 3
   template:
      metadata:
         name: backend
      labels:
         k8s-app: beckend
         component: play
         env: dev
spec:
   nodeSelector:
      resource-group: oppv
      containers:
         - name: appname
            image: IMAGE_TEMPLATE
            imagePullPolicy: Always
            ports:
            - containerPort: 9000
            command: [ "./docker-entrypoint.sh" ]
            resources:
               requests:
                  memory: "request_mem"
                  cpu: "request_cpu"
               limits:
                  memory: "limit_mem"
                  cpu: "limit_cpu"
            volumeMounts:
               - name: config-volume
               mountPath: /app/vipin/play/conf
         volumes:
            - name: config-volume
            configMap:
            name: appname
</pre>
<h1>Kubernetes - Autoscaling</h1>
<p><b>Autoscaling</b> is one of the key features in Kubernetes cluster. It is a feature in which the cluster is capable of increasing the number of nodes as the demand for service response increases and decrease the number of nodes as the requirement decreases. This feature of auto scaling is currently supported in Google Cloud Engine (GCE) and Google Container Engine (GKE) and will start with AWS pretty soon.</p>
<p>In order to set up scalable infrastructure in GCE, we need to first have an active GCE project with features of Google cloud monitoring, google cloud logging, and stackdriver enabled.</p>
<p>First, we will set up the cluster with few nodes running in it. Once done, we need to set up the following environment variable.</p>
<h2>Environment Variable</h2>
<pre class="result notranslate">
export NUM_NODES = 2
export KUBE_AUTOSCALER_MIN_NODES = 2
export KUBE_AUTOSCALER_MAX_NODES = 5
export KUBE_ENABLE_CLUSTER_AUTOSCALER = true
</pre>
<p>Once done, we will start the cluster by running <b>kube-up.sh</b>. This will create a cluster together with cluster auto-scalar add on.</p>
<pre class="result notranslate">
./cluster/kube-up.sh
</pre>
<p>On creation of the cluster, we can check our cluster using the following kubectl command.</p>
<pre class="result notranslate">
$ kubectl get nodes
NAME                             STATUS                       AGE
kubernetes-master                Ready,SchedulingDisabled     10m
kubernetes-minion-group-de5q     Ready                        10m
kubernetes-minion-group-yhdx     Ready                        8m
</pre>
<p>Now, we can deploy an application on the cluster and then enable the horizontal pod autoscaler. This can be done using the following command.</p>
<pre class="result notranslate">
$ kubectl autoscale deployment &lt;Application Name&gt; --cpu-percent = 50 --min = 1 --
max = 10
</pre>
<p>The above command shows that we will maintain at least one and maximum 10 replica of the POD as the load on the application increases.</p>
<p>We can check the status of autoscaler by running the <b>$kubclt get hpa</b> command. We will increase the load on the pods using the following command.</p>
<pre class="result notranslate">
$ kubectl run -i --tty load-generator --image = busybox /bin/sh
$ while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done
</pre>
<p>We can check the <b>hpa</b> by running <b>$ kubectl get hpa</b> command.</p>
<pre class="result notranslate">
$ kubectl get hpa
NAME         REFERENCE                     TARGET CURRENT
php-apache   Deployment/php-apache/scale    50%    310%

MINPODS  MAXPODS   AGE
  1        20      2m
  
$ kubectl get deployment php-apache
NAME         DESIRED    CURRENT    UP-TO-DATE    AVAILABLE   AGE
php-apache      7          7           7            3        4m
</pre>
<p>We can check the number of pods running using the following command.</p>
<pre class="result notranslate">
jsz@jsz-desk2:~/k8s-src$ kubectl get pods
php-apache-2046965998-3ewo6 0/1        Pending 0         1m
php-apache-2046965998-8m03k 1/1        Running 0         1m
php-apache-2046965998-ddpgp 1/1        Running 0         5m
php-apache-2046965998-lrik6 1/1        Running 0         1m
php-apache-2046965998-nj465 0/1        Pending 0         1m
php-apache-2046965998-tmwg1 1/1        Running 0         1m
php-apache-2046965998-xkbw1 0/1        Pending 0         1m
</pre>
<p>And finally, we can get the node status.</p>
<pre class="result notranslate">
$ kubectl get nodes
NAME                             STATUS                        AGE
kubernetes-master                Ready,SchedulingDisabled      9m
kubernetes-minion-group-6z5i     Ready                         43s
kubernetes-minion-group-de5q     Ready                         9m
kubernetes-minion-group-yhdx     Ready                         9m
</pre>
<h1>Kubernetes - Dashboard Setup</h1>
<p>Setting up Kubernetes dashboard involves several steps with a set of tools required as the prerequisites to set it up.</p>
<ul class="list">
<li>Docker (1.3+)</li>
<li>go (1.5+)</li>
<li>nodejs (4.2.2+)</li>
<li>npm (1.3+)</li>
<li>java (7+)</li>
<li>gulp (3.9+)</li>
<li>Kubernetes (1.1.2+)</li>
</ul>
<h2>Setting Up the Dashboard</h2>
<pre class="result notranslate">
$ sudo apt-get update &amp;&amp; sudo apt-get upgrade

Installing Python
$ sudo apt-get install python
$ sudo apt-get install python3

Installing GCC
$ sudo apt-get install gcc-4.8 g++-4.8

Installing make
$ sudo apt-get install make

Installing Java
$ sudo apt-get install openjdk-7-jdk

Installing Node.js
$ wget https://nodejs.org/dist/v4.2.2/node-v4.2.2.tar.gz
$ tar -xzf node-v4.2.2.tar.gz
$ cd node-v4.2.2
$ ./configure
$ make
$ sudo make install

Installing gulp
$ npm install -g gulp
$ npm install gulp
</pre>
<h2>Verifying Versions</h2>
<pre class="result notranslate">
Java Version
$ java –version
java version "1.7.0_91"
OpenJDK Runtime Environment (IcedTea 2.6.3) (7u91-2.6.3-1~deb8u1+rpi1)
OpenJDK Zero VM (build 24.91-b01, mixed mode)

$ node –v
V4.2.2

$ npn -v
2.14.7

$ gulp -v
[09:51:28] CLI version 3.9.0

$ sudo gcc --version
gcc (Raspbian 4.8.4-1) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc. This is free software; 
see the source for copying conditions. There is NO warranty; not even for 
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</pre>
<h2>Installing GO</h2>
<pre class="result notranslate">
$ git clone https://go.googlesource.com/go
$ cd go
$ git checkout go1.4.3
$ cd src

Building GO
$ ./all.bash
$ vi /root/.bashrc
In the .bashrc
   export GOROOT = $HOME/go
   export PATH = $PATH:$GOROOT/bin
   
$ go version
go version go1.4.3 linux/arm
</pre>
<h2>Installing Kubernetes Dashboard</h2>
<pre class="result notranslate">
$ git clone https://github.com/kubernetes/dashboard.git
$ cd dashboard
$ npm install -g bower
</pre>
<h2>Running the Dashboard</h2>
<pre class="result notranslate">
$ git clone https://github.com/kubernetes/dashboard.git
$ cd dashboard
$ npm install -g bower
$ gulp serve
[11:19:12] Requiring external module babel-core/register
[11:20:50] Using gulpfile ~/dashboard/gulpfile.babel.js
[11:20:50] Starting 'package-backend-source'...
[11:20:50] Starting 'kill-backend'...
[11:20:50] Finished 'kill-backend' after 1.39 ms
[11:20:50] Starting 'scripts'...
[11:20:53] Starting 'styles'...
[11:21:41] Finished 'scripts' after 50 s
[11:21:42] Finished 'package-backend-source' after 52 s
[11:21:42] Starting 'backend'...
[11:21:43] Finished 'styles' after 49 s
[11:21:43] Starting 'index'...
[11:21:44] Finished 'index' after 1.43 s
[11:21:44] Starting 'watch'...
[11:21:45] Finished 'watch' after 1.41 s
[11:23:27] Finished 'backend' after 1.73 min
[11:23:27] Starting 'spawn-backend'...
[11:23:27] Finished 'spawn-backend' after 88 ms
[11:23:27] Starting 'serve'...
2016/02/01 11:23:27 Starting HTTP server on port 9091
2016/02/01 11:23:27 Creating API client for
2016/02/01 11:23:27 Creating Heapster REST client for http://localhost:8082
[11:23:27] Finished 'serve' after 312 ms
[BS] [BrowserSync SPA] Running...
[BS] Access URLs:
--------------------------------------
Local: http://localhost:9090/
External: http://192.168.1.21:9090/
--------------------------------------
UI: http://localhost:3001
UI External: http://192.168.1.21:3001
--------------------------------------
[BS] Serving files from: /root/dashboard/.tmp/serve
[BS] Serving files from: /root/dashboard/src/app/frontend
[BS] Serving files from: /root/dashboard/src/app
</pre>
<h2>The Kubernetes Dashboard</h2>
<img src="/kubernetes/images/kubernetes_dashboard.jpg" alt="Kubernetes Dashboard"/>
<h1>Kubernetes - Monitoring</h1>
<p>Monitoring is one of the key component for managing large clusters. For this, we have a number of tools.</p>
<h2>Monitoring with Prometheus</h2>
<p>It is a monitoring and alerting system. It was built at SoundCloud and was open sourced in 2012. It handles the multi-dimensional data very well.</p>
<p>Prometheus has multiple components to participate in monitoring &minus;</p>
<ul class="list">
<li><p><b>Prometheus</b> &minus; It is the core component that scraps and stores data.</p></li>
<li><p><b>Prometheus node explore</b> &minus; Gets the host level matrices and exposes them to Prometheus.</p></li>
<li><p><b>Ranch-eye</b> &minus; is an <b>haproxy</b> and exposes <b>cAdvisor</b> stats to Prometheus.</p></li>
<li><p><b>Grafana</b> &minus; Visualization of data.</p></li>
<li><p><b>InfuxDB</b> &minus; Time series database specifically used to store data from rancher.</p></li>
<li><p><b>Prom-ranch-exporter</b> &minus; It is a simple node.js application, which helps in querying Rancher server for the status of stack of service.</p></li>
</ul>
<img src="/kubernetes/images/monitoring_prometheus.jpg" alt="Monitoring with Prometheus "/>
<h2>Sematext Docker Agent</h2>
<p>It is a modern Docker-aware metrics, events, and log collection agent. It runs as a tiny container on every Docker host and collects logs, metrics, and events for all cluster node and containers. It discovers all containers (one pod might contain multiple containers) including containers for Kubernetes core services, if the core services are deployed in Docker containers. After its deployment, all logs and metrics are immediately available out of the box.</p>
<h2>Deploying Agents to Nodes</h2>
<p>Kubernetes provides DeamonSets which ensures pods are added to the cluster.</p>
<h2>Configuring SemaText Docker Agent</h2>
<p>It is configured via environment variables.</p>
<ul class="list">
<li><p>Get a free account at <a target="_blank" rel="nofollow" href="https://apps.sematext.com/ui/registration">apps.sematext.com</a>, if you don’t have one already.</p></li>
<li><p>Create an SPM App of type “Docker” to obtain the SPM App Token. SPM App will hold your Kubernetes performance metrics and event.</p></li>
<li><p>Create a Logsene App to obtain the Logsene App Token. Logsene App will hold your Kubernetes logs.</p></li>
<li><p>Edit values of LOGSENE_TOKEN and SPM_TOKEN in the DaemonSet definition as shown below.</p></li>
<ul class="list">
<li><p>Grab the latest sematext-agent-daemonset.yml (raw plain-text) template (also shown below).</p></li>
<li><p>Store it somewhere on the disk.</p></li>
<li><p>Replace the SPM_TOKEN and LOGSENE_TOKEN placeholders with your SPM and Logsene App tokens.</p></li>
</ul>
</ul>
<h2>Create DaemonSet Object</h2>
<pre class="result notranslate">
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
   name: sematext-agent
spec:
   template:
      metadata:
         labels:
            app: sematext-agent
      spec:
         selector: {}
         dnsPolicy: "ClusterFirst"
         restartPolicy: "Always"
         containers:
         - name: sematext-agent
            image: sematext/sematext-agent-docker:latest
            imagePullPolicy: "Always"
            env:
            - name: SPM_TOKEN
               value: "REPLACE THIS WITH YOUR SPM TOKEN"
            - name: LOGSENE_TOKEN
               value: "REPLACE THIS WITH YOUR LOGSENE TOKEN"
            - name: KUBERNETES
               value: "1"
            volumeMounts:
               - mountPath: /var/run/docker.sock
                  name: docker-sock
               - mountPath: /etc/localtime
                  name: localtime
            volumes:
               - name: docker-sock
                  hostPath:
                     path: /var/run/docker.sock
               - name: localtime
                  hostPath:
                     path: /etc/localtime
</pre>
<h2>Running the Sematext Agent Docker with kubectl</h2>
<pre class="result notranslate">
$ kubectl create -f sematext-agent-daemonset.yml
daemonset "sematext-agent-daemonset" created
</pre>
<h2>Kubernetes Log</h2>
<p>Kubernetes containers’ logs are not much different from Docker container logs. However, Kubernetes users need to view logs for the deployed pods. Hence, it is very useful to have Kubernetes-specific information available for log search, such as &minus;</p>
<ul class="list">
<li>Kubernetes namespace</li>
<li>Kubernetes pod name</li>
<li>Kubernetes container name</li>
<li>Docker image name</li>
<li>Kubernetes UID</li>
</ul>
<h2>Using ELK Stack and LogSpout</h2>
<p>ELK stack includes Elasticsearch, Logstash, and Kibana. To collect and forward the logs to the logging platform, we will use LogSpout (though there are other options such as FluentD).</p>
<p>The following code shows how to set up ELK cluster on Kubernetes and create service for ElasticSearch &minus;</p>
<pre class="result notranslate">
apiVersion: v1
kind: Service
metadata:
   name: elasticsearch
   namespace: elk
   labels:
      component: elasticsearch
spec:
   type: LoadBalancer
   selector:
      component: elasticsearch
   ports:
   - name: http
      port: 9200
      protocol: TCP
   - name: transport
      port: 9300
      protocol: TCP
</pre>
<h2>Creating Replication Controller</h2>
<pre class="result notranslate">
apiVersion: v1
kind: ReplicationController
metadata:
   name: es
   namespace: elk
   labels:
      component: elasticsearch
spec:
   replicas: 1
   template:
      metadata:
         labels:
            component: elasticsearch
spec:
serviceAccount: elasticsearch
containers:
   - name: es
      securityContext:
      capabilities:
      add:
      - IPC_LOCK
   image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4
   env:
   - name: KUBERNETES_CA_CERTIFICATE_FILE
   value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
   - name: NAMESPACE
   valueFrom:
      fieldRef:
         fieldPath: metadata.namespace
   - name: "CLUSTER_NAME"
      value: "myesdb"
   - name: "DISCOVERY_SERVICE"
      value: "elasticsearch"
   - name: NODE_MASTER
      value: "true"
   - name: NODE_DATA
      value: "true"
   - name: HTTP_ENABLE
      value: "true"
ports:
- containerPort: 9200
   name: http
   protocol: TCP
- containerPort: 9300
volumeMounts:
- mountPath: /data
   name: storage
volumes:
   - name: storage
      emptyDir: {}
</pre>
<h2>Kibana URL</h2>
<p>For Kibana, we provide the Elasticsearch URL as an environment variable.</p>
<pre class="result notranslate">
- name: KIBANA_ES_URL
value: "http://elasticsearch.elk.svc.cluster.local:9200"
- name: KUBERNETES_TRUST_CERT
value: "true"
</pre>
<p>Kibana UI will be reachable at container port 5601 and corresponding host/Node Port combination. When you begin, there won’t be any data in Kibana (which is expected as you have not pushed any data).</p>
<div class="mui-container-fluid button-borders show">
<div class="pre-btn">
<a href="/kubernetes/kubernetes_monitoring.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/kubernetes/kubernetes_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="google-bottom-ads">
<div>Advertisements</div>
<script><!--
var width = 580;
var height = 400;
var format = "580x400_as";
if( window.innerWidth < 468 ){
   width = 300;
   height = 250;
   format = "300x250_as";
}
google_ad_client = "pub-7133395778201029";
google_ad_width = width;
google_ad_height = height;
google_ad_format = format;
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
<div class="space-bottom"></div>
</div>
</div>
<!-- Tutorial Content Ends Here -->
<!-- Right Column Starts Here -->
<div class="mui-col-md-2 google-right-ads">
<div class="space-top"></div>
<div class="google-right-ad" style="margin: 0px auto !important;margin-top:5px;">
<script><!--
google_ad_client = "pub-2537027957187252";
google_ad_width = 300;
google_ad_height = 250;
google_ad_format = "300x250_as";
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9012177"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9012177")})</script>
</div>
<div class="space-bottom"></div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9013289"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9013289")})</script>
</div>
<div class="space-bottom" style="margin-bottom:15px;"></div>
</div>
<!-- Right Column Ends Here -->
</div>
</div>
<div class="clear"></div>
<footer id="footer">
<div class="mui--text-center">
<div class="mui--text-caption mui--text-light">
<a href="/index.htm" class="logo"><img class="img-responsive" src="/images/logo-black.png" alt="Tutorials Point" title="Tutorials Point"></a>
</div>
<ul class="mui-list--inline mui--text-body2 mui--text-light">
<li><a href="/about/index.htm"><i class="fal fa-globe"></i> About us</a></li>
<li><a href="/about/about_terms_of_use.htm"><i class="fal fa-asterisk"></i> Terms of use</a></li>
<li><a href="/about/about_privacy.htm#cookies"> <i class="fal fa-shield-check"></i> Cookies Policy</a></li>
<li><a href="/about/faq.htm"><i class="fal fa-question-circle"></i> FAQ's</a></li>
<li><a href="/about/about_helping.htm"><i class="fal fa-hands-helping"></i> Helping</a></li>
<li><a href="/about/contact_us.htm"><i class="fal fa-map-marker-alt"></i> Contact</a></li>
</ul>
<div class="mui--text-caption mui--text-light bottom-copyright-text">&copy; Copyright 2019. All Rights Reserved.</div>
</div>
<div id="privacy-banner">
  <div>
    <p>
      We use cookies to provide and improve our services. By using our site, you consent to our Cookies Policy.
      <a id="banner-accept" href="#">Accept</a>
      <a id="banner-learn" href="/about/about_cookies.htm" target="_blank">Learn more</a>
    </p>
  </div>
</div>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-232293-17"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-232293-6');
</script>
</footer>
</body>
</html>
